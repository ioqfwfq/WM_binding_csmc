{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "311b5f76",
   "metadata": {},
   "source": [
    "# Neural Delay Activity and Distractor Resistance Analysis\n",
    "\n",
    "This notebook analyzes delay period neural activity and distractor resistance in working memory binding tasks.\n",
    "\n",
    "## 🚀 **QUICK START GUIDE**\n",
    "\n",
    "### **1. Install Required Packages**\n",
    "```bash\n",
    "pip install numpy pandas matplotlib seaborn scipy statsmodels tqdm\n",
    "```\n",
    "\n",
    "### **2. Core Analysis (Run cells 1-32)**\n",
    "- ✅ **Cells 1-32**: Core analysis pipeline (data loading → selectivity → distractor resistance)\n",
    "- ⚠️ **Cells 33+**: Advanced plotting (requires optional neural_analysis package)\n",
    "\n",
    "### **3. Expected Results**\n",
    "- **Selectivity analysis** identifies responsive neurons\n",
    "- **Distractor resistance analysis** finds persistent neurons  \n",
    "- **CSV outputs** saved to local files\n",
    "\n",
    "---\n",
    "\n",
    "## Analysis Overview\n",
    "\n",
    "1. **Data Loading & Preprocessing** - Load MATLAB data and prepare neural firing rate calculations\n",
    "2. **Single Unit Selectivity Analysis** - Identify neurons selective for categories and numerosities  \n",
    "3. **Delay Activity Analysis** - Find neurons maintaining selectivity during delay periods\n",
    "4. **Distractor Resistance Analysis** - Test which neurons resist interference from distractors\n",
    "5. **Visualization** - Plot distractor-resistant neurons and interference patterns (optional)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Data Loading & Preprocessing\n",
    "\n",
    "### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12b221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af4a7f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install numpy pandas matplotlib seaborn scipy statsmodels tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from scipy.stats import ttest_1samp, ttest_ind\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e1e17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c48947",
   "metadata": {},
   "source": [
    "### Loading MATLAB Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9c79ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all WMB_P*_v7.mat files in the current directory\n",
    "mat_files = glob.glob('./WMB_P*_v7.mat')\n",
    "print(f\"Found {len(mat_files)} .mat files: \\n{[os.path.basename(f) for f in mat_files]}\\n\")\n",
    "\n",
    "# Initialize lists to store data from each file\n",
    "cell_mats = []\n",
    "total_mats = []\n",
    "\n",
    "# Load each file and append its data\n",
    "for mat_file in mat_files:\n",
    "    print(f\"\\nLoading {mat_file}...\")\n",
    "    mat_data = loadmat(mat_file)\n",
    "    cell_mats.append(mat_data['cellStatsAll'])\n",
    "    total_mats.append(mat_data['totStats'])\n",
    "\n",
    "# Print shapes of loaded data for debugging\n",
    "print(\"\\nShapes of loaded data:\")\n",
    "for i, (cell, total) in enumerate(zip(cell_mats, total_mats)):\n",
    "    print(f\"File {i}: cell_mat shape: {cell.shape}, total_mat shape: {total.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6773f69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the data\n",
    "# For cell_mat, we need to handle the different dimensions\n",
    "# First, convert each cell_mat to a list of records\n",
    "all_cell_records = []\n",
    "for cell_mat in cell_mats:\n",
    "    # Convert to list of records\n",
    "    cell_list = cell_mat[0]  # now shape is (n,)\n",
    "    records = []\n",
    "    for cell in cell_list:\n",
    "        record = {key: cell[key] for key in cell.dtype.names}\n",
    "        records.append(record)\n",
    "    all_cell_records.extend(records)\n",
    "\n",
    "# Convert combined records to DataFrame\n",
    "df = pd.DataFrame(all_cell_records)\n",
    "\n",
    "# For total_mat, we can concatenate directly since they have the same structure\n",
    "total_mat = np.concatenate(total_mats, axis=0)\n",
    "\n",
    "print(f\"\\nCombined data shape - total_mat: {total_mat.shape}\")\n",
    "print(f\"\\nCombined data shape - df: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f401d2",
   "metadata": {},
   "source": [
    "### Brain Area Code Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d83a648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_area_codes(area_column):\n",
    "    \n",
    "    mapping = {\n",
    "        1: 'RH', 2: 'LH', 3: 'RA', 4: 'LA', 5: 'RAC', 6: 'LAC',\n",
    "        7: 'RSMA', 8: 'LSMA', 9: 'RPT', 10: 'LPT', 11: 'ROFC', 12: 'LOFC',\n",
    "        50: 'RFFA', 51: 'REC', 52: 'RCM', 53: 'LCM', 54: 'RPUL', 55: 'LPUL',\n",
    "        56: 'N/A', 57: 'RPRV', 58: 'LPRV'\n",
    "    }\n",
    "    \n",
    "    labels = []\n",
    "    for code in area_column:\n",
    "        label = mapping.get(code, 'Unknown')\n",
    "        labels.append(label)\n",
    "    \n",
    "    return dict(Counter(labels))\n",
    "\n",
    "# neuron number in each area\n",
    "area_codes = total_mat[:, 3]\n",
    "\n",
    "counts = count_area_codes(area_codes)\n",
    "print(\"Area counts (no prefix):\")\n",
    "for area, count in counts.items():\n",
    "    print(f\"{area}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6192ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2029f482",
   "metadata": {},
   "source": [
    "### Data Preprocessing & Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d032add5",
   "metadata": {},
   "outputs": [],
   "source": [
    "collapsed_area_map = {\n",
    "    1: 'H', 2: 'H',\n",
    "    3: 'A', 4: 'A',\n",
    "    5: 'AC', 6: 'AC',\n",
    "    7: 'SMA', 8: 'SMA',\n",
    "    9: 'PT', 10: 'PT',\n",
    "    11: 'OFC', 12: 'OFC',\n",
    "    50: 'FFA', 51: 'EC',\n",
    "    52: 'CM', 53: 'CM',\n",
    "    54: 'PUL', 55: 'PUL',\n",
    "    56: 'N/A', 57: 'PRV', 58: 'PRV'\n",
    "}\n",
    "# Convert brain area codes in the DataFrame\n",
    "df['brainAreaOfCell'] = df['brainAreaOfCell'].apply(\n",
    "    lambda x: collapsed_area_map.get(int(x[0, 0]), 'Unknown') if isinstance(x, np.ndarray) else collapsed_area_map.get(x, 'Unknown')\n",
    ")\n",
    "\n",
    "# Filter out units with low firing rate\n",
    "fr = df['timestamps'].apply(lambda x: len(x) / (x[-1] - x[0]) * 1e6)\n",
    "df_sample_new = df[fr > 0.1].reset_index(drop=True)\n",
    "\n",
    "# Add unit identifiers\n",
    "df_sample_new = df_sample_new.reset_index(drop=True)\n",
    "df_sample_new[\"unit_id\"] = df_sample_new.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3099b7eb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "375ea159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03c7fde2",
   "metadata": {},
   "source": [
    "### Trial Information Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d90cfa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_trial_info(trials_struct, unit_id):\n",
    "    # Build a DataFrame from the trials structure.\n",
    "    df_trial = pd.DataFrame({field: trials_struct[field].squeeze() \n",
    "                             for field in trials_struct.dtype.names})\n",
    "    # Add the unit_id so that you can later separate trials by unit/session.\n",
    "    df_trial[\"unit_id\"] = unit_id\n",
    "    df_trial[\"trial_nr\"] = df_trial[\"trial\"].apply(lambda x: np.squeeze(x).item() if isinstance(x, (list, np.ndarray)) else x) - 1 # Adjust for 0-indexing\n",
    "    return df_trial\n",
    "\n",
    "trial_info_list = []\n",
    "for idx, row in df_sample_new.iterrows():\n",
    "    # Use the unit identifier from this row\n",
    "    unit_id = row[\"unit_id\"]  \n",
    "    # Extract the trial DataFrame, including the unit identifier.\n",
    "    trial_info_list.append(extract_trial_info(row[\"Trials\"], unit_id))\n",
    "\n",
    "# Concatenate the list of trial info DataFrames into one.\n",
    "trial_info = pd.concat(trial_info_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd9fe9d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d84f5bb5",
   "metadata": {},
   "source": [
    "### Firing Rate Calculation Across Task Epochs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3281cbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is redundant - neural analysis setup is now in cell 33\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e26bdd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Event ts extraction\n",
    "def extract_event_timestamps(df_sample_new, start_idx_col='idxEnc1', end_idx_col='idxDel1', is_window=False, window_size=0.5):\n",
    "    # Extract timestamp arrays for each epoch\n",
    "    epoch_ts = {}\n",
    "    \n",
    "    for i, row in df_sample_new.iterrows():\n",
    "        unit_id = row['unit_id']\n",
    "        events = row['events'].squeeze()       # Ensure it's 1D array\n",
    "        \n",
    "        if is_window:\n",
    "            # For window around an event (±window_size seconds)\n",
    "            idxs = row[start_idx_col].squeeze() - 1\n",
    "            extracted = events[idxs]\n",
    "            center_times = extracted[:, 0]\n",
    "            window_start = center_times - window_size * 1e6  # window_size before in microseconds\n",
    "            window_end = center_times + window_size * 1e6    # window_size after in microseconds\n",
    "            combined = np.column_stack((window_start, window_end))\n",
    "        else:\n",
    "            # For regular epochs between two events\n",
    "            idxs_start = row[start_idx_col].squeeze() - 1   # Ensure indices are 1D array; start with 0\n",
    "            idxs_end = row[end_idx_col].squeeze() - 1   # Use specified end index\n",
    "            \n",
    "            # Handle case where start and end indices have different lengths\n",
    "            min_length = min(len(idxs_start), len(idxs_end))\n",
    "            idxs_start = idxs_start[:min_length]\n",
    "            idxs_end = idxs_end[:min_length]\n",
    "            \n",
    "            # Index into events using the adjusted indices\n",
    "            extracted_start = events[idxs_start]   # shape (n_trials, 3)\n",
    "            extracted_end = events[idxs_end]   # shape (n_trials, 3)\n",
    "\n",
    "            # Store as event start/end times\n",
    "            combined = np.column_stack((extracted_start[:, 0], extracted_end[:, 0]))\n",
    "        \n",
    "        epoch_ts[unit_id] = combined\n",
    "        \n",
    "    return epoch_ts\n",
    "\n",
    "# Get event timestamps for a specific epoch\n",
    "def compute_firing_rates(df_sample_new, start_idx_col='idxEnc1', end_idx_col='idxDel1', \n",
    "                         fr_prefix='fr', is_window=False, window_size=0.5):\n",
    "    # Extract timestamps for the specified epoch\n",
    "    epoch_ts = extract_event_timestamps(df_sample_new, start_idx_col, end_idx_col, is_window, window_size)\n",
    "    \n",
    "    # Calculate baseline firing rate (between idxEnc1-1 and idxEnc1) if this is the first call\n",
    "    if 'fr_baseline' not in df_sample_new.columns:\n",
    "        # Create a baseline period 1 second before idxEnc1\n",
    "        \n",
    "        # First get the encoding timestamps\n",
    "        enc_ts = extract_event_timestamps(df_sample_new, 'idxEnc1', 'idxEnc1')\n",
    "        \n",
    "        # Create baseline timestamps 1 second before encoding\n",
    "        baseline_ts = {}\n",
    "        for unit_id, timestamps in enc_ts.items():\n",
    "            # For each trial, create a 1-second window ending at the encoding start\n",
    "            baseline_start = timestamps[:, 0] - 1e6  # 1 second before in microseconds\n",
    "            baseline_end = timestamps[:, 0]  # End at encoding start\n",
    "            baseline_ts[unit_id] = np.column_stack((baseline_start, baseline_end))\n",
    "        df_sample_new['fr_baseline'] = df_sample_new.apply(\n",
    "            lambda row: [\n",
    "                np.sum((np.ravel(row[\"timestamps\"]) >= baseline_on) & \n",
    "                       (np.ravel(row[\"timestamps\"]) < baseline_off)) / ((baseline_off - baseline_on) / 1e6)\n",
    "                for baseline_on, baseline_off in baseline_ts[row[\"unit_id\"]]\n",
    "            ],\n",
    "            axis=1\n",
    "        )\n",
    "    \n",
    "    # Stimulus period: from stimulus onset to end of specified epoch\n",
    "    epoch_col = f\"{fr_prefix}_epoch\"\n",
    "    df_sample_new[epoch_col] = df_sample_new.apply(\n",
    "        lambda row: [\n",
    "            np.sum((np.ravel(row[\"timestamps\"]) >= epoch_on) & \n",
    "                   (np.ravel(row[\"timestamps\"]) < epoch_off)) / ((epoch_off - epoch_on) / 1e6)\n",
    "            for epoch_on, epoch_off in epoch_ts[row[\"unit_id\"]]\n",
    "        ],\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Add trial_nr column to track trial numbers if not already present\n",
    "    if \"trial_nr\" not in df_sample_new.columns:\n",
    "        df_sample_new[\"trial_nr\"] = df_sample_new[epoch_col].apply(lambda x: np.arange(len(x)))\n",
    "    \n",
    "    return df_sample_new\n",
    "\n",
    "# Compute firing rates for different epochs\n",
    "# Encoding 1 period\n",
    "df_sample_new = compute_firing_rates(df_sample_new, 'idxEnc1', 'idxDel1')\n",
    "\n",
    "# Delay 1 period\n",
    "df_sample_new = compute_firing_rates(df_sample_new, 'idxDel1', 'idxEnc2', fr_prefix='fr_del1')\n",
    "\n",
    "# Encoding 2 period\n",
    "df_sample_new = compute_firing_rates(df_sample_new, 'idxEnc2', 'idxDel2', fr_prefix='fr_enc2')\n",
    "\n",
    "# Delay 2 period\n",
    "df_sample_new = compute_firing_rates(df_sample_new, 'idxDel2', 'idxProbeOn', fr_prefix='fr_del2')\n",
    "\n",
    "# Response period (±0.5s window around response)\n",
    "df_sample_new = compute_firing_rates(df_sample_new, 'idxResp', None, fr_prefix='fr_resp', is_window=True, window_size=0.5)\n",
    "\n",
    "# Now explode the dataframe after all calculations are done\n",
    "columns_to_explode = ['fr_baseline', 'fr_epoch', 'fr_del1_epoch', 'fr_enc2_epoch', 'fr_del2_epoch', 'fr_resp_epoch', \"trial_nr\"]\n",
    "df_sample_new = df_sample_new.explode(columns_to_explode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2060c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge neural data with trial information\n",
    "df_sample_new = df_sample_new.reset_index(drop=True)\n",
    "trial_info = trial_info.reset_index(drop=True)\n",
    "\n",
    "data = pd.merge(\n",
    "    df_sample_new,\n",
    "    trial_info,\n",
    "    on=[\"unit_id\", \"trial_nr\"],\n",
    "    how=\"left\",\n",
    ").infer_objects()\n",
    "\n",
    "print(f\"Merged data shape: {data.shape}\")\n",
    "print(f\"Available columns: {list(data.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7dba4618",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = [\n",
    "    \"unit_id\", \"timestamps\", \"brainAreaOfCell\", \"fr_epoch\", \"fr_baseline\", \"fr_del1_epoch\", \n",
    "    \"fr_enc2_epoch\", \"fr_del2_epoch\", \"fr_resp_epoch\", \"trial_nr\",\n",
    "    \"first_cat\", \"second_cat\", \"first_num\", \"second_num\",\n",
    "    \"first_pic\", \"second_pic\", \"probe_cat\", \"probe_pic\",\n",
    "    \"probe_validity\", \"probe_num\", \"correct_answer\",\n",
    "    \"rt\", \"acc\", \"key\", \"cat_comparison\", \"events\", \"nTrials\", \"Trials\", \"idxEnc1\", \"idxEnc2\", \"idxDel1\",\n",
    "    \"idxDel2\", \"idxProbeOn\", \"idxResp\", \"nrProcessed\", \"periods_Enc1\", \"periods_Enc2\", \"periods_Del1\",\n",
    "    \"periods_Del2\", \"periods_Probe\", \"periods_Resp\", \"prestimEnc\", \"prestimMaint\", \"prestimProbe\",\n",
    "    \"prestimButtonPress\", \"poststimEnc\", \"poststimMaint\", \"poststimProbe\", \"poststimButtonPress\", \"sessionIdx\", \"channel\", \"cellNr\", \"sessionID\", \"origClusterID\"\n",
    "]\n",
    "\n",
    "data_filtered = data[cols_to_keep].copy()\n",
    "print(f\"Filtered data shape: {data_filtered.shape}\")\n",
    "print(f\"Final dataset ready for analysis!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9579da77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to simpler, hashable values for categories and numbers\n",
    "data_filtered[\"first_cat_simple\"] = data_filtered[\"first_cat\"].apply(\n",
    "    lambda x: str(np.squeeze(x)) if isinstance(x, (list, np.ndarray)) else str(x)\n",
    ")\n",
    "data_filtered[\"second_cat_simple\"] = data_filtered[\"second_cat\"].apply(\n",
    "    lambda x: str(np.squeeze(x)) if isinstance(x, (list, np.ndarray)) else str(x)\n",
    ")\n",
    "data_filtered[\"first_num_simple\"] = data_filtered[\"first_num\"].apply(\n",
    "    lambda x: str(np.squeeze(x)) if isinstance(x, (list, np.ndarray)) else str(x)\n",
    ")\n",
    "data_filtered[\"second_num_simple\"] = data_filtered[\"second_num\"].apply(\n",
    "    lambda x: str(np.squeeze(x)) if isinstance(x, (list, np.ndarray)) else str(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf2bee2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Single Unit Selectivity Analysis\n",
    "\n",
    "### Overview\n",
    "\n",
    "Identify neurons that show selective responses to stimulus categories vs numerosities during different task epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3dcb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# % ttl values\n",
    "# c.marker.expstart        = 89;\n",
    "# c.marker.expend          = 90;\n",
    "# c.marker.fixOnset        = 10;\n",
    "# c.marker.pic1            = 1;\n",
    "# c.marker.delay1          = 2;\n",
    "# c.marker.pic2            = 3;\n",
    "# c.marker.delay2          = 4;\n",
    "# c.marker.probeOnset      = 5;\n",
    "# c.marker.response        = 6;\n",
    "# c.marker.break           = 91;\n",
    "\n",
    "# what names are in the df\n",
    "data_filtered.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c49b2e",
   "metadata": {},
   "source": [
    "### Neuron Selectivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b194cd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def identify_selective_neurons(data_filtered):\n",
    "    \"\"\"\n",
    "    Analyzes neural data to identify neurons that show selective responses to different stimulus categories and numerosities.\n",
    "    \n",
    "    This function performs three main analyses for each neuron:\n",
    "    1. First stimulus analysis: Tests if the neuron responds differently to different categories/numbers of the first stimulus\n",
    "    2. Second stimulus analysis: Tests if the neuron responds differently to different categories/numbers of the second stimulus\n",
    "    3. Position analysis: Tests if the neuron responds differently based on stimulus position (first vs second)\n",
    "    \n",
    "    Args:\n",
    "        data_filtered: DataFrame containing neural data with columns for firing rates, categories, and numerosities\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame containing statistical results for each neuron's selectivity\n",
    "    \"\"\"\n",
    "    selectivity_stats = []\n",
    "\n",
    "    for unit_id, unit_df in tqdm(data_filtered.groupby(\"unit_id\")):\n",
    "        unit_df = unit_df.copy()\n",
    "        \n",
    "        # Extract firing rates for first and second stimulus presentations\n",
    "        unit_df[\"fr_first\"] = unit_df[\"fr_epoch\"]\n",
    "        unit_df[\"fr_second\"] = unit_df[\"fr_enc2_epoch\"]\n",
    "\n",
    "        # Skip neurons with no variance in firing rates\n",
    "        if unit_df[\"fr_first\"].std() == 0 or unit_df[\"fr_second\"].std() == 0:\n",
    "            continue\n",
    "\n",
    "        # Analyze selectivity to first stimulus using ANOVA\n",
    "        # Tests if firing rate varies with category or number of first stimulus\n",
    "        model_first = smf.ols(\n",
    "            \"fr_first ~ C(first_cat_simple) + C(first_num_simple)\", \n",
    "            data=unit_df\n",
    "        ).fit()\n",
    "        anova_first = sm.stats.anova_lm(model_first, typ=2)\n",
    "\n",
    "        # Analyze selectivity to second stimulus using ANOVA\n",
    "        # Tests if firing rate varies with category or number of second stimulus\n",
    "        model_second = smf.ols(\n",
    "            \"fr_second ~ C(second_cat_simple) + C(second_num_simple)\", \n",
    "            data=unit_df\n",
    "        ).fit()\n",
    "        anova_second = sm.stats.anova_lm(model_second, typ=2)\n",
    "\n",
    "        # Analyze position selectivity\n",
    "        # Combines data from both stimuli to test if firing rate varies with stimulus position\n",
    "        position_df = pd.DataFrame({\n",
    "            'fr': np.concatenate([unit_df['fr_first'], unit_df['fr_second']]),\n",
    "            'category': np.concatenate([unit_df['first_cat_simple'], unit_df['second_cat_simple']]),\n",
    "            'number': np.concatenate([unit_df['first_num_simple'], unit_df['second_num_simple']]),\n",
    "            'position': ['first'] * len(unit_df) + ['second'] * len(unit_df)\n",
    "        })\n",
    "\n",
    "        model_position = smf.ols(\n",
    "            \"fr ~ C(category) + C(number) + C(position)\", \n",
    "            data=position_df\n",
    "        ).fit()\n",
    "        anova_position = sm.stats.anova_lm(model_position, typ=2)\n",
    "\n",
    "        # Compile statistical results for this neuron\n",
    "        stats = {\n",
    "            'unit_id': unit_id,\n",
    "            'area': unit_df['brainAreaOfCell'].iloc[0],\n",
    "            # P-values for category and number selectivity in first stimulus\n",
    "            'first_cat_pvalue': anova_first.loc['C(first_cat_simple)', 'PR(>F)'],\n",
    "            'first_num_pvalue': anova_first.loc['C(first_num_simple)', 'PR(>F)'],\n",
    "            # P-values for category and number selectivity in second stimulus\n",
    "            'second_cat_pvalue': anova_second.loc['C(second_cat_simple)', 'PR(>F)'],\n",
    "            'second_num_pvalue': anova_second.loc['C(second_num_simple)', 'PR(>F)'],\n",
    "            # P-value for position selectivity\n",
    "            'position_pvalue': anova_position.loc['C(position)', 'PR(>F)'],\n",
    "            # Binary flags indicating significant selectivity (p < 0.05)\n",
    "            'is_first_cat_selective': anova_first.loc['C(first_cat_simple)', 'PR(>F)'] < 0.05,\n",
    "            'is_first_num_selective': anova_first.loc['C(first_num_simple)', 'PR(>F)'] < 0.05,\n",
    "            'is_second_cat_selective': anova_second.loc['C(second_cat_simple)', 'PR(>F)'] < 0.05,\n",
    "            'is_second_num_selective': anova_second.loc['C(second_num_simple)', 'PR(>F)'] < 0.05,\n",
    "            'is_position_selective': anova_position.loc['C(position)', 'PR(>F)'] < 0.05,\n",
    "            # R-squared values indicating model fit\n",
    "            'r2_first': model_first.rsquared,\n",
    "            'r2_second': model_second.rsquared,\n",
    "            'r2_position': model_position.rsquared,\n",
    "            # Overall selectivity flag\n",
    "            'is_any_selective': (\n",
    "                (anova_first.loc['C(first_cat_simple)', 'PR(>F)'] < 0.05) or\n",
    "                (anova_first.loc['C(first_num_simple)', 'PR(>F)'] < 0.05) or\n",
    "                (anova_second.loc['C(second_cat_simple)', 'PR(>F)'] < 0.05) or\n",
    "                (anova_second.loc['C(second_num_simple)', 'PR(>F)'] < 0.05) or\n",
    "                (anova_position.loc['C(position)', 'PR(>F)'] < 0.05)\n",
    "            )\n",
    "        }\n",
    "        selectivity_stats.append(stats)\n",
    "\n",
    "    results_df = pd.DataFrame(selectivity_stats)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a78af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the selectivity analysis\n",
    "selectivity_results = identify_selective_neurons(data_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f558e18",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## 3. Delay Activity & Distractor Resistance Analysis\n",
    "\n",
    "### Principled Distractor Resistance Analysis\n",
    "\n",
    "This analysis identifies neurons that:\n",
    "1. Show category selectivity during encoding (Step 1)\n",
    "2. Maintain selectivity during delay period (Step 2) \n",
    "3. Resist interference from distractors (Step 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0aefac",
   "metadata": {},
   "source": [
    "### distractor resistance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365bc957",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# ---\n",
    "\n",
    "## 4. Visualization of Distractor-Resistant Neurons\n",
    "\n",
    "### Setup for Plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4dff7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== PRINCIPLED DISTRACTOR RESISTANCE ANALYSIS =====\n",
    "\n",
    "def find_distractor_resistant_neurons_principled(data_filtered, selectivity_results):\n",
    "    \"\"\"\n",
    "    Identify neurons that maintain stim1 category information through distraction.\n",
    "    \n",
    "    This implements a principled 3-step filtering approach:\n",
    "    1. Start with stim1 category-selective neurons (encoding period)\n",
    "    2. Filter to delay neurons (maintain selectivity in delay1)  \n",
    "    3. Filter to distractor-resistant neurons (maintain stim 1 selectivity in delay2)\n",
    "    \n",
    "    Args:\n",
    "        data_filtered: Trial-level neural data\n",
    "        selectivity_results: Output from identify_selective_neurons()\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with distractor resistance analysis for each stim1-selective neuron\n",
    "    \"\"\"\n",
    "    print(\"=== PRINCIPLED DISTRACTOR RESISTANCE ANALYSIS ===\")\n",
    "    \n",
    "    # Step 1: Start with neurons selective for stim1 category during encoding\n",
    "    stim1_selective = selectivity_results[selectivity_results[\"is_first_cat_selective\"]].copy()\n",
    "    print(f\"Step 1: Found {len(stim1_selective)} neurons selective for stim1 category during encoding\")\n",
    "    \n",
    "    if len(stim1_selective) == 0:\n",
    "        print(\"No stim1-selective neurons found!\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Analyze each stim1-selective neuron for distractor resistance\n",
    "    distractor_analysis = []\n",
    "    \n",
    "    for _, neuron_row in tqdm(stim1_selective.iterrows(), desc=\"Analyzing stim1-selective neurons\"):\n",
    "        unit_id = neuron_row['unit_id']\n",
    "        unit_df = data_filtered[data_filtered['unit_id'] == unit_id].copy()\n",
    "        \n",
    "        if len(unit_df) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Create variable for whether stim1 and stim2 categories match\n",
    "        unit_df['category_match'] = (unit_df['first_cat_simple'] == unit_df['second_cat_simple'])\n",
    "        \n",
    "        try:\n",
    "            # Step 2: Test if neuron maintains stim1 selectivity during delay1 (pre-distractor)\n",
    "            model_del1 = smf.ols(\"fr_del1_epoch ~ C(first_cat_simple)\", data=unit_df).fit()\n",
    "            anova_del1 = sm.stats.anova_lm(model_del1, typ=2)\n",
    "            del1_pval = anova_del1.loc['C(first_cat_simple)', 'PR(>F)']\n",
    "            is_delay_neuron = del1_pval < 0.05\n",
    "            \n",
    "            # Step 3: Test if neuron maintains stim1 selectivity during delay2 (post-distractor)\n",
    "            model_del2 = smf.ols(\"fr_del2_epoch ~ C(first_cat_simple)\", data=unit_df).fit()\n",
    "            anova_del2 = sm.stats.anova_lm(model_del2, typ=2)\n",
    "            del2_pval = anova_del2.loc['C(first_cat_simple)', 'PR(>F)']\n",
    "            is_distractor_resistant = del2_pval < 0.05\n",
    "            \n",
    "            # Test for stim1 selectivity in different trial types\n",
    "            same_cat_trials = unit_df[unit_df['category_match']]\n",
    "            diff_cat_trials = unit_df[~unit_df['category_match']]\n",
    "            \n",
    "            # Analyze same-category trials\n",
    "            same_cat_del2_pval = 1.0\n",
    "            if len(same_cat_trials) >= 10 and same_cat_trials['fr_del2_epoch'].std() > 0:\n",
    "                try:\n",
    "                    model_same = smf.ols(\"fr_del2_epoch ~ C(first_cat_simple)\", data=same_cat_trials).fit()\n",
    "                    anova_same = sm.stats.anova_lm(model_same, typ=2)\n",
    "                    same_cat_del2_pval = anova_same.loc['C(first_cat_simple)', 'PR(>F)']\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # Analyze different-category trials\n",
    "            diff_cat_del2_pval = 1.0\n",
    "            if len(diff_cat_trials) >= 10 and diff_cat_trials['fr_del2_epoch'].std() > 0:\n",
    "                try:\n",
    "                    model_diff = smf.ols(\"fr_del2_epoch ~ C(first_cat_simple)\", data=diff_cat_trials).fit()\n",
    "                    anova_diff = sm.stats.anova_lm(model_diff, typ=2)\n",
    "                    diff_cat_del2_pval = anova_diff.loc['C(first_cat_simple)', 'PR(>F)']\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # Calculate correlation between delay1 and delay2 response patterns\n",
    "            del1_means = unit_df.groupby('first_cat_simple')['fr_del1_epoch'].mean()\n",
    "            del2_means = unit_df.groupby('first_cat_simple')['fr_del2_epoch'].mean()\n",
    "            \n",
    "            correlation = 0\n",
    "            if len(del1_means) > 1 and len(del2_means) > 1:\n",
    "                correlation = np.corrcoef(del1_means.values, del2_means.values)[0,1]\n",
    "            \n",
    "            # Calculate pattern similarity between same and different category trials\n",
    "            same_cat_responses = {}\n",
    "            diff_cat_responses = {}\n",
    "            \n",
    "            for cat in unit_df['first_cat_simple'].unique():\n",
    "                same_cat_mask = same_cat_trials['first_cat_simple'] == cat\n",
    "                diff_cat_mask = diff_cat_trials['first_cat_simple'] == cat\n",
    "                \n",
    "                same_cat_responses[cat] = same_cat_trials[same_cat_mask]['fr_del2_epoch'].mean() if same_cat_mask.any() else np.nan\n",
    "                diff_cat_responses[cat] = diff_cat_trials[diff_cat_mask]['fr_del2_epoch'].mean() if diff_cat_mask.any() else np.nan\n",
    "            \n",
    "            # Calculate pattern similarity\n",
    "            valid_cats = [cat for cat in same_cat_responses.keys() \n",
    "                         if not np.isnan(same_cat_responses[cat]) and not np.isnan(diff_cat_responses[cat])]\n",
    "            \n",
    "            pattern_similarity = 0\n",
    "            if len(valid_cats) > 1:\n",
    "                same_pattern = [same_cat_responses[cat] for cat in valid_cats]\n",
    "                diff_pattern = [diff_cat_responses[cat] for cat in valid_cats]\n",
    "                if np.std(same_pattern) > 0 and np.std(diff_pattern) > 0:\n",
    "                    pattern_similarity = np.corrcoef(same_pattern, diff_pattern)[0,1]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing unit {unit_id}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Compile results for this neuron\n",
    "        result = {\n",
    "            'unit_id': unit_id,  # Unique identifier for each neuron\n",
    "            'area': unit_df['brainAreaOfCell'].iloc[0],  # Brain area where neuron was recorded\n",
    "            'stim1_encoding_pval': neuron_row['first_cat_pvalue'],  # P-value for stim1 category selectivity during encoding\n",
    "            'delay1_stim1_pval': del1_pval,  # P-value for stim1 category selectivity during delay1\n",
    "            'is_delay_neuron': is_delay_neuron,  # Boolean indicating if neuron maintains selectivity in delay1\n",
    "            'delay2_stim1_pval': del2_pval,  # P-value for stim1 category selectivity during delay2\n",
    "            'is_distractor_resistant': is_distractor_resistant,  # Boolean indicating if neuron maintains selectivity in delay2\n",
    "            'delay2_same_cat_pval': same_cat_del2_pval,  # P-value for stim1 selectivity in same-category trials during delay2\n",
    "            'delay2_diff_cat_pval': diff_cat_del2_pval,  # P-value for stim1 selectivity in different-category trials during delay2\n",
    "            'pattern_similarity': pattern_similarity,  # Correlation between response patterns in same vs different category trials\n",
    "            'delay1_delay2_correlation': correlation,  # Correlation between delay1 and delay2 response patterns\n",
    "            'n_same_cat_trials': len(same_cat_trials),  # Number of trials where stim1 and stim2 categories match\n",
    "            'n_diff_cat_trials': len(diff_cat_trials),  # Number of trials where stim1 and stim2 categories differ\n",
    "            'analysis_stage': (  # Classification of neuron based on selectivity across epochs\n",
    "                'encoding_only' if not is_delay_neuron else\n",
    "                'delay_only' if not is_distractor_resistant else  \n",
    "                'distractor_resistant'\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        distractor_analysis.append(result)\n",
    "    \n",
    "    results_df = pd.DataFrame(distractor_analysis)\n",
    "    \n",
    "    # Print analysis summary\n",
    "    if len(results_df) > 0:\n",
    "        print(f\"\\nStep 2: {results_df['is_delay_neuron'].sum()}/{len(results_df)} are delay neurons\")\n",
    "        print(f\"Step 3: {results_df['is_distractor_resistant'].sum()}/{len(results_df)} are distractor resistant\")  \n",
    "        \n",
    "        print(f\"\\nBreakdown by analysis stage:\")\n",
    "        stage_counts = results_df['analysis_stage'].value_counts()\n",
    "        for stage, count in stage_counts.items():\n",
    "            print(f\"  {stage}: {count}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Execute the principled distractor resistance analysis\n",
    "print(\"Running principled distractor resistance analysis...\")\n",
    "distractor_results_principled = find_distractor_resistant_neurons_principled(data_filtered, selectivity_results)\n",
    "\n",
    "# Extract the distractor-resistant neurons\n",
    "distractor_resistant_neurons = distractor_results_principled[\n",
    "    distractor_results_principled['is_distractor_resistant']\n",
    "].copy()\n",
    "\n",
    "print(f\"\\n=== DISTRACTOR-RESISTANT NEURONS ===\")\n",
    "if len(distractor_resistant_neurons) > 0:\n",
    "    print(f\"Found {len(distractor_resistant_neurons)} distractor-resistant neurons:\")\n",
    "    for _, row in distractor_resistant_neurons.iterrows():\n",
    "        print(f\"  Unit {row['unit_id']} ({row['area']}) - Pattern similarity: {row['pattern_similarity']:.3f}\")\n",
    "    \n",
    "    # Save complete analysis results\n",
    "    distractor_results_principled.to_csv(\"principled_distractor_analysis.csv\", index=False)\n",
    "    distractor_resistant_neurons.to_csv(\"distractor_resistant_neurons.csv\", index=False)\n",
    "    print(\"Results saved to CSV files\")\n",
    "    \n",
    "else:\n",
    "    print(\"No distractor-resistant neurons found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03e13926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural analysis package setup and function definitions\n",
    "# Install with: pip install git+https://github.com/ioqfwfq/rlab_neural_analysis.git@jz\n",
    "try:\n",
    "    from neural_analysis.visualize import plot_spikes_with_PSTH\n",
    "    from neural_analysis.spikes import get_spikes\n",
    "    NEURAL_ANALYSIS_AVAILABLE = True\n",
    "    print(\"✅ Neural analysis package loaded successfully\")\n",
    "except ImportError:\n",
    "    NEURAL_ANALYSIS_AVAILABLE = False\n",
    "    print(\"⚠️ Warning: neural_analysis package not found.\")\n",
    "    print(\"Install with: pip install git+https://github.com/ioqfwfq/rlab_neural_analysis.git@jz\")\n",
    "    print(\"Some visualization functions will be skipped without this package.\")\n",
    "    \n",
    "    # Define a dummy function to prevent errors\n",
    "    def plot_spikes_with_PSTH(*args, **kwargs):\n",
    "        print(\"plot_spikes_with_PSTH is not available - neural_analysis package not installed\")\n",
    "        return None, None\n",
    "\n",
    "# Define extract_event_timestamps function if not available\n",
    "def extract_event_timestamps(df_sample_new, start_idx_col='idxEnc1', is_window=True, window_size=0):\n",
    "    \"\"\"\n",
    "    Extract event timestamps for alignment.\n",
    "    Returns a dictionary mapping unit_id to timestamp arrays.\n",
    "    \"\"\"\n",
    "    epoch_ts = {}\n",
    "    for unit_id, unit_df in df_sample_new.groupby(\"unit_id\"):\n",
    "        # Extract alignment timestamps from the specified column\n",
    "        timestamps = []\n",
    "        for idx, row in unit_df.iterrows():\n",
    "            if start_idx_col in row and pd.notna(row[start_idx_col]):\n",
    "                ts = row[start_idx_col]\n",
    "                if isinstance(ts, (list, np.ndarray)):\n",
    "                    ts = np.squeeze(ts)\n",
    "                timestamps.append(ts)\n",
    "        \n",
    "        if timestamps:\n",
    "            epoch_ts[unit_id] = np.array(timestamps).reshape(-1, 1)\n",
    "        else:\n",
    "            epoch_ts[unit_id] = np.array([]).reshape(0, 1)\n",
    "    \n",
    "    print(f\"Created epoch timestamps for {len(epoch_ts)} units\")\n",
    "    return epoch_ts\n",
    "\n",
    "# Create epoch timestamps for plotting\n",
    "print(\"Creating epoch timestamps...\")\n",
    "epoch_ts = extract_event_timestamps(\n",
    "    df_sample_new=data_filtered,\n",
    "    start_idx_col='idxEnc1',\n",
    "    is_window=True,\n",
    "    window_size=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325ba247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== PLOT STEP 3 DISTRACTOR-RESISTANT NEURONS =====\n",
    "\n",
    "# Make sure output directory exists\n",
    "os.makedirs(\"step3_distractor_plots\", exist_ok=True)\n",
    "\n",
    "# Get neurons that passed step 3 (distractor-resistant) \n",
    "if 'distractor_results_principled' in locals():\n",
    "    step3_neurons = distractor_results_principled[\n",
    "        distractor_results_principled['is_distractor_resistant']\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"Found {len(step3_neurons)} step 3 distractor-resistant neurons\")\n",
    "    \n",
    "    if len(step3_neurons) > 0:\n",
    "        # Use all step3 neurons, sorted by strongest delay2 selectivity (lowest p-value)\n",
    "        all_step3 = step3_neurons.sort_values('delay2_stim1_pval')\n",
    "        all_step3_ids = all_step3['unit_id'].tolist()\n",
    "        \n",
    "        print(f\"All {len(all_step3)} step 3 neurons by delay2 selectivity:\")\n",
    "        for _, row in all_step3.iterrows():\n",
    "            print(f\"  Unit {row['unit_id']} ({row['area']}) - Delay2 p={row['delay2_stim1_pval']:.4f}\")\n",
    "        \n",
    "        # Plot each neuron\n",
    "        for unit_id in all_step3_ids:\n",
    "            print(f\"\\nPlotting Unit {unit_id}...\")\n",
    "            \n",
    "            df_unit = data_filtered[data_filtered[\"unit_id\"] == unit_id].reset_index(drop=True)\n",
    "            if df_unit.empty or unit_id not in epoch_ts:\n",
    "                print(f\"  Skipping unit {unit_id} - no data\")\n",
    "                continue\n",
    "            \n",
    "            # Get unit info\n",
    "            unit_stats = step3_neurons[step3_neurons['unit_id'] == unit_id].iloc[0]\n",
    "            area = unit_stats['area']\n",
    "            encoding_pval = unit_stats['stim1_encoding_pval']\n",
    "            delay1_pval = unit_stats['delay1_stim1_pval'] \n",
    "            delay2_pval = unit_stats['delay2_stim1_pval']\n",
    "            correlation = unit_stats['delay1_delay2_correlation']\n",
    "            \n",
    "            try:\n",
    "                spikes = np.asarray(df_unit[\"timestamps\"].iloc[0]).flatten().astype(np.float64) / 1e6\n",
    "                spikes = np.sort(spikes)\n",
    "                \n",
    "                # Group by stim1 category\n",
    "                group_labels = df_unit['first_cat_simple'].apply(\n",
    "                    lambda x: np.squeeze(x).item() if isinstance(x, (list, np.ndarray)) else x\n",
    "                )\n",
    "                \n",
    "                # Use stim1 numerosity for additional info\n",
    "                stats = df_unit['first_num_simple'].apply(\n",
    "                    lambda x: np.squeeze(x).item() if isinstance(x, (list, np.ndarray)) else x\n",
    "                )\n",
    "                \n",
    "                alignments = np.asarray(epoch_ts[unit_id][:, 0], dtype=np.float64) / 1e6\n",
    "                \n",
    "                # Plot spikes with PSTH (if neural_analysis package is available)\n",
    "                if NEURAL_ANALYSIS_AVAILABLE:\n",
    "                    axes = plot_spikes_with_PSTH(\n",
    "                        spikes,\n",
    "                        alignments,\n",
    "                        window=(-1, 8),\n",
    "                        group_labels=group_labels,\n",
    "                        stats=stats,\n",
    "                        plot_stats=False,\n",
    "                        sig_test=True,\n",
    "                        cmap=\"Set1\",\n",
    "                    )\n",
    "                else:\n",
    "                    print(f\"  Skipping plot for unit {unit_id} - neural_analysis package not available\")\n",
    "                    continue\n",
    "                \n",
    "                if axes is None:\n",
    "                    print(f\"  Failed to create plot for unit {unit_id}\")\n",
    "                    continue\n",
    "                \n",
    "                # Add baseline\n",
    "                unit_baseline = np.mean(df_unit[\"fr_baseline\"])\n",
    "                xmin, xmax = axes[1].get_xlim()\n",
    "                axes[1].hlines(\n",
    "                    y=unit_baseline,\n",
    "                    xmin=xmin,\n",
    "                    xmax=xmax,\n",
    "                    colors=\"gray\",\n",
    "                    linestyles=\"--\",\n",
    "                    alpha=0.7,\n",
    "                    label=f\"baseline = {unit_baseline:.1f}\"\n",
    "                )\n",
    "                \n",
    "                # Add task event lines\n",
    "                event_times = [1, 2, 3, 5.5]\n",
    "                event_labels = ['Stim1', 'Delay1', 'Stim2', 'Delay2']\n",
    "                \n",
    "                for v, label in zip(event_times, event_labels):\n",
    "                    for ax in axes:\n",
    "                        ax.axvline(x=v, color=\"black\", linestyle=\"--\", linewidth=1, alpha=0.8)\n",
    "                    axes[0].text(v, axes[0].get_ylim()[1]*0.95, label, \n",
    "                               rotation=90, ha='right', va='top', fontsize=8)\n",
    "                \n",
    "                # Highlight delay2 period\n",
    "                for ax in axes:\n",
    "                    ax.axvspan(3, 5.5, alpha=0.2, color='yellow', label='Delay2')\n",
    "                \n",
    "                # Title with statistics\n",
    "                axes[0].set_title(\n",
    "                    f\"{area} Unit {unit_id} — Distractor-Resistant Neuron\\n\"\n",
    "                    f\"Encoding p={encoding_pval:.4f}, Delay1 p={delay1_pval:.4f}, \"\n",
    "                    f\"Delay2 p={delay2_pval:.4f}\\n\"\n",
    "                    f\"Delay1-Delay2 correlation: {correlation:.3f}\"\n",
    "                )\n",
    "                \n",
    "                axes[1].set_xlabel(\"Time from Stim1 onset [s]\")\n",
    "                axes[1].set_ylabel(\"Firing Rate (Hz)\")\n",
    "                \n",
    "                # Save figure\n",
    "                fname = f\"step3_distractor_plots/{area}_Unit{unit_id}_distractor_resistant.png\"\n",
    "                plt.savefig(fname, dpi=300, bbox_inches=\"tight\")\n",
    "                plt.close()\n",
    "                print(f\"  Saved: {fname}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Error plotting unit {unit_id}: {e}\")\n",
    "    \n",
    "    print(f\"\\nCompleted plotting all step 3 distractor-resistant neurons\")\n",
    "    \n",
    "else:\n",
    "    print(\"Need to run the principled distractor analysis first!\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Retroactive Interference Analysis\n",
    "\n",
    "Analyze how stim2 affects maintenance of stim1 information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bca8421",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Summary of distractor-resistant neurons found\n",
    "if 'distractor_results_principled' in locals():\n",
    "    print(f\"\\n=== DISTRACTOR RESISTANCE SUMMARY ===\")\n",
    "    step1_count = len(distractor_results_principled)\n",
    "    step2_count = distractor_results_principled['is_delay_neuron'].sum()\n",
    "    step3_count = distractor_results_principled['is_distractor_resistant'].sum()\n",
    "    \n",
    "    print(f\"Step 1 (Stim1-selective): {step1_count} neurons\")\n",
    "    print(f\"Step 2 (Delay neurons): {step2_count} neurons ({step2_count/step1_count*100:.1f}%)\")\n",
    "    print(f\"Step 3 (Distractor-resistant): {step3_count} neurons ({step3_count/step1_count*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"Run the principled distractor analysis first to get results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285988b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ADDITIONAL PLOTTING FUNCTIONS =====\n",
    "# Note: Advanced plotting functions require the neural_analysis package\n",
    "# Install with: pip install git+https://github.com/ioqfwfq/rlab_neural_analysis.git@jz\n",
    "\n",
    "print(\"Advanced plotting functions available when neural_analysis package is installed\")\n",
    "print(\"Run the core analysis (cells 1-32) first to get basic results\")\n",
    "\n",
    "# Basic analysis summary function that works without additional packages\n",
    "def summarize_results():\n",
    "    \"\"\"Provide a summary of the analysis results\"\"\"\n",
    "    if 'selectivity_results' in locals() or 'selectivity_results' in globals():\n",
    "        print(\"✅ Selectivity analysis completed\")\n",
    "        print(f\"   Found {len(selectivity_results)} responsive neurons\")\n",
    "        \n",
    "    if 'distractor_results_principled' in locals() or 'distractor_results_principled' in globals():\n",
    "        print(\"✅ Distractor resistance analysis completed\") \n",
    "        print(f\"   Found {distractor_results_principled['is_distractor_resistant'].sum()} distractor-resistant neurons\")\n",
    "    else:\n",
    "        print(\"⚠️ Run distractor resistance analysis first\")\n",
    "        \n",
    "    print(\"\\nTo run advanced plotting:\")\n",
    "    print(\"1. Install neural_analysis package\")\n",
    "    print(\"2. Run plotting cells (34+)\")\n",
    "\n",
    "summarize_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1710ac25",
   "metadata": {},
   "source": [
    "### proactive interference"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Proactive Interference Analysis\n",
    "\n",
    "Analyze how stim1 affects maintenance of stim2 information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cad3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== STIM2-TUNED TRIALS GROUPED BY STIM1 CATEGORY (PROACTIVE INTERFERENCE) =====\n",
    "\n",
    "# Make sure output directory exists\n",
    "os.makedirs(\"stim2_tuned_by_stim1_plots\", exist_ok=True)\n",
    "\n",
    "def plot_stim2_tuned_by_stim1(data_filtered, step3_neurons, unit_ids, epoch_ts):\n",
    "    \"\"\"\n",
    "    Test proactive interference: How does stim1 affect stim2 maintenance?\n",
    "    \n",
    "    For each neuron:\n",
    "    1. Find the neuron's best stim2 category (highest response during encoding2)\n",
    "    2. Filter to trials where stim2 was that best category\n",
    "    3. Group those trials by stim1 category (the \"proactive distractor\")\n",
    "    4. Plot delay2 activity to see if stim1 interferes with stim2 maintenance\n",
    "    \n",
    "    This is the reverse of the previous analysis - now stim1 is the \"distractor\"\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Analyzing proactive interference (stim1 → stim2) for {len(unit_ids)} neurons...\")\n",
    "    \n",
    "    for unit_id in unit_ids:\n",
    "        print(f\"\\nAnalyzing Unit {unit_id} for proactive interference...\")\n",
    "        \n",
    "        try:\n",
    "            df_unit = data_filtered[data_filtered[\"unit_id\"] == unit_id].reset_index(drop=True)\n",
    "            if df_unit.empty or unit_id not in epoch_ts:\n",
    "                print(f\"  Skipping unit {unit_id} - no data\")\n",
    "                continue\n",
    "            \n",
    "            # Get unit info\n",
    "            unit_stats = step3_neurons[step3_neurons['unit_id'] == unit_id].iloc[0]\n",
    "            area = unit_stats['area']\n",
    "            \n",
    "            # Step 1: Find the neuron's best stim2 category\n",
    "            # Use encoding2 responses to determine best stim2 category\n",
    "            stim2_responses = df_unit.groupby('second_cat_simple')['fr_enc2_epoch'].mean()\n",
    "            best_stim2_category = stim2_responses.idxmax()\n",
    "            best_stim2_response = stim2_responses.max()\n",
    "            \n",
    "            print(f\"  Best stim2 category: {best_stim2_category} (mean response: {best_stim2_response:.2f} Hz)\")\n",
    "            print(f\"  All stim2 responses: {stim2_responses.to_dict()}\")\n",
    "            \n",
    "            # Step 2: Filter to trials where stim2 was the best category\n",
    "            best_stim2_trials = df_unit[df_unit['second_cat_simple'] == best_stim2_category].copy()\n",
    "            \n",
    "            if len(best_stim2_trials) < 10:\n",
    "                print(f\"  Insufficient trials with best stim2 category ({len(best_stim2_trials)}) - skipping\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"  Trials with best stim2 category: {len(best_stim2_trials)}\")\n",
    "            \n",
    "            # Step 3: Group by stim1 category (the proactive distractor)\n",
    "            stim1_counts = best_stim2_trials['first_cat_simple'].value_counts()\n",
    "            print(f\"  Stim1 category distribution: {stim1_counts.to_dict()}\")\n",
    "            \n",
    "            # Check if we have enough trials in each stim1 category\n",
    "            min_trials_per_stim1 = 3\n",
    "            valid_stim1_cats = stim1_counts[stim1_counts >= min_trials_per_stim1].index.tolist()\n",
    "            \n",
    "            if len(valid_stim1_cats) < 2:\n",
    "                print(f\"  Insufficient stim1 category diversity - skipping\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"  Valid stim1 categories (≥{min_trials_per_stim1} trials): {valid_stim1_cats}\")\n",
    "            \n",
    "            # Filter to only trials with valid stim1 categories\n",
    "            plot_trials = best_stim2_trials[best_stim2_trials['first_cat_simple'].isin(valid_stim1_cats)].copy()\n",
    "            \n",
    "            # Step 4: Plot using existing infrastructure\n",
    "            spikes = np.asarray(df_unit[\"timestamps\"].iloc[0]).flatten().astype(np.float64) / 1e6\n",
    "            spikes = np.sort(spikes)\n",
    "            \n",
    "            # Get alignments for the filtered trials\n",
    "            all_alignments = np.asarray(epoch_ts[unit_id][:, 0], dtype=np.float64) / 1e6\n",
    "            plot_alignments = all_alignments[plot_trials.index.values]\n",
    "            \n",
    "            # Group labels = stim1 categories (the proactive distractors)\n",
    "            group_labels = plot_trials['first_cat_simple'].apply(\n",
    "                lambda x: np.squeeze(x).item() if isinstance(x, (list, np.ndarray)) else x\n",
    "            )\n",
    "            \n",
    "            # Stats = stim2 numerosity for additional info\n",
    "            stats = plot_trials['second_num_simple'].apply(\n",
    "                lambda x: np.squeeze(x).item() if isinstance(x, (list, np.ndarray)) else x\n",
    "            )\n",
    "            \n",
    "            # Use vibrant colormap - different from previous analysis\n",
    "            axes = plot_spikes_with_PSTH(\n",
    "                spikes,\n",
    "                plot_alignments,\n",
    "                window=(-1, 8),\n",
    "                group_labels=group_labels,  # Groups by stim1 category (proactive distractor)\n",
    "                stats=stats,\n",
    "                plot_stats=False,\n",
    "                sig_test=True,\n",
    "                cmap=\"viridis\",  # Different colormap to distinguish this analysis\n",
    "            )\n",
    "            \n",
    "            # Add baseline\n",
    "            unit_baseline = np.mean(df_unit[\"fr_baseline\"])\n",
    "            xmin, xmax = axes[1].get_xlim()\n",
    "            axes[1].hlines(\n",
    "                y=unit_baseline,\n",
    "                xmin=xmin,\n",
    "                xmax=xmax,\n",
    "                colors=\"gray\",\n",
    "                linestyles=\"--\",\n",
    "                alpha=0.7,\n",
    "                label=f\"baseline = {unit_baseline:.1f}\"\n",
    "            )\n",
    "            \n",
    "            # Add task event lines (no labels for cleaner appearance)\n",
    "            event_times = [1, 2, 3, 5.5]\n",
    "            for v in event_times:\n",
    "                for ax in axes:\n",
    "                    ax.axvline(x=v, color=\"black\", linestyle=\"--\", linewidth=1, alpha=0.8)\n",
    "            \n",
    "            # Highlight key periods with different color scheme\n",
    "            for ax in axes:\n",
    "                ax.axvspan(0, 1, alpha=0.1, color='red', label='Stim1 (proactive distractor)')\n",
    "                ax.axvspan(2, 3, alpha=0.1, color='green', label='Stim2 (best category)')\n",
    "                ax.axvspan(3, 5.5, alpha=0.2, color='yellow', label='Delay2 (interference test)')\n",
    "            \n",
    "            # Title explaining the analysis\n",
    "            axes[0].set_title(\n",
    "                f\"{area} Unit {unit_id} — Best Stim2 Category: '{best_stim2_category}'\\n\"\n",
    "                f\"Grouped by Stim1 Category (Lines = different Stim1, n={len(plot_trials)} trials)\\n\"\n",
    "                f\"Question: Does Stim1 interfere with Stim2 maintenance? (Proactive interference)\"\n",
    "            )\n",
    "            \n",
    "            axes[1].set_xlabel(\"Time from Stim1 onset [s]\")\n",
    "            axes[1].set_ylabel(\"Firing Rate (Hz)\")\n",
    "            # axes[1].legend(loc='upper right', fontsize=8)\n",
    "            \n",
    "            # Save the figure\n",
    "            fname = f\"stim2_tuned_by_stim1_plots/{area}_Unit{unit_id}_best_stim2_{best_stim2_category}_by_stim1.png\"\n",
    "            plt.savefig(fname, dpi=300, bbox_inches=\"tight\")\n",
    "            plt.close()\n",
    "            print(f\"  Saved: {fname}\")\n",
    "            \n",
    "            # Detailed analysis of proactive interference\n",
    "            print(f\"  === Proactive Interference Analysis ===\")\n",
    "            \n",
    "            # Compare encoding2 responses by stim1 category\n",
    "            stim1_enc2_responses = plot_trials.groupby('first_cat_simple')['fr_enc2_epoch'].agg(['mean', 'sem', 'count'])\n",
    "            print(f\"  Stim2 encoding responses by stim1 category:\")\n",
    "            for stim1_cat, row in stim1_enc2_responses.iterrows():\n",
    "                print(f\"    Stim1={stim1_cat}: {row['mean']:.2f}±{row['sem']:.2f} Hz (n={row['count']})\")\n",
    "            \n",
    "            # Compare delay2 responses (key test for proactive interference)\n",
    "            stim1_del2_responses = plot_trials.groupby('first_cat_simple')['fr_del2_epoch'].agg(['mean', 'sem', 'count'])\n",
    "            print(f\"  Delay2 responses by stim1 category (proactive interference test):\")\n",
    "            for stim1_cat, row in stim1_del2_responses.iterrows():\n",
    "                print(f\"    Stim1={stim1_cat}: {row['mean']:.2f}±{row['sem']:.2f} Hz (n={row['count']})\")\n",
    "            \n",
    "            # Test if stim1 category affects stim2 maintenance (proactive interference)\n",
    "            if len(valid_stim1_cats) >= 2:\n",
    "                try:\n",
    "                    # Test effect on encoding2 period\n",
    "                    model_stim1_enc2 = smf.ols(\"fr_enc2_epoch ~ C(first_cat_simple)\", data=plot_trials).fit()\n",
    "                    anova_stim1_enc2 = sm.stats.anova_lm(model_stim1_enc2, typ=2)\n",
    "                    stim1_enc2_pval = anova_stim1_enc2.loc['C(first_cat_simple)', 'PR(>F)']\n",
    "                    \n",
    "                    # Test effect on delay2 period (main test for proactive interference)\n",
    "                    model_stim1_del2 = smf.ols(\"fr_del2_epoch ~ C(first_cat_simple)\", data=plot_trials).fit()\n",
    "                    anova_stim1_del2 = sm.stats.anova_lm(model_stim1_del2, typ=2)\n",
    "                    stim1_del2_pval = anova_stim1_del2.loc['C(first_cat_simple)', 'PR(>F)']\n",
    "                    \n",
    "                    print(f\"  Stim1 effect on stim2 encoding: p = {stim1_enc2_pval:.4f} {'*' if stim1_enc2_pval < 0.05 else ''}\")\n",
    "                    print(f\"  Stim1 effect on stim2 maintenance (delay2): p = {stim1_del2_pval:.4f} {'*' if stim1_del2_pval < 0.05 else ''}\")\n",
    "                    \n",
    "                    # Determine interference pattern\n",
    "                    if stim1_del2_pval < 0.05:\n",
    "                        print(f\"  → PROACTIVE INTERFERENCE DETECTED: Stim1 interferes with stim2 maintenance\")\n",
    "                    else:\n",
    "                        print(f\"  → No proactive interference: Stim2 maintenance resistant to stim1\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"  Could not test proactive interference: {e}\")\n",
    "            # Test if stim1 category affects stim2 maintenance (proactive interference)\\n\",\n",
    "            if len(valid_stim1_cats) >= 2:\n",
    "                try:\n",
    "                    # Test effect on encoding2 period\n",
    "                    model_stim1_enc2 = smf.ols(\"fr_enc2_epoch ~ C(first_cat_simple)\", data=plot_trials).fit()\n",
    "                    anova_stim1_enc2 = sm.stats.anova_lm(model_stim1_enc2, typ=2)\n",
    "                    stim1_enc2_pval = anova_stim1_enc2.loc['C(first_cat_simple)', 'PR(>F)']\n",
    "                    \n",
    "                    # Test effect on delay2 period (main test for proactive interference)\n",
    "                    model_stim1_del2 = smf.ols(\"fr_del2_epoch ~ C(first_cat_simple)\", data=plot_trials).fit()\n",
    "                    anova_stim1_del2 = sm.stats.anova_lm(model_stim1_del2, typ=2)\n",
    "                    stim1_del2_pval = anova_stim1_del2.loc['C(first_cat_simple)', 'PR(>F)']\n",
    "                    \n",
    "                    print(f\"  Stim1 effect on stim2 encoding: p = {stim1_enc2_pval:.4f} {'*' if stim1_enc2_pval < 0.05 else ''}\")\n",
    "                    print(f\"  Stim1 effect on stim2 maintenance (delay2): p = {stim1_del2_pval:.4f} {'*' if stim1_del2_pval < 0.05 else ''}\")\n",
    "                    \n",
    "                    # Determine interference pattern\n",
    "                    if stim1_del2_pval < 0.05:\n",
    "                        print(f\"  → PROACTIVE INTERFERENCE DETECTED: Stim1 interferes with stim2 maintenance\")\n",
    "                    else:\n",
    "                        print(f\"  → No proactive interference: Stim2 maintenance resistant to stim1\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"  Could not test proactive interference: {e}\")\n",
    "            # Compare with category congruence\\n\",\n",
    "            # Compare with category congruence\n",
    "            same_cat_mask = plot_trials['first_cat_simple'] == plot_trials['second_cat_simple']\n",
    "            if same_cat_mask.any() and (~same_cat_mask).any():\n",
    "                same_cat_del2 = plot_trials[same_cat_mask]['fr_del2_epoch'].mean()\n",
    "                diff_cat_del2 = plot_trials[~same_cat_mask]['fr_del2_epoch'].mean()\n",
    "                print(f\"  Same category (stim1=stim2): {same_cat_del2:.2f} Hz\")\n",
    "                print(f\"  Different categories: {diff_cat_del2:.2f} Hz\")\n",
    "                print(f\"  Category congruence effect: {same_cat_del2 - diff_cat_del2:.2f} Hz\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error analyzing unit {unit_id}: {e}\")\n",
    "    \n",
    "    print(f\"\\nCompleted proactive interference analysis for all step 3 neurons\")\n",
    "    print(\"Key insights to look for:\")\n",
    "    print(\"- Do colored lines (stim1 categories) affect delay2 responses when stim2 is optimal?\")\n",
    "    print(\"- Strong proactive interference = lines diverge and stay separated in delay2\")\n",
    "    print(\"- Weak proactive interference = lines converge together in delay2\")\n",
    "    print(\"- Compare with previous analysis to see bidirectional vs unidirectional interference\")\n",
    "\n",
    "# Run the proactive interference analysis\n",
    "if 'all_step3_ids' in locals() and 'step3_neurons' in locals() and 'epoch_ts' in locals():\n",
    "    plot_stim2_tuned_by_stim1(data_filtered, step3_neurons, all_step3_ids, epoch_ts)\n",
    "else:\n",
    "    print(\"Need to run the step 3 analysis first to get:\")\n",
    "    print(\"- all_step3_ids\")\n",
    "    print(\"- step3_neurons\")\n",
    "    print(\"- epoch_ts\")\n",
    "    print(\"\\nOr run the setup variables code block first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee5bb23",
   "metadata": {},
   "source": [
    "### category congruence effects -- during stim 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f037fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CATEGORY CONGRUENCE ANALYSIS - ALL NEURONS =====\n",
    "\n",
    "def analyze_category_congruence_all_neurons(data_filtered, selectivity_results):\n",
    "    \"\"\"\n",
    "    Analyze category congruence effects (stim1 vs stim2 relationship) across ALL neurons.\n",
    "    \n",
    "    This tests adaptation/facilitation effects in the broader population, not just \n",
    "    distractor-resistant neurons, since the core question is about encoding responses.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Analyzing category congruence effects across ALL neurons...\")\n",
    "    \n",
    "    # Get all neurons with any selectivity (broader than just stim1 category selective)\n",
    "    responsive_neurons = selectivity_results[selectivity_results[\"is_any_selective\"]].copy()\n",
    "    print(f\"Found {len(responsive_neurons)} responsive neurons to analyze\")\n",
    "    \n",
    "    congruence_results = []\n",
    "    \n",
    "    for _, neuron_row in tqdm(responsive_neurons.iterrows(), desc=\"Analyzing neurons\"):\n",
    "        unit_id = neuron_row['unit_id']\n",
    "        \n",
    "        try:\n",
    "            df_unit = data_filtered[data_filtered[\"unit_id\"] == unit_id].reset_index(drop=True)\n",
    "            if df_unit.empty:\n",
    "                continue\n",
    "            \n",
    "            area = neuron_row['area']\n",
    "            \n",
    "            # Create category match variable\n",
    "            df_unit['category_match'] = (df_unit['first_cat_simple'] == df_unit['second_cat_simple'])\n",
    "            \n",
    "            # Split into same vs different category trials\n",
    "            same_cat_trials = df_unit[df_unit['category_match']].copy()\n",
    "            diff_cat_trials = df_unit[~df_unit['category_match']].copy()\n",
    "            \n",
    "            if len(same_cat_trials) < 5 or len(diff_cat_trials) < 5:\n",
    "                continue\n",
    "            \n",
    "            # === CORE ANALYSIS: Response Differences (Stim2 - Stim1) ===\n",
    "            same_cat_trials['response_diff'] = same_cat_trials['fr_enc2_epoch'] - same_cat_trials['fr_epoch']\n",
    "            diff_cat_trials['response_diff'] = diff_cat_trials['fr_enc2_epoch'] - diff_cat_trials['fr_epoch']\n",
    "            \n",
    "            same_cat_diff_mean = same_cat_trials['response_diff'].mean()\n",
    "            same_cat_diff_sem = same_cat_trials['response_diff'].sem()\n",
    "            diff_cat_diff_mean = diff_cat_trials['response_diff'].mean()\n",
    "            diff_cat_diff_sem = diff_cat_trials['response_diff'].sem()\n",
    "            \n",
    "            # Statistical test\n",
    "            from scipy.stats import ttest_ind\n",
    "            diff_ttest_stat, diff_ttest_pval = ttest_ind(\n",
    "                same_cat_trials['response_diff'].dropna(), \n",
    "                diff_cat_trials['response_diff'].dropna()\n",
    "            )\n",
    "            \n",
    "            # === Response Ratios ===\n",
    "            epsilon = 0.01  # Avoid division by zero\n",
    "            same_cat_trials['response_ratio'] = (same_cat_trials['fr_enc2_epoch'] + epsilon) / (same_cat_trials['fr_epoch'] + epsilon)\n",
    "            diff_cat_trials['response_ratio'] = (diff_cat_trials['fr_enc2_epoch'] + epsilon) / (diff_cat_trials['fr_epoch'] + epsilon)\n",
    "            \n",
    "            same_cat_ratio_mean = same_cat_trials['response_ratio'].mean()\n",
    "            diff_cat_ratio_mean = diff_cat_trials['response_ratio'].mean()\n",
    "            \n",
    "            # Test ratio differences\n",
    "            ratio_ttest_stat, ratio_ttest_pval = ttest_ind(\n",
    "                same_cat_trials['response_ratio'].dropna(), \n",
    "                diff_cat_trials['response_ratio'].dropna()\n",
    "            )\n",
    "            \n",
    "            # === Correlations ===\n",
    "            same_cat_corr = same_cat_trials[['fr_epoch', 'fr_enc2_epoch']].corr().iloc[0,1]\n",
    "            diff_cat_corr = diff_cat_trials[['fr_epoch', 'fr_enc2_epoch']].corr().iloc[0,1]\n",
    "            \n",
    "            # === Effect Classification ===\n",
    "            adaptation_effect = \"none\"\n",
    "            if min(diff_ttest_pval, ratio_ttest_pval) < 0.05:\n",
    "                if same_cat_diff_mean < diff_cat_diff_mean:\n",
    "                    if same_cat_diff_mean < 0:\n",
    "                        adaptation_effect = \"adaptation\"  # Same category shows reduction\n",
    "                    else:\n",
    "                        adaptation_effect = \"less_facilitation\"\n",
    "                else:\n",
    "                    if same_cat_diff_mean > 0:\n",
    "                        adaptation_effect = \"facilitation\"  # Same category shows enhancement\n",
    "                    else:\n",
    "                        adaptation_effect = \"less_adaptation\"\n",
    "            \n",
    "            # === Include Neuron Type Classification ===\n",
    "            # Determine what type of neuron this is\n",
    "            neuron_type = \"encoding_only\"\n",
    "            if 'distractor_results_principled' in globals():\n",
    "                distractor_info = distractor_results_principled[distractor_results_principled['unit_id'] == unit_id]\n",
    "                if len(distractor_info) > 0:\n",
    "                    neuron_type = distractor_info.iloc[0]['analysis_stage']\n",
    "            \n",
    "            # === Store Results ===\n",
    "            result = {\n",
    "                'unit_id': unit_id,\n",
    "                'area': area,\n",
    "                'neuron_type': neuron_type,\n",
    "                'n_same_cat_trials': len(same_cat_trials),\n",
    "                'n_diff_cat_trials': len(diff_cat_trials),\n",
    "                \n",
    "                # Response differences\n",
    "                'same_cat_diff_mean': same_cat_diff_mean,\n",
    "                'same_cat_diff_sem': same_cat_diff_sem,\n",
    "                'diff_cat_diff_mean': diff_cat_diff_mean,\n",
    "                'diff_cat_diff_sem': diff_cat_diff_sem,\n",
    "                'difference_pvalue': diff_ttest_pval,\n",
    "                \n",
    "                # Response ratios\n",
    "                'same_cat_ratio_mean': same_cat_ratio_mean,\n",
    "                'diff_cat_ratio_mean': diff_cat_ratio_mean,\n",
    "                'ratio_pvalue': ratio_ttest_pval,\n",
    "                \n",
    "                # Correlations\n",
    "                'same_cat_correlation': same_cat_corr,\n",
    "                'diff_cat_correlation': diff_cat_corr,\n",
    "                \n",
    "                # Classification\n",
    "                'adaptation_effect': adaptation_effect,\n",
    "                'significant_congruence_effect': min(diff_ttest_pval, ratio_ttest_pval) < 0.05,\n",
    "                \n",
    "                # Selectivity info\n",
    "                'is_first_cat_selective': neuron_row['is_first_cat_selective'],\n",
    "                'is_first_num_selective': neuron_row['is_first_num_selective'],\n",
    "                'is_second_cat_selective': neuron_row['is_second_cat_selective'],\n",
    "                'is_second_num_selective': neuron_row['is_second_num_selective'],\n",
    "            }\n",
    "            \n",
    "            congruence_results.append(result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing unit {unit_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    congruence_df = pd.DataFrame(congruence_results)\n",
    "    \n",
    "    if len(congruence_df) == 0:\n",
    "        print(\"No neurons analyzed successfully!\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # === SUMMARY ANALYSIS ===\n",
    "    print(f\"\\n=== CATEGORY CONGRUENCE SUMMARY (ALL NEURONS) ===\")\n",
    "    print(f\"Successfully analyzed: {len(congruence_df)} neurons\")\n",
    "    print(f\"Significant congruence effects: {congruence_df['significant_congruence_effect'].sum()}/{len(congruence_df)} ({congruence_df['significant_congruence_effect'].mean()*100:.1f}%)\")\n",
    "    \n",
    "    # Effect type distribution\n",
    "    print(f\"\\n=== EFFECT TYPE DISTRIBUTION ===\")\n",
    "    effect_counts = congruence_df['adaptation_effect'].value_counts()\n",
    "    for effect, count in effect_counts.items():\n",
    "        print(f\"  {effect}: {count} neurons ({count/len(congruence_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Average effects\n",
    "    avg_same_diff = congruence_df['same_cat_diff_mean'].mean()\n",
    "    avg_diff_diff = congruence_df['diff_cat_diff_mean'].mean()\n",
    "    print(f\"\\n=== AVERAGE RESPONSE DIFFERENCES ===\")\n",
    "    print(f\"  Same category trials: {avg_same_diff:.3f} Hz\")\n",
    "    print(f\"  Different category trials: {avg_diff_diff:.3f} Hz\")\n",
    "    print(f\"  Overall congruence effect: {avg_same_diff - avg_diff_diff:.3f} Hz\")\n",
    "    \n",
    "    if avg_same_diff < avg_diff_diff:\n",
    "        print(\"  → Population shows ADAPTATION (same category reduces stim2 response)\")\n",
    "    else:\n",
    "        print(\"  → Population shows FACILITATION (same category enhances stim2 response)\")\n",
    "    \n",
    "    # === ANALYSIS BY NEURON TYPE ===\n",
    "    if 'neuron_type' in congruence_df.columns:\n",
    "        print(f\"\\n=== EFFECTS BY NEURON TYPE ===\")\n",
    "        type_summary = congruence_df.groupby('neuron_type').agg({\n",
    "            'significant_congruence_effect': ['count', 'sum', 'mean'],\n",
    "            'same_cat_diff_mean': 'mean',\n",
    "            'diff_cat_diff_mean': 'mean'\n",
    "        }).round(3)\n",
    "        \n",
    "        for neuron_type in congruence_df['neuron_type'].unique():\n",
    "            type_data = congruence_df[congruence_df['neuron_type'] == neuron_type]\n",
    "            n_total = len(type_data)\n",
    "            n_sig = type_data['significant_congruence_effect'].sum()\n",
    "            avg_effect = type_data['same_cat_diff_mean'].mean() - type_data['diff_cat_diff_mean'].mean()\n",
    "            \n",
    "            print(f\"  {neuron_type}: {n_sig}/{n_total} significant ({n_sig/n_total*100:.1f}%), avg effect: {avg_effect:.3f} Hz\")\n",
    "    \n",
    "    # === ANALYSIS BY BRAIN AREA ===\n",
    "    print(f\"\\n=== EFFECTS BY BRAIN AREA ===\")\n",
    "    area_summary = congruence_df.groupby('area').agg({\n",
    "        'significant_congruence_effect': ['count', 'sum'],\n",
    "        'same_cat_diff_mean': 'mean',\n",
    "        'adaptation_effect': lambda x: (x == 'adaptation').sum()\n",
    "    }).round(3)\n",
    "    \n",
    "    for area in congruence_df['area'].unique():\n",
    "        area_data = congruence_df[congruence_df['area'] == area]\n",
    "        if len(area_data) >= 3:  # Only show areas with enough neurons\n",
    "            n_total = len(area_data)\n",
    "            n_sig = area_data['significant_congruence_effect'].sum()\n",
    "            n_adapt = (area_data['adaptation_effect'] == 'adaptation').sum()\n",
    "            avg_effect = area_data['same_cat_diff_mean'].mean() - area_data['diff_cat_diff_mean'].mean()\n",
    "            \n",
    "            print(f\"  {area}: {n_sig}/{n_total} significant, {n_adapt} adaptation, avg effect: {avg_effect:.3f} Hz\")\n",
    "    \n",
    "    # Save results\n",
    "    os.makedirs(\"category_congruence_analysis\", exist_ok=True)\n",
    "    congruence_df.to_csv(\"category_congruence_analysis/all_neurons_congruence_analysis.csv\", index=False)\n",
    "    print(f\"\\nResults saved to: category_congruence_analysis/all_neurons_congruence_analysis.csv\")\n",
    "    \n",
    "    return congruence_df\n",
    "\n",
    "\n",
    "# === RUN THE ANALYSIS ===\n",
    "if 'selectivity_results' in locals() and 'data_filtered' in locals():\n",
    "    print(\"Running category congruence analysis on all responsive neurons...\")\n",
    "    all_neurons_congruence = analyze_category_congruence_all_neurons(data_filtered, selectivity_results)\n",
    "    \n",
    "    # Quick comparison with distractor-resistant neurons if available\n",
    "    if 'all_step3_ids' in locals() and len(all_neurons_congruence) > 0:\n",
    "        print(f\"\\n=== COMPARISON: ALL NEURONS vs DISTRACTOR-RESISTANT ===\")\n",
    "        \n",
    "        all_neurons_effects = all_neurons_congruence['significant_congruence_effect'].mean()\n",
    "        distractor_resistant_data = all_neurons_congruence[all_neurons_congruence['neuron_type'] == 'distractor_resistant']\n",
    "        \n",
    "        if len(distractor_resistant_data) > 0:\n",
    "            distractor_effects = distractor_resistant_data['significant_congruence_effect'].mean()\n",
    "            print(f\"All neurons: {all_neurons_effects*100:.1f}% show significant congruence effects\")\n",
    "            print(f\"Distractor-resistant neurons: {distractor_effects*100:.1f}% show significant congruence effects\")\n",
    "        else:\n",
    "            print(\"No distractor-resistant neurons found in the analysis\")\n",
    "    \n",
    "else:\n",
    "    print(\"Need selectivity_results and data_filtered to run this analysis!\")\n",
    "    print(\"Make sure you've run the neuron selectivity analysis first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cdd5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CATEGORY CONGRUENCE VISUALIZATIONS FOR DISTRACTOR RESISTANCE =====\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# Make sure we have the results\n",
    "if 'all_neurons_congruence' not in locals():\n",
    "    print(\"Need to run the category congruence analysis first!\")\n",
    "else:\n",
    "    # Create output directory\n",
    "    os.makedirs(\"category_congruence_plots\", exist_ok=True)\n",
    "    \n",
    "    print(\"Creating category congruence visualizations...\")\n",
    "    \n",
    "    # === PLOT 1: Distractor Resistance vs Congruence Effects ===\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Plot 1A: Percentage with significant effects by neuron type\n",
    "    type_summary = all_neurons_congruence.groupby('neuron_type').agg({\n",
    "        'significant_congruence_effect': ['count', 'sum', 'mean']\n",
    "    }).round(3)\n",
    "    \n",
    "    neuron_types = []\n",
    "    percentages = []\n",
    "    counts = []\n",
    "    \n",
    "    for ntype in all_neurons_congruence['neuron_type'].unique():\n",
    "        type_data = all_neurons_congruence[all_neurons_congruence['neuron_type'] == ntype]\n",
    "        pct = type_data['significant_congruence_effect'].mean() * 100\n",
    "        n_sig = type_data['significant_congruence_effect'].sum()\n",
    "        n_total = len(type_data)\n",
    "        \n",
    "        neuron_types.append(f\"{ntype}\\n(n={n_total})\")\n",
    "        percentages.append(pct)\n",
    "        counts.append(f\"{n_sig}/{n_total}\")\n",
    "    \n",
    "    bars = axes[0,0].bar(neuron_types, percentages, \n",
    "                        color=['lightblue', 'orange', 'red'])\n",
    "    axes[0,0].set_ylabel('% with Significant Congruence Effects')\n",
    "    axes[0,0].set_title('Congruence Effects by Neuron Type')\n",
    "    axes[0,0].set_ylim(0, max(percentages) * 1.2)\n",
    "    \n",
    "    # Add count labels on bars\n",
    "    for bar, count in zip(bars, counts):\n",
    "        height = bar.get_height()\n",
    "        axes[0,0].text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                      count, ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    # Plot 1B: Effect size by neuron type\n",
    "    type_effects = []\n",
    "    type_labels = []\n",
    "    for ntype in all_neurons_congruence['neuron_type'].unique():\n",
    "        type_data = all_neurons_congruence[all_neurons_congruence['neuron_type'] == ntype]\n",
    "        effect = type_data['same_cat_diff_mean'].mean() - type_data['diff_cat_diff_mean'].mean()\n",
    "        type_effects.append(effect)\n",
    "        type_labels.append(ntype)\n",
    "    \n",
    "    bars = axes[0,1].bar(type_labels, type_effects, \n",
    "                        color=['lightblue', 'orange', 'red'])\n",
    "    axes[0,1].set_ylabel('Average Congruence Effect (Hz)')\n",
    "    axes[0,1].set_title('Effect Size by Neuron Type')\n",
    "    axes[0,1].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    plt.setp(axes[0,1].xaxis.get_majorticklabels(), rotation=45)\n",
    "    \n",
    "    # Plot 1C: Brain area effects\n",
    "    area_data = []\n",
    "    for area in all_neurons_congruence['area'].unique():\n",
    "        area_subset = all_neurons_congruence[all_neurons_congruence['area'] == area]\n",
    "        if len(area_subset) >= 10:  # Only areas with enough neurons\n",
    "            pct_sig = area_subset['significant_congruence_effect'].mean() * 100\n",
    "            avg_effect = area_subset['same_cat_diff_mean'].mean() - area_subset['diff_cat_diff_mean'].mean()\n",
    "            n_neurons = len(area_subset)\n",
    "            area_data.append({'area': area, 'pct_sig': pct_sig, 'avg_effect': avg_effect, 'n_neurons': n_neurons})\n",
    "    \n",
    "    area_df = pd.DataFrame(area_data)\n",
    "    if len(area_df) > 0:\n",
    "        bars = axes[1,0].bar(area_df['area'], area_df['pct_sig'])\n",
    "        axes[1,0].set_ylabel('% with Significant Effects')\n",
    "        axes[1,0].set_title('Congruence Effects by Brain Area')\n",
    "        plt.setp(axes[1,0].xaxis.get_majorticklabels(), rotation=45)\n",
    "        \n",
    "        # Add sample size labels\n",
    "        for i, (bar, n) in enumerate(zip(bars, area_df['n_neurons'])):\n",
    "            height = bar.get_height()\n",
    "            axes[1,0].text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                          f'n={n}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    # Plot 1D: Effect types distribution\n",
    "    effect_counts = all_neurons_congruence['adaptation_effect'].value_counts()\n",
    "    colors = {'none': 'lightgray', 'adaptation': 'blue', 'facilitation': 'red', \n",
    "              'less_adaptation': 'lightblue', 'less_facilitation': 'pink'}\n",
    "    \n",
    "    pie_colors = [colors.get(effect, 'gray') for effect in effect_counts.index]\n",
    "    axes[1,1].pie(effect_counts.values, labels=effect_counts.index, autopct='%1.1f%%',\n",
    "                 colors=pie_colors, startangle=90)\n",
    "    axes[1,1].set_title('Distribution of Effect Types')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"category_congruence_plots/congruence_by_neuron_type.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(\"  Saved: congruence_by_neuron_type.png\")\n",
    "    \n",
    "    # === PLOT 2: Individual Neuron Effects ===\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Plot 2A: Scatter plot of same vs different category effects\n",
    "    axes[0,0].scatter(all_neurons_congruence['same_cat_diff_mean'], \n",
    "                     all_neurons_congruence['diff_cat_diff_mean'],\n",
    "                     c=all_neurons_congruence['significant_congruence_effect'].map({True: 'red', False: 'lightblue'}),\n",
    "                     alpha=0.6)\n",
    "    \n",
    "    # Add diagonal line\n",
    "    max_val = max(all_neurons_congruence['same_cat_diff_mean'].max(), \n",
    "                  all_neurons_congruence['diff_cat_diff_mean'].max())\n",
    "    min_val = min(all_neurons_congruence['same_cat_diff_mean'].min(), \n",
    "                  all_neurons_congruence['diff_cat_diff_mean'].min())\n",
    "    axes[0,0].plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.5)\n",
    "    \n",
    "    axes[0,0].set_xlabel('Same Category Effect (Hz)')\n",
    "    axes[0,0].set_ylabel('Different Category Effect (Hz)')\n",
    "    axes[0,0].set_title('Same vs Different Category Effects')\n",
    "    axes[0,0].axhline(y=0, color='gray', linestyle='--', alpha=0.3)\n",
    "    axes[0,0].axvline(x=0, color='gray', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # Add quadrant labels\n",
    "    axes[0,0].text(0.7*max_val, 0.1*max_val, 'Same category\\nfacilitation only', \n",
    "                  ha='center', va='center', bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\", alpha=0.5))\n",
    "    \n",
    "    # Plot 2B: Distribution of congruence effects\n",
    "    axes[0,1].hist(all_neurons_congruence['same_cat_diff_mean'] - all_neurons_congruence['diff_cat_diff_mean'], \n",
    "                  bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0,1].axvline(x=0, color='red', linestyle='--', linewidth=2, label='No effect')\n",
    "    axes[0,1].set_xlabel('Congruence Effect Size (Hz)')\n",
    "    axes[0,1].set_ylabel('Number of Neurons')\n",
    "    axes[0,1].set_title('Distribution of Congruence Effect Sizes')\n",
    "    axes[0,1].legend()\n",
    "    \n",
    "    # Plot 2C: Effect size vs significance\n",
    "    effect_sizes = all_neurons_congruence['same_cat_diff_mean'] - all_neurons_congruence['diff_cat_diff_mean']\n",
    "    p_values = np.minimum(all_neurons_congruence['difference_pvalue'], all_neurons_congruence['ratio_pvalue'])\n",
    "    \n",
    "    # Color by neuron type\n",
    "    type_colors = {'encoding_only': 'blue', 'delay_only': 'orange', 'distractor_resistant': 'red'}\n",
    "    colors = [type_colors.get(t, 'gray') for t in all_neurons_congruence['neuron_type']]\n",
    "    \n",
    "    axes[1,0].scatter(effect_sizes, -np.log10(p_values), c=colors, alpha=0.6)\n",
    "    axes[1,0].axhline(y=-np.log10(0.05), color='red', linestyle='--', label='p=0.05')\n",
    "    axes[1,0].axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    axes[1,0].set_xlabel('Congruence Effect Size (Hz)')\n",
    "    axes[1,0].set_ylabel('-log10(p-value)')\n",
    "    axes[1,0].set_title('Volcano Plot: Effect Size vs Significance')\n",
    "    axes[1,0].legend()\n",
    "    \n",
    "    # Add legend for neuron types\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [Patch(facecolor=color, label=ntype) \n",
    "                      for ntype, color in type_colors.items()]\n",
    "    axes[1,0].legend(handles=legend_elements, loc='upper right')\n",
    "    \n",
    "    # Plot 2D: Response correlations same vs different\n",
    "    axes[1,1].scatter(all_neurons_congruence['same_cat_correlation'], \n",
    "                     all_neurons_congruence['diff_cat_correlation'],\n",
    "                     c=all_neurons_congruence['significant_congruence_effect'].map({True: 'red', False: 'lightblue'}),\n",
    "                     alpha=0.6)\n",
    "    axes[1,1].plot([-1, 1], [-1, 1], 'k--', alpha=0.5)\n",
    "    axes[1,1].set_xlabel('Same Category Stim1-Stim2 Correlation')\n",
    "    axes[1,1].set_ylabel('Different Category Stim1-Stim2 Correlation')\n",
    "    axes[1,1].set_title('Response Correlations: Same vs Different Categories')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"category_congruence_plots/individual_neuron_effects.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(\"  Saved: individual_neuron_effects.png\")\n",
    "    \n",
    "    # === PLOT 3: Distractor-Resistant Neuron Focus ===\n",
    "    distractor_resistant = all_neurons_congruence[all_neurons_congruence['neuron_type'] == 'distractor_resistant']\n",
    "    \n",
    "    if len(distractor_resistant) > 0:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # Plot 3A: Distractor-resistant neurons individual effects\n",
    "        axes[0,0].scatter(distractor_resistant['same_cat_diff_mean'], \n",
    "                         distractor_resistant['diff_cat_diff_mean'],\n",
    "                         c=distractor_resistant['significant_congruence_effect'].map({True: 'red', False: 'blue'}),\n",
    "                         s=100, alpha=0.7)\n",
    "        \n",
    "        # Add unit ID labels\n",
    "        for _, row in distractor_resistant.iterrows():\n",
    "            axes[0,0].annotate(f\"U{row['unit_id']}\", \n",
    "                              (row['same_cat_diff_mean'], row['diff_cat_diff_mean']),\n",
    "                              xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "        \n",
    "        axes[0,0].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "        axes[0,0].axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "        axes[0,0].set_xlabel('Same Category Effect (Hz)')\n",
    "        axes[0,0].set_ylabel('Different Category Effect (Hz)')\n",
    "        axes[0,0].set_title('Distractor-Resistant Neurons: Individual Effects')\n",
    "        \n",
    "        # Plot 3B: Effect types in distractor-resistant neurons\n",
    "        dr_effects = distractor_resistant['adaptation_effect'].value_counts()\n",
    "        if len(dr_effects) > 0:\n",
    "            axes[0,1].bar(dr_effects.index, dr_effects.values, \n",
    "                         color=['lightgray', 'blue', 'red', 'lightblue', 'pink'])\n",
    "            axes[0,1].set_ylabel('Number of Neurons')\n",
    "            axes[0,1].set_title('Effect Types in Distractor-Resistant Neurons')\n",
    "            plt.setp(axes[0,1].xaxis.get_majorticklabels(), rotation=45)\n",
    "        \n",
    "        # Plot 3C: Brain area distribution of distractor-resistant neurons\n",
    "        dr_areas = distractor_resistant['area'].value_counts()\n",
    "        axes[1,0].bar(dr_areas.index, dr_areas.values)\n",
    "        axes[1,0].set_ylabel('Number of Neurons')\n",
    "        axes[1,0].set_title('Distractor-Resistant Neurons by Brain Area')\n",
    "        \n",
    "        # Add congruence effect info\n",
    "        for i, area in enumerate(dr_areas.index):\n",
    "            area_data = distractor_resistant[distractor_resistant['area'] == area]\n",
    "            n_sig = area_data['significant_congruence_effect'].sum()\n",
    "            axes[1,0].text(i, dr_areas.iloc[i] + 0.1, f'{n_sig} sig', \n",
    "                          ha='center', va='bottom', fontsize=8)\n",
    "        \n",
    "        # Plot 3D: Comparison with other neuron types\n",
    "        comparison_data = []\n",
    "        for ntype in all_neurons_congruence['neuron_type'].unique():\n",
    "            type_data = all_neurons_congruence[all_neurons_congruence['neuron_type'] == ntype]\n",
    "            avg_effect = type_data['same_cat_diff_mean'].mean() - type_data['diff_cat_diff_mean'].mean()\n",
    "            pct_sig = type_data['significant_congruence_effect'].mean() * 100\n",
    "            comparison_data.append({'type': ntype, 'avg_effect': avg_effect, 'pct_sig': pct_sig})\n",
    "        \n",
    "        comp_df = pd.DataFrame(comparison_data)\n",
    "        \n",
    "        # Create a dual-axis plot\n",
    "        ax1 = axes[1,1]\n",
    "        ax2 = ax1.twinx()\n",
    "        \n",
    "        x_pos = np.arange(len(comp_df))\n",
    "        bars1 = ax1.bar(x_pos - 0.2, comp_df['avg_effect'], width=0.4, \n",
    "                       label='Avg Effect Size', color='skyblue', alpha=0.7)\n",
    "        bars2 = ax2.bar(x_pos + 0.2, comp_df['pct_sig'], width=0.4, \n",
    "                       label='% Significant', color='orange', alpha=0.7)\n",
    "        \n",
    "        ax1.set_xlabel('Neuron Type')\n",
    "        ax1.set_ylabel('Average Effect Size (Hz)', color='blue')\n",
    "        ax2.set_ylabel('% with Significant Effects', color='orange')\n",
    "        ax1.set_title('Congruence Effects: Type Comparison')\n",
    "        \n",
    "        ax1.set_xticks(x_pos)\n",
    "        ax1.set_xticklabels(comp_df['type'], rotation=45)\n",
    "        ax1.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        # Add legends\n",
    "        lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "        lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "        ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"category_congruence_plots/distractor_resistant_focus.png\", dpi=300, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "        print(\"  Saved: distractor_resistant_focus.png\")\n",
    "    \n",
    "    # === PLOT 4: Example Neurons with Strong Effects ===\n",
    "    # Find neurons with strongest effects\n",
    "    significant_neurons = all_neurons_congruence[all_neurons_congruence['significant_congruence_effect']]\n",
    "    \n",
    "    if len(significant_neurons) > 0:\n",
    "        # Sort by effect size\n",
    "        significant_neurons['abs_effect'] = abs(significant_neurons['same_cat_diff_mean'] - significant_neurons['diff_cat_diff_mean'])\n",
    "        top_neurons = significant_neurons.nlargest(4, 'abs_effect')\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i, (_, neuron) in enumerate(top_neurons.iterrows()):\n",
    "            if i >= 4:\n",
    "                break\n",
    "                \n",
    "            unit_id = neuron['unit_id']\n",
    "            \n",
    "            # Get unit data for detailed plotting\n",
    "            unit_data = data_filtered[data_filtered['unit_id'] == unit_id].copy()\n",
    "            if len(unit_data) > 0:\n",
    "                unit_data['category_match'] = unit_data['first_cat_simple'] == unit_data['second_cat_simple']\n",
    "                \n",
    "                same_cat = unit_data[unit_data['category_match']]\n",
    "                diff_cat = unit_data[~unit_data['category_match']]\n",
    "                \n",
    "                # Create comparison plot\n",
    "                x_pos = [0, 1]\n",
    "                same_means = [same_cat['fr_epoch'].mean(), same_cat['fr_enc2_epoch'].mean()]\n",
    "                same_sems = [same_cat['fr_epoch'].sem(), same_cat['fr_enc2_epoch'].sem()]\n",
    "                diff_means = [diff_cat['fr_epoch'].mean(), diff_cat['fr_enc2_epoch'].mean()]\n",
    "                diff_sems = [diff_cat['fr_epoch'].sem(), diff_cat['fr_enc2_epoch'].sem()]\n",
    "                \n",
    "                axes[i].errorbar(x_pos, same_means, yerr=same_sems, \n",
    "                               marker='o', label='Same Category', linewidth=2, markersize=8)\n",
    "                axes[i].errorbar(x_pos, diff_means, yerr=diff_sems, \n",
    "                               marker='s', label='Different Category', linewidth=2, markersize=8)\n",
    "                \n",
    "                axes[i].set_xticks(x_pos)\n",
    "                axes[i].set_xticklabels(['Stim1', 'Stim2'])\n",
    "                axes[i].set_ylabel('Firing Rate (Hz)')\n",
    "                axes[i].set_title(f\"Unit {unit_id} ({neuron['area']}, {neuron['neuron_type']})\\n\"\n",
    "                                f\"Effect: {neuron['same_cat_diff_mean'] - neuron['diff_cat_diff_mean']:.3f} Hz\")\n",
    "                axes[i].legend()\n",
    "                axes[i].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"category_congruence_plots/example_neurons_with_effects.png\", dpi=300, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "        print(\"  Saved: example_neurons_with_effects.png\")\n",
    "    \n",
    "    print(\"\\nplot finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8322db2f",
   "metadata": {},
   "source": [
    "###  mixed selectivity during stim 2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## 5. Mixed Selectivity Analysis\n",
    "\n",
    "### Overview\n",
    "\n",
    "Test for interaction effects between stimulus features, following Parthasarathy et al. (2017) Nature Neuroscience approach to identify neurons encoding feature combinations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df4aba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== MIXED SELECTIVITY: STIM1 x STIM2 INTERACTION ANALYSIS =====\n",
    "# Testing for interaction effects during stim2 presentation\n",
    "# Inspired by Parthasarathy et al. (2017) Nature Neuroscience\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def analyze_mixed_selectivity_interactions(data_filtered, selectivity_results):\n",
    "    \"\"\"\n",
    "    Test for mixed selectivity during stim2 presentation:\n",
    "    1. CategoryStim1 x CategoryStim2 interaction effects\n",
    "    2. NumerosityStim1 x NumerosityStim2 interaction effects\n",
    "    \n",
    "    This tests whether stim2 responses depend on specific combinations of \n",
    "    stim1 and stim2 features, not just individual features.\n",
    "    \n",
    "    Reference: Parthasarathy et al. (2017) Nature Neuroscience\n",
    "    \"Mixed selectivity morphs population codes in prefrontal cortex\"\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== MIXED SELECTIVITY INTERACTION ANALYSIS ===\")\n",
    "    print(\"Testing for Stim1 x Stim2 interaction effects during stim2 presentation...\")\n",
    "    \n",
    "    # Use ALL neurons (like Parthasarathy et al.) - no pre-selection for individual selectivity\n",
    "    all_units = data_filtered['unit_id'].unique()\n",
    "    print(f\"Analyzing {len(all_units)} total neurons (no pre-selection)\")\n",
    "    print(\"Note: Using all neurons to avoid bias against pure mixed selectivity\")\n",
    "    \n",
    "    interaction_results = []\n",
    "    \n",
    "    for unit_id in tqdm(all_units, desc=\"Testing interactions\"):\n",
    "        try:\n",
    "            df_unit = data_filtered[data_filtered[\"unit_id\"] == unit_id].reset_index(drop=True)\n",
    "            if df_unit.empty:\n",
    "                continue\n",
    "            \n",
    "            # Get area info (handle case where unit isn't in selectivity_results)\n",
    "            area_info = selectivity_results[selectivity_results['unit_id'] == unit_id]\n",
    "            if len(area_info) > 0:\n",
    "                area = area_info.iloc[0]['area']\n",
    "            else:\n",
    "                # Get area from data_filtered directly\n",
    "                area = df_unit['brainAreaOfCell'].iloc[0]\n",
    "            \n",
    "            # Get selectivity info if available\n",
    "            selectivity_info = selectivity_results[selectivity_results['unit_id'] == unit_id]\n",
    "            if len(selectivity_info) > 0:\n",
    "                is_first_cat_selective = selectivity_info.iloc[0]['is_first_cat_selective']\n",
    "                is_second_cat_selective = selectivity_info.iloc[0]['is_second_cat_selective'] \n",
    "                is_first_num_selective = selectivity_info.iloc[0]['is_first_num_selective']\n",
    "                is_second_num_selective = selectivity_info.iloc[0]['is_second_num_selective']\n",
    "            else:\n",
    "                # Neuron wasn't in selectivity analysis (low firing rate, etc.)\n",
    "                is_first_cat_selective = False\n",
    "                is_second_cat_selective = False\n",
    "                is_first_num_selective = False\n",
    "                is_second_num_selective = False\n",
    "            \n",
    "            # Check data quality for interaction analysis\n",
    "            n_stim1_cats = df_unit['first_cat_simple'].nunique()\n",
    "            n_stim2_cats = df_unit['second_cat_simple'].nunique()\n",
    "            n_stim1_nums = df_unit['first_num_simple'].nunique()\n",
    "            n_stim2_nums = df_unit['second_num_simple'].nunique()\n",
    "            \n",
    "            if n_stim1_cats < 2 or n_stim2_cats < 2:\n",
    "                continue\n",
    "            \n",
    "            # === ANALYSIS 1: Category x Category Interaction ===\n",
    "            cat_interaction_pval = np.nan\n",
    "            cat_main_stim1_pval = np.nan\n",
    "            cat_main_stim2_pval = np.nan\n",
    "            cat_model_r2 = np.nan\n",
    "            \n",
    "            try:\n",
    "                # 2-way ANOVA with interaction: stim2_response ~ stim1_cat * stim2_cat\n",
    "                cat_model = smf.ols(\n",
    "                    \"fr_enc2_epoch ~ C(first_cat_simple) * C(second_cat_simple)\", \n",
    "                    data=df_unit\n",
    "                ).fit()\n",
    "                cat_anova = sm.stats.anova_lm(cat_model, typ=2)\n",
    "                \n",
    "                # Extract p-values\n",
    "                cat_main_stim1_pval = cat_anova.loc['C(first_cat_simple)', 'PR(>F)']\n",
    "                cat_main_stim2_pval = cat_anova.loc['C(second_cat_simple)', 'PR(>F)']\n",
    "                cat_interaction_pval = cat_anova.loc['C(first_cat_simple):C(second_cat_simple)', 'PR(>F)']\n",
    "                cat_model_r2 = cat_model.rsquared\n",
    "                \n",
    "            except Exception as e:\n",
    "                pass  # Keep NaN values\n",
    "            \n",
    "            # === ANALYSIS 2: Numerosity x Numerosity Interaction ===\n",
    "            num_interaction_pval = np.nan\n",
    "            num_main_stim1_pval = np.nan\n",
    "            num_main_stim2_pval = np.nan\n",
    "            num_model_r2 = np.nan\n",
    "            \n",
    "            if n_stim1_nums >= 2 and n_stim2_nums >= 2:\n",
    "                try:\n",
    "                    # 2-way ANOVA with interaction: stim2_response ~ stim1_num * stim2_num\n",
    "                    num_model = smf.ols(\n",
    "                        \"fr_enc2_epoch ~ C(first_num_simple) * C(second_num_simple)\", \n",
    "                        data=df_unit\n",
    "                    ).fit()\n",
    "                    num_anova = sm.stats.anova_lm(num_model, typ=2)\n",
    "                    \n",
    "                    # Extract p-values\n",
    "                    num_main_stim1_pval = num_anova.loc['C(first_num_simple)', 'PR(>F)']\n",
    "                    num_main_stim2_pval = num_anova.loc['C(second_num_simple)', 'PR(>F)']\n",
    "                    num_interaction_pval = num_anova.loc['C(first_num_simple):C(second_num_simple)', 'PR(>F)']\n",
    "                    num_model_r2 = num_model.rsquared\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    pass  # Keep NaN values\n",
    "            \n",
    "            # === ANALYSIS 3: Mixed Model with All Features ===\n",
    "            mixed_model_r2 = np.nan\n",
    "            mixed_cat1_pval = np.nan\n",
    "            mixed_cat2_pval = np.nan\n",
    "            mixed_num1_pval = np.nan\n",
    "            mixed_num2_pval = np.nan\n",
    "            \n",
    "            try:\n",
    "                # Full model with all features (no interactions for complexity)\n",
    "                mixed_model = smf.ols(\n",
    "                    \"fr_enc2_epoch ~ C(first_cat_simple) + C(second_cat_simple) + C(first_num_simple) + C(second_num_simple)\", \n",
    "                    data=df_unit\n",
    "                ).fit()\n",
    "                mixed_anova = sm.stats.anova_lm(mixed_model, typ=2)\n",
    "                \n",
    "                mixed_model_r2 = mixed_model.rsquared\n",
    "                mixed_cat1_pval = mixed_anova.loc['C(first_cat_simple)', 'PR(>F)']\n",
    "                mixed_cat2_pval = mixed_anova.loc['C(second_cat_simple)', 'PR(>F)']\n",
    "                mixed_num1_pval = mixed_anova.loc['C(first_num_simple)', 'PR(>F)']\n",
    "                mixed_num2_pval = mixed_anova.loc['C(second_num_simple)', 'PR(>F)']\n",
    "                \n",
    "            except Exception as e:\n",
    "                pass\n",
    "            \n",
    "            # === Neuron Type Classification ===\n",
    "            neuron_type = \"no_selectivity\"  # Default for neurons not in selectivity analysis\n",
    "            if len(selectivity_info) > 0:\n",
    "                neuron_type = \"encoding_only\"  # Has some selectivity\n",
    "                if 'distractor_results_principled' in globals():\n",
    "                    distractor_info = distractor_results_principled[distractor_results_principled['unit_id'] == unit_id]\n",
    "                    if len(distractor_info) > 0:\n",
    "                        neuron_type = distractor_info.iloc[0]['analysis_stage']\n",
    "            \n",
    "            # Mixed selectivity classification\n",
    "            mixed_selectivity_type = \"none\"\n",
    "            if not np.isnan(cat_interaction_pval) and cat_interaction_pval < 0.05:\n",
    "                if not np.isnan(num_interaction_pval) and num_interaction_pval < 0.05:\n",
    "                    mixed_selectivity_type = \"both_interactions\"\n",
    "                else:\n",
    "                    mixed_selectivity_type = \"category_interaction\"\n",
    "            elif not np.isnan(num_interaction_pval) and num_interaction_pval < 0.05:\n",
    "                mixed_selectivity_type = \"numerosity_interaction\"\n",
    "            \n",
    "            # Store results\n",
    "            result = {\n",
    "                'unit_id': unit_id,\n",
    "                'area': area,\n",
    "                'neuron_type': neuron_type,\n",
    "                \n",
    "                # Data quality\n",
    "                'n_trials': len(df_unit),\n",
    "                'n_stim1_categories': n_stim1_cats,\n",
    "                'n_stim2_categories': n_stim2_cats,\n",
    "                'n_stim1_numerosities': n_stim1_nums,\n",
    "                'n_stim2_numerosities': n_stim2_nums,\n",
    "                \n",
    "                # Category interaction analysis\n",
    "                'cat_interaction_pval': cat_interaction_pval,\n",
    "                'cat_main_stim1_pval': cat_main_stim1_pval,\n",
    "                'cat_main_stim2_pval': cat_main_stim2_pval,\n",
    "                'cat_model_r2': cat_model_r2,\n",
    "                'cat_interaction_significant': cat_interaction_pval < 0.05 if not np.isnan(cat_interaction_pval) else False,\n",
    "                \n",
    "                # Numerosity interaction analysis  \n",
    "                'num_interaction_pval': num_interaction_pval,\n",
    "                'num_main_stim1_pval': num_main_stim1_pval,\n",
    "                'num_main_stim2_pval': num_main_stim2_pval,\n",
    "                'num_model_r2': num_model_r2,\n",
    "                'num_interaction_significant': num_interaction_pval < 0.05 if not np.isnan(num_interaction_pval) else False,\n",
    "                \n",
    "                # Mixed model\n",
    "                'mixed_model_r2': mixed_model_r2,\n",
    "                'mixed_cat1_pval': mixed_cat1_pval,\n",
    "                'mixed_cat2_pval': mixed_cat2_pval,\n",
    "                'mixed_num1_pval': mixed_num1_pval,\n",
    "                'mixed_num2_pval': mixed_num2_pval,\n",
    "                \n",
    "                # Classification\n",
    "                'mixed_selectivity_type': mixed_selectivity_type,\n",
    "                'any_interaction_significant': (\n",
    "                    (cat_interaction_pval < 0.05 if not np.isnan(cat_interaction_pval) else False) or\n",
    "                    (num_interaction_pval < 0.05 if not np.isnan(num_interaction_pval) else False)\n",
    "                ),\n",
    "                \n",
    "                # Selectivity info (may be False if neuron wasn't in original analysis)\n",
    "                'is_first_cat_selective': is_first_cat_selective,\n",
    "                'is_second_cat_selective': is_second_cat_selective,\n",
    "                'is_first_num_selective': is_first_num_selective,\n",
    "                'is_second_num_selective': is_second_num_selective,\n",
    "            }\n",
    "            \n",
    "            interaction_results.append(result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing unit {unit_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    interaction_df = pd.DataFrame(interaction_results)\n",
    "    \n",
    "    if len(interaction_df) == 0:\n",
    "        print(\"No neurons analyzed successfully!\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # === SUMMARY ANALYSIS ===\n",
    "    print(f\"\\n=== MIXED SELECTIVITY RESULTS ===\")\n",
    "    print(f\"Successfully analyzed: {len(interaction_df)} neurons (all recorded neurons)\")\n",
    "    print(f\"Note: This includes neurons with no individual feature selectivity\")\n",
    "    print(f\"Mixed selectivity can emerge in apparently 'non-selective' neurons\")\n",
    "    \n",
    "    # Overall interaction statistics\n",
    "    cat_interactions = interaction_df['cat_interaction_significant'].sum()\n",
    "    num_interactions = interaction_df['num_interaction_significant'].sum()\n",
    "    any_interactions = interaction_df['any_interaction_significant'].sum()\n",
    "    \n",
    "    print(f\"\\n=== INTERACTION EFFECTS ===\")\n",
    "    print(f\"Category x Category interactions: {cat_interactions}/{len(interaction_df)} ({cat_interactions/len(interaction_df)*100:.1f}%)\")\n",
    "    print(f\"Numerosity x Numerosity interactions: {num_interactions}/{len(interaction_df)} ({num_interactions/len(interaction_df)*100:.1f}%)\")\n",
    "    print(f\"Any interaction effects: {any_interactions}/{len(interaction_df)} ({any_interactions/len(interaction_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Mixed selectivity type distribution\n",
    "    print(f\"\\n=== MIXED SELECTIVITY TYPES ===\")\n",
    "    type_counts = interaction_df['mixed_selectivity_type'].value_counts()\n",
    "    for stype, count in type_counts.items():\n",
    "        print(f\"  {stype}: {count} neurons ({count/len(interaction_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Analysis by neuron type\n",
    "    print(f\"\\n=== BY NEURON TYPE ===\")\n",
    "    for ntype in interaction_df['neuron_type'].unique():\n",
    "        type_data = interaction_df[interaction_df['neuron_type'] == ntype]\n",
    "        n_total = len(type_data)\n",
    "        n_any_interact = type_data['any_interaction_significant'].sum()\n",
    "        n_cat_interact = type_data['cat_interaction_significant'].sum()\n",
    "        n_num_interact = type_data['num_interaction_significant'].sum()\n",
    "        \n",
    "        print(f\"  {ntype} (n={n_total}):\")\n",
    "        print(f\"    Any interactions: {n_any_interact}/{n_total} ({n_any_interact/n_total*100:.1f}%)\")\n",
    "        print(f\"    Category interactions: {n_cat_interact}/{n_total} ({n_cat_interact/n_total*100:.1f}%)\")\n",
    "        print(f\"    Numerosity interactions: {n_num_interact}/{n_total} ({n_num_interact/n_total*100:.1f}%)\")\n",
    "    \n",
    "    # Analysis by brain area\n",
    "    print(f\"\\n=== BY BRAIN AREA ===\")\n",
    "    for area in interaction_df['area'].unique():\n",
    "        area_data = interaction_df[interaction_df['area'] == area]\n",
    "        if len(area_data) >= 5:  # Only show areas with enough neurons\n",
    "            n_total = len(area_data)\n",
    "            n_any_interact = area_data['any_interaction_significant'].sum()\n",
    "            n_cat_interact = area_data['cat_interaction_significant'].sum()\n",
    "            avg_r2 = area_data['cat_model_r2'].mean()\n",
    "            \n",
    "            print(f\"  {area} (n={n_total}): {n_any_interact} interactions ({n_any_interact/n_total*100:.1f}%), avg R²={avg_r2:.3f}\")\n",
    "    \n",
    "    # Model performance comparison\n",
    "    print(f\"\\n=== MODEL PERFORMANCE ===\")\n",
    "    avg_cat_r2 = interaction_df['cat_model_r2'].mean()\n",
    "    avg_num_r2 = interaction_df['num_model_r2'].mean() \n",
    "    avg_mixed_r2 = interaction_df['mixed_model_r2'].mean()\n",
    "    \n",
    "    print(f\"Average R² - Category model: {avg_cat_r2:.3f}\")\n",
    "    print(f\"Average R² - Numerosity model: {avg_num_r2:.3f}\")\n",
    "    print(f\"Average R² - Mixed model: {avg_mixed_r2:.3f}\")\n",
    "    \n",
    "    # Save results\n",
    "    os.makedirs(\"mixed_selectivity_analysis\", exist_ok=True)\n",
    "    interaction_df.to_csv(\"mixed_selectivity_analysis/stim1_stim2_interactions.csv\", index=False)\n",
    "    print(f\"\\nResults saved to: mixed_selectivity_analysis/stim1_stim2_interactions.csv\")\n",
    "    \n",
    "    # === COMPARISON TO PARTHASARATHY ET AL. ===\n",
    "    print(f\"\\n=== COMPARISON TO PARTHASARATHY ET AL. (2017) ===\")\n",
    "    print(f\"Mixed selectivity prevalence in your data: {any_interactions/len(interaction_df)*100:.1f}%\")\n",
    "    print(\"Parthasarathy et al. found ~30-50% of PFC neurons with mixed selectivity\")\n",
    "    print(\"Key difference: They used all neurons (like this analysis), not pre-selected ones\")\n",
    "    \n",
    "    # Additional breakdown\n",
    "    has_individual_selectivity = interaction_df[\n",
    "        (interaction_df['is_first_cat_selective']) | \n",
    "        (interaction_df['is_second_cat_selective']) |\n",
    "        (interaction_df['is_first_num_selective']) | \n",
    "        (interaction_df['is_second_num_selective'])\n",
    "    ]\n",
    "    \n",
    "    no_individual_selectivity = interaction_df[\n",
    "        (~interaction_df['is_first_cat_selective']) & \n",
    "        (~interaction_df['is_second_cat_selective']) &\n",
    "        (~interaction_df['is_first_num_selective']) & \n",
    "        (~interaction_df['is_second_num_selective'])\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\n=== BREAKDOWN BY INDIVIDUAL SELECTIVITY ===\")\n",
    "    print(f\"Neurons with individual feature selectivity: {len(has_individual_selectivity)}\")\n",
    "    if len(has_individual_selectivity) > 0:\n",
    "        print(f\"  Mixed selectivity in these: {has_individual_selectivity['any_interaction_significant'].sum()}/{len(has_individual_selectivity)} ({has_individual_selectivity['any_interaction_significant'].mean()*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"Neurons with NO individual selectivity: {len(no_individual_selectivity)}\")\n",
    "    if len(no_individual_selectivity) > 0:\n",
    "        print(f\"  Mixed selectivity in these: {no_individual_selectivity['any_interaction_significant'].sum()}/{len(no_individual_selectivity)} ({no_individual_selectivity['any_interaction_significant'].mean()*100:.1f}%)\")\n",
    "        print(f\"  → This is the key insight: pure mixed selectivity without individual tuning!\")\n",
    "    \n",
    "   \n",
    "    print(\"\\nThis analysis uses the Parthasarathy approach: all neurons, no pre-selection\")\n",
    "    print(\"Mixed selectivity = neurons encoding feature combinations, not just individual features\")\n",
    "    \n",
    "    return interaction_df\n",
    "\n",
    "\n",
    "# === RUN THE MIXED SELECTIVITY ANALYSIS ===\n",
    "if 'selectivity_results' in locals() and 'data_filtered' in locals():\n",
    "    print(\"Running mixed selectivity interaction analysis...\")\n",
    "    mixed_selectivity_results = analyze_mixed_selectivity_interactions(data_filtered, selectivity_results)\n",
    "    \n",
    "    print(\"\\nMixed selectivity analysis complete!\")\n",
    "    print(\"This tests whether neurons encode combinations of stim1 and stim2 features,\")\n",
    "    print(\"as described in Parthasarathy et al. (2017) Nature Neuroscience.\")\n",
    "    \n",
    "else:\n",
    "    print(\"Need selectivity_results and data_filtered to run this analysis!\")\n",
    "    print(\"Make sure you've run the neuron selectivity analysis first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94619f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== MIXED SELECTIVITY: STIM1 x STIM2 INTERACTION ANALYSIS =====\n",
    "# Testing for interaction effects during stim2 presentation\n",
    "# Inspired by Parthasarathy et al. (2017) Nature Neuroscience\n",
    "\n",
    "# import statsmodels.formula.api as smf\n",
    "# import statsmodels.api as sm\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "\n",
    "def analyze_mixed_selectivity_interactions(data_filtered, selectivity_results):\n",
    "    \"\"\"\n",
    "    Test for mixed selectivity during stim2 presentation:\n",
    "    1. CategoryStim1 x CategoryStim2 interaction effects\n",
    "    2. NumerosityStim1 x NumerosityStim2 interaction effects\n",
    "    \n",
    "    This tests whether stim2 responses depend on specific combinations of \n",
    "    stim1 and stim2 features, not just individual features.\n",
    "    \n",
    "    Reference: Parthasarathy et al. (2017) Nature Neuroscience\n",
    "    \"Mixed selectivity morphs population codes in prefrontal cortex\"\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== MIXED SELECTIVITY INTERACTION ANALYSIS ===\")\n",
    "    print(\"Testing for Stim1 x Stim2 interaction effects during stim2 presentation...\")\n",
    "    \n",
    "    # Use all responsive neurons for broader analysis\n",
    "    responsive_neurons = selectivity_results[selectivity_results[\"is_any_selective\"]].copy()\n",
    "    print(f\"Analyzing {len(responsive_neurons)} responsive neurons\")\n",
    "    \n",
    "    interaction_results = []\n",
    "    \n",
    "    for _, neuron_row in tqdm(responsive_neurons.iterrows(), desc=\"Testing interactions\"):\n",
    "        unit_id = neuron_row['unit_id']\n",
    "        \n",
    "        try:\n",
    "            df_unit = data_filtered[data_filtered[\"unit_id\"] == unit_id].reset_index(drop=True)\n",
    "            if df_unit.empty:\n",
    "                continue\n",
    "            \n",
    "            area = neuron_row['area']\n",
    "            \n",
    "            # Check if we have enough data for interaction analysis\n",
    "            # Need multiple levels of each factor\n",
    "            n_stim1_cats = df_unit['first_cat_simple'].nunique()\n",
    "            n_stim2_cats = df_unit['second_cat_simple'].nunique()\n",
    "            n_stim1_nums = df_unit['first_num_simple'].nunique()\n",
    "            n_stim2_nums = df_unit['second_num_simple'].nunique()\n",
    "            \n",
    "            if n_stim1_cats < 2 or n_stim2_cats < 2:\n",
    "                continue\n",
    "            \n",
    "            # === ANALYSIS 1: Category x Category Interaction ===\n",
    "            cat_interaction_pval = np.nan\n",
    "            cat_main_stim1_pval = np.nan\n",
    "            cat_main_stim2_pval = np.nan\n",
    "            cat_model_r2 = np.nan\n",
    "            \n",
    "            try:\n",
    "                # 2-way ANOVA with interaction: stim2_response ~ stim1_cat * stim2_cat\n",
    "                cat_model = smf.ols(\n",
    "                    \"fr_enc2_epoch ~ C(first_cat_simple) * C(second_cat_simple)\", \n",
    "                    data=df_unit\n",
    "                ).fit()\n",
    "                cat_anova = sm.stats.anova_lm(cat_model, typ=2)\n",
    "                \n",
    "                # Extract p-values\n",
    "                cat_main_stim1_pval = cat_anova.loc['C(first_cat_simple)', 'PR(>F)']\n",
    "                cat_main_stim2_pval = cat_anova.loc['C(second_cat_simple)', 'PR(>F)']\n",
    "                cat_interaction_pval = cat_anova.loc['C(first_cat_simple):C(second_cat_simple)', 'PR(>F)']\n",
    "                cat_model_r2 = cat_model.rsquared\n",
    "                \n",
    "            except Exception as e:\n",
    "                pass  # Keep NaN values\n",
    "            \n",
    "            # === ANALYSIS 2: Numerosity x Numerosity Interaction ===\n",
    "            num_interaction_pval = np.nan\n",
    "            num_main_stim1_pval = np.nan\n",
    "            num_main_stim2_pval = np.nan\n",
    "            num_model_r2 = np.nan\n",
    "            \n",
    "            if n_stim1_nums >= 2 and n_stim2_nums >= 2:\n",
    "                try:\n",
    "                    # 2-way ANOVA with interaction: stim2_response ~ stim1_num * stim2_num\n",
    "                    num_model = smf.ols(\n",
    "                        \"fr_enc2_epoch ~ C(first_num_simple) * C(second_num_simple)\", \n",
    "                        data=df_unit\n",
    "                    ).fit()\n",
    "                    num_anova = sm.stats.anova_lm(num_model, typ=2)\n",
    "                    \n",
    "                    # Extract p-values\n",
    "                    num_main_stim1_pval = num_anova.loc['C(first_num_simple)', 'PR(>F)']\n",
    "                    num_main_stim2_pval = num_anova.loc['C(second_num_simple)', 'PR(>F)']\n",
    "                    num_interaction_pval = num_anova.loc['C(first_num_simple):C(second_num_simple)', 'PR(>F)']\n",
    "                    num_model_r2 = num_model.rsquared\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    pass  # Keep NaN values\n",
    "            \n",
    "            # === ANALYSIS 3: Mixed Model with Both Features ===\n",
    "            mixed_model_r2 = np.nan\n",
    "            mixed_cat1_pval = np.nan\n",
    "            mixed_cat2_pval = np.nan\n",
    "            mixed_num1_pval = np.nan\n",
    "            mixed_num2_pval = np.nan\n",
    "            \n",
    "            try:\n",
    "                # Full model with all features (no interactions for complexity)\n",
    "                mixed_model = smf.ols(\n",
    "                    \"fr_enc2_epoch ~ C(first_cat_simple) + C(second_cat_simple) + C(first_num_simple) + C(second_num_simple)\", \n",
    "                    data=df_unit\n",
    "                ).fit()\n",
    "                mixed_anova = sm.stats.anova_lm(mixed_model, typ=2)\n",
    "                \n",
    "                mixed_model_r2 = mixed_model.rsquared\n",
    "                mixed_cat1_pval = mixed_anova.loc['C(first_cat_simple)', 'PR(>F)']\n",
    "                mixed_cat2_pval = mixed_anova.loc['C(second_cat_simple)', 'PR(>F)']\n",
    "                mixed_num1_pval = mixed_anova.loc['C(first_num_simple)', 'PR(>F)']\n",
    "                mixed_num2_pval = mixed_anova.loc['C(second_num_simple)', 'PR(>F)']\n",
    "                \n",
    "            except Exception as e:\n",
    "                pass\n",
    "            \n",
    "            # === Classification ===\n",
    "            # Determine neuron type\n",
    "            neuron_type = \"encoding_only\"\n",
    "            if 'distractor_results_principled' in globals():\n",
    "                distractor_info = distractor_results_principled[distractor_results_principled['unit_id'] == unit_id]\n",
    "                if len(distractor_info) > 0:\n",
    "                    neuron_type = distractor_info.iloc[0]['analysis_stage']\n",
    "            \n",
    "            # Mixed selectivity classification\n",
    "            mixed_selectivity_type = \"none\"\n",
    "            if not np.isnan(cat_interaction_pval) and cat_interaction_pval < 0.05:\n",
    "                if not np.isnan(num_interaction_pval) and num_interaction_pval < 0.05:\n",
    "                    mixed_selectivity_type = \"both_interactions\"\n",
    "                else:\n",
    "                    mixed_selectivity_type = \"category_interaction\"\n",
    "            elif not np.isnan(num_interaction_pval) and num_interaction_pval < 0.05:\n",
    "                mixed_selectivity_type = \"numerosity_interaction\"\n",
    "            \n",
    "            # Store results\n",
    "            result = {\n",
    "                'unit_id': unit_id,\n",
    "                'area': area,\n",
    "                'neuron_type': neuron_type,\n",
    "                \n",
    "                # Data quality\n",
    "                'n_trials': len(df_unit),\n",
    "                'n_stim1_categories': n_stim1_cats,\n",
    "                'n_stim2_categories': n_stim2_cats,\n",
    "                'n_stim1_numerosities': n_stim1_nums,\n",
    "                'n_stim2_numerosities': n_stim2_nums,\n",
    "                \n",
    "                # Category interaction analysis\n",
    "                'cat_interaction_pval': cat_interaction_pval,\n",
    "                'cat_main_stim1_pval': cat_main_stim1_pval,\n",
    "                'cat_main_stim2_pval': cat_main_stim2_pval,\n",
    "                'cat_model_r2': cat_model_r2,\n",
    "                'cat_interaction_significant': cat_interaction_pval < 0.05 if not np.isnan(cat_interaction_pval) else False,\n",
    "                \n",
    "                # Numerosity interaction analysis  \n",
    "                'num_interaction_pval': num_interaction_pval,\n",
    "                'num_main_stim1_pval': num_main_stim1_pval,\n",
    "                'num_main_stim2_pval': num_main_stim2_pval,\n",
    "                'num_model_r2': num_model_r2,\n",
    "                'num_interaction_significant': num_interaction_pval < 0.05 if not np.isnan(num_interaction_pval) else False,\n",
    "                \n",
    "                # Mixed model\n",
    "                'mixed_model_r2': mixed_model_r2,\n",
    "                'mixed_cat1_pval': mixed_cat1_pval,\n",
    "                'mixed_cat2_pval': mixed_cat2_pval,\n",
    "                'mixed_num1_pval': mixed_num1_pval,\n",
    "                'mixed_num2_pval': mixed_num2_pval,\n",
    "                \n",
    "                # Classification\n",
    "                'mixed_selectivity_type': mixed_selectivity_type,\n",
    "                'any_interaction_significant': (\n",
    "                    (cat_interaction_pval < 0.05 if not np.isnan(cat_interaction_pval) else False) or\n",
    "                    (num_interaction_pval < 0.05 if not np.isnan(num_interaction_pval) else False)\n",
    "                ),\n",
    "                \n",
    "                # Selectivity info from original analysis\n",
    "                'is_first_cat_selective': neuron_row['is_first_cat_selective'],\n",
    "                'is_second_cat_selective': neuron_row['is_second_cat_selective'],\n",
    "                'is_first_num_selective': neuron_row['is_first_num_selective'],\n",
    "                'is_second_num_selective': neuron_row['is_second_num_selective'],\n",
    "            }\n",
    "            \n",
    "            interaction_results.append(result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing unit {unit_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    interaction_df = pd.DataFrame(interaction_results)\n",
    "    \n",
    "    if len(interaction_df) == 0:\n",
    "        print(\"No neurons analyzed successfully!\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # === SUMMARY ANALYSIS ===\n",
    "    print(f\"\\n=== MIXED SELECTIVITY RESULTS ===\")\n",
    "    print(f\"Successfully analyzed: {len(interaction_df)} neurons\")\n",
    "    \n",
    "    # Overall interaction statistics\n",
    "    cat_interactions = interaction_df['cat_interaction_significant'].sum()\n",
    "    num_interactions = interaction_df['num_interaction_significant'].sum()\n",
    "    any_interactions = interaction_df['any_interaction_significant'].sum()\n",
    "    \n",
    "    print(f\"\\n=== INTERACTION EFFECTS ===\")\n",
    "    print(f\"Category x Category interactions: {cat_interactions}/{len(interaction_df)} ({cat_interactions/len(interaction_df)*100:.1f}%)\")\n",
    "    print(f\"Numerosity x Numerosity interactions: {num_interactions}/{len(interaction_df)} ({num_interactions/len(interaction_df)*100:.1f}%)\")\n",
    "    print(f\"Any interaction effects: {any_interactions}/{len(interaction_df)} ({any_interactions/len(interaction_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Mixed selectivity type distribution\n",
    "    print(f\"\\n=== MIXED SELECTIVITY TYPES ===\")\n",
    "    type_counts = interaction_df['mixed_selectivity_type'].value_counts()\n",
    "    for stype, count in type_counts.items():\n",
    "        print(f\"  {stype}: {count} neurons ({count/len(interaction_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Analysis by neuron type\n",
    "    print(f\"\\n=== BY NEURON TYPE ===\")\n",
    "    for ntype in interaction_df['neuron_type'].unique():\n",
    "        type_data = interaction_df[interaction_df['neuron_type'] == ntype]\n",
    "        n_total = len(type_data)\n",
    "        n_any_interact = type_data['any_interaction_significant'].sum()\n",
    "        n_cat_interact = type_data['cat_interaction_significant'].sum()\n",
    "        n_num_interact = type_data['num_interaction_significant'].sum()\n",
    "        \n",
    "        print(f\"  {ntype} (n={n_total}):\")\n",
    "        print(f\"    Any interactions: {n_any_interact}/{n_total} ({n_any_interact/n_total*100:.1f}%)\")\n",
    "        print(f\"    Category interactions: {n_cat_interact}/{n_total} ({n_cat_interact/n_total*100:.1f}%)\")\n",
    "        print(f\"    Numerosity interactions: {n_num_interact}/{n_total} ({n_num_interact/n_total*100:.1f}%)\")\n",
    "    \n",
    "    # Analysis by brain area\n",
    "    print(f\"\\n=== BY BRAIN AREA ===\")\n",
    "    for area in interaction_df['area'].unique():\n",
    "        area_data = interaction_df[interaction_df['area'] == area]\n",
    "        if len(area_data) >= 5:  # Only show areas with enough neurons\n",
    "            n_total = len(area_data)\n",
    "            n_any_interact = area_data['any_interaction_significant'].sum()\n",
    "            n_cat_interact = area_data['cat_interaction_significant'].sum()\n",
    "            avg_r2 = area_data['cat_model_r2'].mean()\n",
    "            \n",
    "            print(f\"  {area} (n={n_total}): {n_any_interact} interactions ({n_any_interact/n_total*100:.1f}%), avg R²={avg_r2:.3f}\")\n",
    "    \n",
    "    # Model performance comparison\n",
    "    print(f\"\\n=== MODEL PERFORMANCE ===\")\n",
    "    avg_cat_r2 = interaction_df['cat_model_r2'].mean()\n",
    "    avg_num_r2 = interaction_df['num_model_r2'].mean()\n",
    "    avg_mixed_r2 = interaction_df['mixed_model_r2'].mean()\n",
    "    \n",
    "    print(f\"Average R² - Category model: {avg_cat_r2:.3f}\")\n",
    "    print(f\"Average R² - Numerosity model: {avg_num_r2:.3f}\")\n",
    "    print(f\"Average R² - Mixed model: {avg_mixed_r2:.3f}\")\n",
    "    \n",
    "    # Save results\n",
    "    os.makedirs(\"mixed_selectivity_analysis\", exist_ok=True)\n",
    "    interaction_df.to_csv(\"mixed_selectivity_analysis/stim1_stim2_interactions.csv\", index=False)\n",
    "    print(f\"\\nResults saved to: mixed_selectivity_analysis/stim1_stim2_interactions.csv\")\n",
    "    \n",
    "    # === COMPARISON TO PARTHASARATHY ET AL. ===\n",
    "    print(f\"\\n=== COMPARISON TO PARTHASARATHY ET AL. (2017) ===\")\n",
    "    print(f\"Mixed selectivity prevalence in your data: {any_interactions/len(interaction_df)*100:.1f}%\")\n",
    "    print(\"Parthasarathy et al. found ~30-50% of PFC neurons with mixed selectivity\")\n",
    "    \n",
    "    if any_interactions/len(interaction_df) > 0.3:\n",
    "        print(\"→ Your data shows HIGH levels of mixed selectivity, similar to PFC\")\n",
    "    elif any_interactions/len(interaction_df) > 0.15:\n",
    "        print(\"→ Your data shows MODERATE levels of mixed selectivity\")\n",
    "    else:\n",
    "        print(\"→ Your data shows LOW levels of mixed selectivity\")\n",
    "    \n",
    "    print(\"This suggests population coding may rely on feature combinations rather than pure selectivity\")\n",
    "    \n",
    "    return interaction_df\n",
    "\n",
    "\n",
    "# === RUN THE MIXED SELECTIVITY ANALYSIS ===\n",
    "if 'selectivity_results' in locals() and 'data_filtered' in locals():\n",
    "    print(\"Running mixed selectivity interaction analysis...\")\n",
    "    mixed_selectivity_results = analyze_mixed_selectivity_interactions(data_filtered, selectivity_results)\n",
    "    \n",
    "    print(\"\\nMixed selectivity analysis complete!\")\n",
    "    print(\"This tests whether neurons encode combinations of stim1 and stim2 features,\")\n",
    "    print(\"as described in Parthasarathy et al. (2017) Nature Neuroscience.\")\n",
    "    \n",
    "else:\n",
    "    print(\"Need selectivity_results and data_filtered to run this analysis!\")\n",
    "    print(\"Make sure you've run the neuron selectivity analysis first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
