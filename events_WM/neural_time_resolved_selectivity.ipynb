{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b7ea451",
   "metadata": {},
   "source": [
    "# Time-Resolved Neural Selectivity Analysis\n",
    "Quantifying % of neurons selective for different aspects as function of time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597ad5f9",
   "metadata": {},
   "source": [
    "## 1: ENVIRONMENT SETUP AND IMPORTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f682a959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data manipulation and analysis\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.io import loadmat\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Statistical analysis\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"✓ All packages imported successfully\")\n",
    "print(\"✓ Environment setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88907cce",
   "metadata": {},
   "source": [
    "## 2: LOAD DATA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f4dea8",
   "metadata": {},
   "source": [
    "### Loading MATLAB Data Files\n",
    "\n",
    "Loading of MATLAB data files containing neural recordings and trial information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a654eebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all WMB_P*_v7.mat files in the current directory\n",
    "mat_files = glob.glob('./WMB_P*_v7.mat')\n",
    "print(f\"Found {len(mat_files)} .mat files: {[os.path.basename(f) for f in mat_files]}\")\n",
    "\n",
    "# Initialize lists to store data from each file\n",
    "cell_mats = []\n",
    "total_mats = []\n",
    "\n",
    "# Load each file and append its data\n",
    "for mat_file in mat_files:\n",
    "    print(f\"\\nLoading {mat_file}...\")\n",
    "    mat_data = loadmat(mat_file)\n",
    "    cell_mats.append(mat_data['cellStatsAll'])\n",
    "    total_mats.append(mat_data['totStats'])\n",
    "\n",
    "# Print shapes of loaded data for debugging\n",
    "print(\"\\nShapes of loaded data:\")\n",
    "for i, (cell, total) in enumerate(zip(cell_mats, total_mats)):\n",
    "    print(f\"File {i}: cell_mat shape: {cell.shape}, total_mat shape: {total.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdc5e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the data\n",
    "# For cell_mat, we need to handle the different dimensions\n",
    "# First, convert each cell_mat to a list of records\n",
    "all_cell_records = []\n",
    "for cell_mat in cell_mats:\n",
    "    # Convert to list of records\n",
    "    cell_list = cell_mat[0]  # now shape is (n,)\n",
    "    records = []\n",
    "    for cell in cell_list:\n",
    "        record = {key: cell[key] for key in cell.dtype.names}\n",
    "        records.append(record)\n",
    "    all_cell_records.extend(records)\n",
    "\n",
    "# Convert combined records to DataFrame\n",
    "df = pd.DataFrame(all_cell_records)\n",
    "\n",
    "# For total_mat, we can concatenate directly since they have the same structure\n",
    "total_mat = np.concatenate(total_mats, axis=0)\n",
    "\n",
    "print(f\"\\nCombined data shape - total_mat: {total_mat.shape}\")\n",
    "print(f\"\\nCombined data shape - df: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a555c9",
   "metadata": {},
   "source": [
    "### Translates numeric area codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8006817",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_area_codes(area_column):\n",
    "    \n",
    "    mapping = {\n",
    "        1: 'RH', 2: 'LH', 3: 'RA', 4: 'LA', 5: 'RAC', 6: 'LAC',\n",
    "        7: 'RSMA', 8: 'LSMA', 9: 'RPT', 10: 'LPT', 11: 'ROFC', 12: 'LOFC',\n",
    "        50: 'RFFA', 51: 'REC', 52: 'RCM', 53: 'LCM', 54: 'RPUL', 55: 'LPUL',\n",
    "        56: 'N/A', 57: 'RPRV', 58: 'LPRV'\n",
    "    }\n",
    "    \n",
    "    labels = []\n",
    "    for code in area_column:\n",
    "        label = mapping.get(code, 'Unknown')\n",
    "        labels.append(label)\n",
    "    \n",
    "    return dict(Counter(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77735a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neuron number in each area\n",
    "area_codes = total_mat[:, 3]\n",
    "\n",
    "counts = count_area_codes(area_codes)\n",
    "print(\"Area counts (no prefix):\")\n",
    "for area, count in counts.items():\n",
    "    print(f\"{area}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bbedec",
   "metadata": {},
   "source": [
    "### Data Preprocessing & Filtering\n",
    "\n",
    "First, we'll format the cell data and collapse brain areas into broader regions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5503409",
   "metadata": {},
   "outputs": [],
   "source": [
    "collapsed_area_map = {\n",
    "    1: 'H', 2: 'H',\n",
    "    3: 'A', 4: 'A',\n",
    "    5: 'AC', 6: 'AC',\n",
    "    7: 'SMA', 8: 'SMA',\n",
    "    9: 'PT', 10: 'PT',\n",
    "    11: 'OFC', 12: 'OFC',\n",
    "    50: 'FFA', 51: 'EC',\n",
    "    52: 'CM', 53: 'CM',\n",
    "    54: 'PUL', 55: 'PUL',\n",
    "    56: 'N/A', 57: 'PRV', 58: 'PRV'\n",
    "}\n",
    "# Convert brain area codes in the DataFrame\n",
    "df['brainAreaOfCell'] = df['brainAreaOfCell'].apply(\n",
    "    lambda x: collapsed_area_map.get(int(x[0, 0]), 'Unknown') if isinstance(x, np.ndarray) else collapsed_area_map.get(x, 'Unknown')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38bebe9",
   "metadata": {},
   "source": [
    "### Quality Control & Unit Selection\n",
    "\n",
    "Filtering criteria for reliable unit selection:\n",
    "- Minimum firing rate threshold\n",
    "- Signal quality metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f53661c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out units with low firing rate\n",
    "fr = df['timestamps'].apply(lambda x: len(x) / (x[-1] - x[0]) * 1e6)\n",
    "df_sample_new = df[fr > 0.1].reset_index(drop=True)\n",
    "\n",
    "# unit id\n",
    "df_sample_new = df_sample_new.reset_index(drop=True)\n",
    "df_sample_new[\"unit_id\"] = df_sample_new.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad7b7dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out units with low firing rate\n",
    "fr = df['timestamps'].apply(lambda x: len(x) / (x[-1] - x[0]) * 1e6)\n",
    "df_sample_new = df[fr > 0.1].reset_index(drop=True)\n",
    "\n",
    "# unit id\n",
    "df_sample_new = df_sample_new.reset_index(drop=True)\n",
    "df_sample_new[\"unit_id\"] = df_sample_new.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6730337e",
   "metadata": {},
   "source": [
    "###  Neural Activity & Trial Processing\n",
    "\n",
    "#### Trial Data Extraction\n",
    "\n",
    "Extraction and alignment of trial-specific information with neural data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4d370a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_trial_info(trials_struct, unit_id):\n",
    "    # Build a DataFrame from the trials structure.\n",
    "    df_trial = pd.DataFrame({field: trials_struct[field].squeeze() \n",
    "                             for field in trials_struct.dtype.names})\n",
    "    # Add the unit_id so that you can later separate trials by unit/session.\n",
    "    df_trial[\"unit_id\"] = unit_id\n",
    "    df_trial[\"trial_nr\"] = df_trial[\"trial\"].apply(lambda x: np.squeeze(x).item() if isinstance(x, (list, np.ndarray)) else x) - 1 # Adjust for 0-indexing\n",
    "    return df_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ba9428a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_info_list = []\n",
    "for idx, row in df_sample_new.iterrows():\n",
    "    # Use the unit identifier from this row\n",
    "    unit_id = row[\"unit_id\"]  \n",
    "    # Extract the trial DataFrame, including the unit identifier.\n",
    "    trial_info_list.append(extract_trial_info(row[\"Trials\"], unit_id, ))\n",
    "\n",
    "# Concatenate the list of trial info DataFrames into one.\n",
    "trial_info = pd.concat(trial_info_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402880ea",
   "metadata": {},
   "source": [
    "#### Data Integration & Column Selection\n",
    "\n",
    "Merge neural firing rate data with trial information and select relevant columns for analysis. This step combines the computed firing rates with behavioral trial data to enable selectivity analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bb6ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_event_timestamps(df_sample_new, start_idx_col='idxEnc1', end_idx_col='idxDel1', is_window=False, window_size=0.5):\n",
    "    \"\"\"\n",
    "    Extract event timestamps for computing firing rates across task epochs.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_sample_new : DataFrame\n",
    "        Neural data with event indices and timestamps\n",
    "    start_idx_col : str\n",
    "        Column name containing start event indices\n",
    "    end_idx_col : str  \n",
    "        Column name containing end event indices\n",
    "    is_window : bool\n",
    "        If True, extract fixed window around start event; if False, extract epoch between start and end\n",
    "    window_size : float\n",
    "        Window size in seconds (only used if is_window=True)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    epoch_ts : dict\n",
    "        Dictionary mapping unit_id to array of [start_time, end_time] pairs for each trial\n",
    "    \"\"\"\n",
    "    epoch_ts = {}\n",
    "    \n",
    "    for i, row in df_sample_new.iterrows():\n",
    "        unit_id = row['unit_id']\n",
    "        events = row['events'].squeeze()  # Event timestamps array [trial_idx, event_type, timestamp]\n",
    "        \n",
    "        if is_window:\n",
    "            # Extract fixed window around specific event (e.g., ±0.5s around response)\n",
    "            idxs = row[start_idx_col].squeeze() - 1  # Convert to 0-based indexing\n",
    "            extracted = events[idxs]\n",
    "            center_times = extracted[:, 0]  # Get timestamp column\n",
    "            window_start = center_times - window_size * 1e6  # Convert to microseconds\n",
    "            window_end = center_times + window_size * 1e6\n",
    "            combined = np.column_stack((window_start, window_end))\n",
    "        else:\n",
    "            # Extract epoch between two task events (e.g., encoding to delay)\n",
    "            idxs_start = row[start_idx_col].squeeze() - 1  # Start event indices (0-based)\n",
    "            idxs_end = row[end_idx_col].squeeze() - 1      # End event indices (0-based)\n",
    "            \n",
    "            # Handle mismatched trial counts between start and end events\n",
    "            min_length = min(len(idxs_start), len(idxs_end))\n",
    "            idxs_start = idxs_start[:min_length]\n",
    "            idxs_end = idxs_end[:min_length]\n",
    "            \n",
    "            # Extract timestamps for start and end events\n",
    "            extracted_start = events[idxs_start]  # [n_trials, 3] \n",
    "            extracted_end = events[idxs_end]      # [n_trials, 3]\n",
    "\n",
    "            # Combine start and end timestamps for each trial\n",
    "            combined = np.column_stack((extracted_start[:, 0], extracted_end[:, 0]))\n",
    "        \n",
    "        epoch_ts[unit_id] = combined\n",
    "        \n",
    "    return epoch_ts\n",
    "\n",
    "\n",
    "def compute_firing_rates(df_sample_new, start_idx_col='idxEnc1', end_idx_col='idxDel1', \n",
    "                         fr_prefix='fr', is_window=False, window_size=0.5):\n",
    "    \"\"\"\n",
    "    Compute firing rates for a specific task epoch.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_sample_new : DataFrame\n",
    "        Neural data containing spike timestamps and event indices\n",
    "    start_idx_col : str\n",
    "        Column with start event indices\n",
    "    end_idx_col : str\n",
    "        Column with end event indices  \n",
    "    fr_prefix : str\n",
    "        Prefix for the firing rate column name\n",
    "    is_window : bool\n",
    "        Whether to use fixed window (True) or epoch between events (False)\n",
    "    window_size : float\n",
    "        Window size in seconds (only for is_window=True)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    df_sample_new : DataFrame\n",
    "        Input dataframe with added firing rate columns\n",
    "    \"\"\"\n",
    "    # Extract epoch timestamps for all units\n",
    "    epoch_ts = extract_event_timestamps(df_sample_new, start_idx_col, end_idx_col, is_window, window_size)\n",
    "    \n",
    "    # Compute baseline firing rate (1 second before first stimulus) - only on first call\n",
    "    if 'fr_baseline' not in df_sample_new.columns:\n",
    "        print(\"Computing baseline firing rates...\")\n",
    "        \n",
    "        # Get first stimulus onset timestamps\n",
    "        enc_ts = extract_event_timestamps(df_sample_new, 'idxEnc1', 'idxEnc1')\n",
    "        \n",
    "        # Create baseline windows: 1 second before stimulus onset\n",
    "        baseline_ts = {}\n",
    "        for unit_id, timestamps in enc_ts.items():\n",
    "            baseline_start = timestamps[:, 0] - 1e6  # 1 second before (microseconds)\n",
    "            baseline_end = timestamps[:, 0]          # End at stimulus onset\n",
    "            baseline_ts[unit_id] = np.column_stack((baseline_start, baseline_end))\n",
    "            \n",
    "        # Compute baseline firing rates for each unit and trial\n",
    "        df_sample_new['fr_baseline'] = df_sample_new.apply(\n",
    "            lambda row: [\n",
    "                # Count spikes in baseline window and convert to Hz\n",
    "                np.sum((np.ravel(row[\"timestamps\"]) >= baseline_on) & \n",
    "                       (np.ravel(row[\"timestamps\"]) < baseline_off)) / ((baseline_off - baseline_on) / 1e6)\n",
    "                for baseline_on, baseline_off in baseline_ts[row[\"unit_id\"]]\n",
    "            ],\n",
    "            axis=1\n",
    "        )\n",
    "    \n",
    "    # Compute firing rates for the specified epoch\n",
    "    epoch_col = f\"{fr_prefix}_epoch\"\n",
    "    print(f\"Computing firing rates for {epoch_col}...\")\n",
    "    \n",
    "    df_sample_new[epoch_col] = df_sample_new.apply(\n",
    "        lambda row: [\n",
    "            # Count spikes in epoch window and convert to Hz\n",
    "            np.sum((np.ravel(row[\"timestamps\"]) >= epoch_on) & \n",
    "                   (np.ravel(row[\"timestamps\"]) < epoch_off)) / ((epoch_off - epoch_on) / 1e6)\n",
    "            for epoch_on, epoch_off in epoch_ts[row[\"unit_id\"]]\n",
    "        ],\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Add trial numbers if not already present\n",
    "    if \"trial_nr\" not in df_sample_new.columns:\n",
    "        df_sample_new[\"trial_nr\"] = df_sample_new[epoch_col].apply(lambda x: np.arange(len(x)))\n",
    "    \n",
    "    return df_sample_new\n",
    "\n",
    "\n",
    "print(\"Computing firing rates for all task epochs...\")\n",
    "\n",
    "# Compute firing rates for each task epoch\n",
    "# Each call adds a new firing rate column to the dataframe\n",
    "\n",
    "# 1. First stimulus encoding period (from stimulus onset to delay start)\n",
    "df_sample_new = compute_firing_rates(df_sample_new, 'idxEnc1', 'idxDel1', fr_prefix='fr')\n",
    "\n",
    "# 2. First delay period (between first stimulus and second stimulus)  \n",
    "df_sample_new = compute_firing_rates(df_sample_new, 'idxDel1', 'idxEnc2', fr_prefix='fr_del1')\n",
    "\n",
    "# 3. Second stimulus encoding period (from second stimulus onset to second delay)\n",
    "df_sample_new = compute_firing_rates(df_sample_new, 'idxEnc2', 'idxDel2', fr_prefix='fr_enc2')\n",
    "\n",
    "# 4. Second delay period (between second stimulus and probe)\n",
    "df_sample_new = compute_firing_rates(df_sample_new, 'idxDel2', 'idxProbeOn', fr_prefix='fr_del2')\n",
    "\n",
    "# 5. Response period (±0.5 second window around button press)\n",
    "df_sample_new = compute_firing_rates(df_sample_new, 'idxResp', None, fr_prefix='fr_resp', \n",
    "                                   is_window=True, window_size=0.5)\n",
    "\n",
    "print(\"Expanding dataframe from unit-based to trial-based format...\")\n",
    "\n",
    "# Transform from unit-level to trial-level format\n",
    "# Each unit currently has lists of firing rates (one per trial)\n",
    "# After exploding, each row will represent one unit-trial combination\n",
    "columns_to_explode = ['fr_baseline', 'fr_epoch', 'fr_del1_epoch', 'fr_enc2_epoch', \n",
    "                      'fr_del2_epoch', 'fr_resp_epoch', \"trial_nr\"]\n",
    "df_sample_new = df_sample_new.explode(columns_to_explode)\n",
    "\n",
    "print(f\"Final neural data shape: {df_sample_new.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabd0f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Merging neural data with trial information...\")\n",
    "\n",
    "# Reset indices to ensure proper alignment\n",
    "df_sample_new = df_sample_new.reset_index(drop=True)\n",
    "trial_info = trial_info.reset_index(drop=True)\n",
    "\n",
    "# Merge neural firing rates with behavioral trial data\n",
    "# This combines unit-trial firing rates with stimulus information for each trial\n",
    "data = pd.merge(\n",
    "    df_sample_new,\n",
    "    trial_info,\n",
    "    on=[\"unit_id\", \"trial_nr\"],  # Join on unit and trial identifiers\n",
    "    how=\"left\",                  # Keep all neural data, add matching trial info\n",
    ").infer_objects()\n",
    "\n",
    "print(f\"Combined data shape: {data.shape}\")\n",
    "\n",
    "# Select columns needed for selectivity analysis\n",
    "# Include: neural identifiers, firing rates, stimulus properties, task timing\n",
    "cols_to_keep = [\n",
    "    # Neural data identifiers and firing rates\n",
    "    \"unit_id\", \"timestamps\", \"brainAreaOfCell\", \n",
    "    \"fr_epoch\", \"fr_baseline\", \"fr_del1_epoch\", \"fr_enc2_epoch\", \"fr_del2_epoch\", \"fr_resp_epoch\", \n",
    "    \"trial_nr\",\n",
    "    \n",
    "    # Stimulus properties for selectivity analysis\n",
    "    \"first_cat\", \"second_cat\", \"first_num\", \"second_num\",\n",
    "    \"first_pic\", \"second_pic\", \"probe_cat\", \"probe_pic\",\n",
    "    \n",
    "    # Task variables\n",
    "    \"probe_validity\", \"probe_num\", \"correct_answer\",\n",
    "    \"rt\", \"acc\", \"key\", \"cat_comparison\", \n",
    "    \n",
    "    # Event timing and metadata\n",
    "    \"events\", \"nTrials\", \"Trials\", \n",
    "    \"idxEnc1\", \"idxEnc2\", \"idxDel1\", \"idxDel2\", \"idxProbeOn\", \"idxResp\", \n",
    "    \"nrProcessed\", \"periods_Enc1\", \"periods_Enc2\", \"periods_Del1\", \"periods_Del2\", \n",
    "    \"periods_Probe\", \"periods_Resp\", \"prestimEnc\", \"prestimMaint\", \"prestimProbe\",\n",
    "    \"prestimButtonPress\", \"poststimEnc\", \"poststimMaint\", \"poststimProbe\", \n",
    "    \"poststimButtonPress\", \"sessionIdx\", \"channel\", \"cellNr\", \"sessionID\", \"origClusterID\"\n",
    "]\n",
    "\n",
    "# Create filtered dataset with only necessary columns\n",
    "data_filtered = data[cols_to_keep].copy()\n",
    "\n",
    "print(\"Converting stimulus variables to simple string format for statistical analysis...\")\n",
    "\n",
    "# Convert stimulus variables to simple string format\n",
    "# MATLAB arrays need to be converted to hashable Python types for grouping operations\n",
    "data_filtered[\"first_cat_simple\"] = data_filtered[\"first_cat\"].apply(\n",
    "    lambda x: str(np.squeeze(x)) if isinstance(x, (list, np.ndarray)) else str(x)\n",
    ")\n",
    "data_filtered[\"second_cat_simple\"] = data_filtered[\"second_cat\"].apply(\n",
    "    lambda x: str(np.squeeze(x)) if isinstance(x, (list, np.ndarray)) else str(x)\n",
    ")\n",
    "data_filtered[\"first_num_simple\"] = data_filtered[\"first_num\"].apply(\n",
    "    lambda x: str(np.squeeze(x)) if isinstance(x, (list, np.ndarray)) else str(x)\n",
    ")\n",
    "data_filtered[\"second_num_simple\"] = data_filtered[\"second_num\"].apply(\n",
    "    lambda x: str(np.squeeze(x)) if isinstance(x, (list, np.ndarray)) else str(x)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d58115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data structure\n",
    "print(f\"Data shape: {data_filtered.shape if 'data_filtered' in locals() else 'data_filtered not found'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cf48d9",
   "metadata": {},
   "source": [
    "## 3: DEFINE TIME ANALYSIS PARAMETERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37b2b85",
   "metadata": {},
   "source": [
    "Configuration parameters for time-resolved analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cfe928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the columns in full_results to see what's available\n",
    "print(\"Columns in full_results:\")\n",
    "print(full_results.columns.tolist())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(full_results.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196937ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeAnalysisConfig:\n",
    "    \n",
    "    \n",
    "    # Time window parameters (all in milliseconds)\n",
    "    WINDOW_WIDTH = 500      # Width of each analysis window\n",
    "    STEP_SIZE = 100         # Step between windows\n",
    "    \n",
    "    # Analysis period relative to stimulus onset\n",
    "    TIME_START = -500      # Pre-stimulus baseline\n",
    "    TIME_END = 6000        # Post-stimulus through response\n",
    "    \n",
    "    # Statistical parameters\n",
    "    P_THRESHOLD = 0.05     # Significance threshold for selectivity\n",
    "    MIN_TRIALS = 4        # Minimum trials per condition per neuron\n",
    "    MIN_FIRING_RATE = 0.5  # Minimum average firing rate (Hz)\n",
    "    \n",
    "    # Event timings (in ms, relative to first stimulus)\n",
    "    EVENTS = {\n",
    "        'first_stimulus': 0,\n",
    "        'delay1': 1000,\n",
    "        'second_stimulus': 2000, \n",
    "        'delay2': 3000,\n",
    "        'probe': 5500,\n",
    "        'response': 5500\n",
    "    }\n",
    "\n",
    "config = TimeAnalysisConfig()\n",
    "\n",
    "# Generate time windows\n",
    "time_windows = []\n",
    "window_centers = []\n",
    "\n",
    "for t_start in range(config.TIME_START, config.TIME_END - config.WINDOW_WIDTH, config.STEP_SIZE):\n",
    "    t_end = t_start + config.WINDOW_WIDTH\n",
    "    time_windows.append((t_start, t_end))\n",
    "    window_centers.append(t_start + config.WINDOW_WIDTH // 2)\n",
    "\n",
    "print(f\"✓ Created {len(time_windows)} time windows\")\n",
    "print(f\"✓ Window width: {config.WINDOW_WIDTH}ms, Step: {config.STEP_SIZE}ms\")\n",
    "print(f\"✓ Analysis period: {config.TIME_START}ms to {config.TIME_END}ms\")\n",
    "print(f\"✓ First few window centers: {window_centers[:5]}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd37e53",
   "metadata": {},
   "source": [
    "## 4: EXTRACT TRIAL-ALIGNED SPIKE TIMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65df9d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_trial_aligned_spikes(data_filtered, unit_id, alignment_event='first_stimulus'):\n",
    "    \n",
    "    \n",
    "    # Get data for this unit\n",
    "    unit_data = data_filtered[data_filtered['unit_id'] == unit_id].copy()\n",
    "    \n",
    "    if len(unit_data) == 0:\n",
    "        return {}\n",
    "    \n",
    "    # Get spike timestamps (convert to seconds)\n",
    "    spike_times = np.asarray(unit_data[\"timestamps\"].iloc[0]).flatten() / 1e6\n",
    "    \n",
    "    # Get trial events and alignment indices\n",
    "    events = unit_data['events'].iloc[0].squeeze()\n",
    "    \n",
    "    # Choose alignment based on event type\n",
    "    if alignment_event == 'first_stimulus':\n",
    "        align_indices = unit_data['idxEnc1'].iloc[0].squeeze() - 1\n",
    "    elif alignment_event == 'second_stimulus':\n",
    "        align_indices = unit_data['idxEnc2'].iloc[0].squeeze() - 1\n",
    "    elif alignment_event == 'probe':\n",
    "        align_indices = unit_data['idxProbeOn'].iloc[0].squeeze() - 1\n",
    "    else:\n",
    "        align_indices = unit_data['idxEnc1'].iloc[0].squeeze() - 1  # Default to first stimulus\n",
    "    \n",
    "    trial_data = {}\n",
    "    \n",
    "    # Extract spikes for each trial\n",
    "    for trial_idx, event_idx in enumerate(align_indices):\n",
    "        try:\n",
    "            # Get alignment time for this trial\n",
    "            align_time = events[event_idx, 0] / 1e6  # Convert to seconds\n",
    "            \n",
    "            # Extract spikes in analysis window around alignment\n",
    "            window_start = align_time + config.TIME_START / 1000  # Convert ms to s\n",
    "            window_end = align_time + config.TIME_END / 1000\n",
    "            \n",
    "            trial_spikes = spike_times[(spike_times >= window_start) & (spike_times <= window_end)]\n",
    "            trial_spikes_aligned = (trial_spikes - align_time) * 1000  # Convert to ms relative to event\n",
    "            \n",
    "            # Get trial conditions\n",
    "            trial_data[trial_idx] = {\n",
    "                'spikes': trial_spikes_aligned,\n",
    "                'first_cat': unit_data.iloc[trial_idx]['first_cat_simple'],\n",
    "                'second_cat': unit_data.iloc[trial_idx]['second_cat_simple'],\n",
    "                'first_num': unit_data.iloc[trial_idx]['first_num_simple'],\n",
    "                'second_num': unit_data.iloc[trial_idx]['second_num_simple'],\n",
    "                'probe_validity': unit_data.iloc[trial_idx].get('probe_validity', 'unknown')\n",
    "            }\n",
    "            \n",
    "        except (IndexError, KeyError):\n",
    "            # Skip trials with missing data\n",
    "            continue\n",
    "            \n",
    "    return trial_data\n",
    "\n",
    "# Test the function\n",
    "print(\"✓ Trial alignment function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364f120b",
   "metadata": {},
   "source": [
    "## 5: COMPUTE SLIDING WINDOW FIRING RATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25abc1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sliding_window_firing_rates(trial_aligned_spikes, time_windows):\n",
    "    \n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for trial_idx, trial_data in trial_aligned_spikes.items():\n",
    "        spikes = trial_data['spikes']\n",
    "        \n",
    "        for window_idx, (t_start, t_end) in enumerate(time_windows):\n",
    "            # Count spikes in this window\n",
    "            spike_count = np.sum((spikes >= t_start) & (spikes < t_end))\n",
    "            \n",
    "            # Convert to firing rate (Hz)\n",
    "            window_duration = (t_end - t_start) / 1000  # Convert ms to seconds\n",
    "            firing_rate = spike_count / window_duration\n",
    "            \n",
    "            # Store results\n",
    "            results.append({\n",
    "                'trial_idx': trial_idx,\n",
    "                'window_idx': window_idx,\n",
    "                'window_start': t_start,\n",
    "                'window_end': t_end,\n",
    "                'window_center': (t_start + t_end) / 2,\n",
    "                'firing_rate': firing_rate,\n",
    "                'spike_count': spike_count,\n",
    "                'first_cat': trial_data['first_cat'],\n",
    "                'second_cat': trial_data['second_cat'],\n",
    "                'first_num': trial_data['first_num'],\n",
    "                'second_num': trial_data['second_num'],\n",
    "                'probe_validity': trial_data['probe_validity']\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Test the function\n",
    "print(\"✓ Sliding window firing rate function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5ee420",
   "metadata": {},
   "source": [
    "## 6: TEST SELECTIVITY AT SINGLE TIME POINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "499f9723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_selectivity_at_timepoint_corrected(firing_rate_data, window_center):\n",
    "    \"\"\"\n",
    "    Test neural selectivity with proper statistical framework\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Group by unit\n",
    "    for unit_id, unit_data in firing_rate_data.groupby('unit_id'):\n",
    "        \n",
    "        # Check minimum requirements\n",
    "        if len(unit_data) < config.MIN_TRIALS:\n",
    "            continue\n",
    "            \n",
    "        if unit_data['firing_rate'].mean() < config.MIN_FIRING_RATE:\n",
    "            continue\n",
    "            \n",
    "        # Skip if no variance\n",
    "        if unit_data['firing_rate'].std() == 0:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Test category selectivity for first stimulus\n",
    "            categories = unit_data['first_cat'].unique()\n",
    "            if len(categories) > 1:\n",
    "                cat1_pval = stats.f_oneway(*[unit_data[unit_data['first_cat'] == cat]['firing_rate'] \n",
    "                                           for cat in categories]).pvalue\n",
    "            else:\n",
    "                cat1_pval = 1.0\n",
    "            \n",
    "            # Test category selectivity for second stimulus  \n",
    "            categories = unit_data['second_cat'].unique()\n",
    "            if len(categories) > 1:\n",
    "                cat2_pval = stats.f_oneway(*[unit_data[unit_data['second_cat'] == cat]['firing_rate'] \n",
    "                                           for cat in categories]).pvalue\n",
    "            else:\n",
    "                cat2_pval = 1.0\n",
    "            \n",
    "            # Test numerosity selectivity for first stimulus\n",
    "            numbers = unit_data['first_num'].unique()\n",
    "            if len(numbers) > 1:\n",
    "                num1_pval = stats.f_oneway(*[unit_data[unit_data['first_num'] == num]['firing_rate'] \n",
    "                                           for num in numbers]).pvalue\n",
    "            else:\n",
    "                num1_pval = 1.0\n",
    "                \n",
    "            # Test numerosity selectivity for second stimulus\n",
    "            numbers = unit_data['second_num'].unique()\n",
    "            if len(numbers) > 1:\n",
    "                num2_pval = stats.f_oneway(*[unit_data[unit_data['second_num'] == num]['firing_rate'] \n",
    "                                           for num in numbers]).pvalue\n",
    "            else:\n",
    "                num2_pval = 1.0\n",
    "            \n",
    "            # Store results with RAW p-values (correction applied later)\n",
    "            results.append({\n",
    "                'unit_id': unit_id,\n",
    "                'window_center': window_center,\n",
    "                'n_trials': len(unit_data),\n",
    "                'mean_firing_rate': unit_data['firing_rate'].mean(),\n",
    "                \n",
    "                # Raw p-values (no correction yet)\n",
    "                'cat1_pval': cat1_pval,\n",
    "                'cat2_pval': cat2_pval, \n",
    "                'num1_pval': num1_pval,\n",
    "                'num2_pval': num2_pval,\n",
    "                \n",
    "                # Uncorrected significance (for comparison)\n",
    "                'cat1_selective_uncorrected': cat1_pval < config.P_THRESHOLD,\n",
    "                'cat2_selective_uncorrected': cat2_pval < config.P_THRESHOLD,\n",
    "                'num1_selective_uncorrected': num1_pval < config.P_THRESHOLD,\n",
    "                'num2_selective_uncorrected': num2_pval < config.P_THRESHOLD\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Handle any statistical errors\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def apply_multiple_comparison_corrections(full_results):\n",
    "    \"\"\"\n",
    "    Apply different multiple comparison correction methods\n",
    "    \"\"\"\n",
    "    from statsmodels.stats.multitest import multipletests\n",
    "    \n",
    "    print(\"Applying multiple comparison corrections...\")\n",
    "    \n",
    "    corrected_results = []\n",
    "    \n",
    "    for unit_id, unit_data in tqdm(full_results.groupby('unit_id'), desc=\"Correcting p-values\"):\n",
    "        unit_data = unit_data.copy().sort_values('window_center')\n",
    "        \n",
    "        # METHOD 1: FDR Correction (across time for each condition)\n",
    "        for condition in ['cat1', 'cat2', 'num1', 'num2']:\n",
    "            pval_col = f'{condition}_pval'\n",
    "            pvals = unit_data[pval_col].values\n",
    "            \n",
    "            # Apply FDR correction\n",
    "            _, fdr_corrected, _, _ = multipletests(pvals, method='fdr_bh')\n",
    "            unit_data[f'{condition}_pval_fdr'] = fdr_corrected\n",
    "            unit_data[f'{condition}_selective_fdr'] = fdr_corrected < 0.05\n",
    "        \n",
    "        # METHOD 2: Bonferroni Correction (very conservative)\n",
    "        bonferroni_threshold = 0.05 / (len(unit_data) * 4)  # 4 conditions tested\n",
    "        for condition in ['cat1', 'cat2', 'num1', 'num2']:\n",
    "            pval_col = f'{condition}_pval'\n",
    "            unit_data[f'{condition}_selective_bonferroni'] = unit_data[pval_col] < bonferroni_threshold\n",
    "        \n",
    "        # METHOD 3: Cluster-based correction (consecutive significant windows)\n",
    "        min_consecutive = 3  # Require 3 consecutive significant windows\n",
    "        \n",
    "        for condition in ['cat1', 'cat2', 'num1', 'num2']:\n",
    "            uncorrected_col = f'{condition}_selective_uncorrected'\n",
    "            sig_windows = unit_data[uncorrected_col].values\n",
    "            \n",
    "            # Find consecutive runs\n",
    "            cluster_corrected = np.zeros_like(sig_windows, dtype=bool)\n",
    "            current_run = []\n",
    "            \n",
    "            for i, is_sig in enumerate(sig_windows):\n",
    "                if is_sig:\n",
    "                    current_run.append(i)\n",
    "                else:\n",
    "                    if len(current_run) >= min_consecutive:\n",
    "                        cluster_corrected[current_run] = True\n",
    "                    current_run = []\n",
    "            \n",
    "            # Check final run\n",
    "            if len(current_run) >= min_consecutive:\n",
    "                cluster_corrected[current_run] = True\n",
    "            \n",
    "            unit_data[f'{condition}_selective_cluster'] = cluster_corrected\n",
    "        \n",
    "        corrected_results.append(unit_data)\n",
    "    \n",
    "    return pd.concat(corrected_results, ignore_index=True)\n",
    "\n",
    "\n",
    "def calculate_corrected_population_timecourse(full_results_corrected):\n",
    "    \"\"\"\n",
    "    Calculate population percentages using different correction methods\n",
    "    \"\"\"\n",
    "    \n",
    "    # Group by time and calculate percentages for each correction method\n",
    "    methods = ['uncorrected', 'fdr', 'bonferroni', 'cluster']\n",
    "    conditions = ['cat1', 'cat2', 'num1', 'num2']\n",
    "    \n",
    "    population_data = []\n",
    "    \n",
    "    for time_point, time_data in full_results_corrected.groupby('window_center'):\n",
    "        \n",
    "        row = {'time_ms': time_point, 'n_units': len(time_data)}\n",
    "        \n",
    "        for method in methods:\n",
    "            for condition in conditions:\n",
    "                if method == 'uncorrected':\n",
    "                    col_name = f'{condition}_selective_uncorrected'\n",
    "                else:\n",
    "                    col_name = f'{condition}_selective_{method}'\n",
    "                \n",
    "                if col_name in time_data.columns:\n",
    "                    percentage = time_data[col_name].mean() * 100\n",
    "                    row[f'pct_{condition}_selective_{method}'] = percentage\n",
    "                else:\n",
    "                    row[f'pct_{condition}_selective_{method}'] = 0\n",
    "        \n",
    "        population_data.append(row)\n",
    "    \n",
    "    return pd.DataFrame(population_data)\n",
    "\n",
    "\n",
    "# UPDATED MAIN ANALYSIS FUNCTION\n",
    "def analyze_time_resolved_selectivity_with_corrections(data_filtered, alignment_event='first_stimulus'):\n",
    "    \"\"\"\n",
    "    Main analysis with proper multiple comparison corrections\n",
    "    \"\"\"\n",
    "    print(f\"Starting time-resolved selectivity analysis with corrections...\")\n",
    "    \n",
    "    # Get all unique units\n",
    "    all_units = data_filtered['unit_id'].unique()\n",
    "    all_selectivity_results = []\n",
    "    \n",
    "    # Process each time window (same as before)\n",
    "    for window_idx, window_center in enumerate(tqdm(window_centers, desc=\"Processing time windows\")):\n",
    "        \n",
    "        timepoint_data = []\n",
    "        \n",
    "        for unit_id in all_units:\n",
    "            trial_spikes = extract_trial_aligned_spikes(data_filtered, unit_id, alignment_event)\n",
    "            \n",
    "            if len(trial_spikes) == 0:\n",
    "                continue\n",
    "                \n",
    "            unit_rates = compute_sliding_window_firing_rates(trial_spikes, [time_windows[window_idx]])\n",
    "            \n",
    "            if len(unit_rates) > 0:\n",
    "                unit_rates['unit_id'] = unit_id\n",
    "                timepoint_data.append(unit_rates)\n",
    "        \n",
    "        if len(timepoint_data) == 0:\n",
    "            continue\n",
    "            \n",
    "        timepoint_df = pd.concat(timepoint_data, ignore_index=True)\n",
    "        \n",
    "        # Use corrected version of selectivity testing\n",
    "        selectivity_results = test_selectivity_at_timepoint_corrected(timepoint_df, window_center)\n",
    "        all_selectivity_results.append(selectivity_results)\n",
    "    \n",
    "    # Combine results across time\n",
    "    if len(all_selectivity_results) > 0:\n",
    "        full_results = pd.concat(all_selectivity_results, ignore_index=True)\n",
    "        \n",
    "        # Apply multiple comparison corrections\n",
    "        full_results_corrected = apply_multiple_comparison_corrections(full_results)\n",
    "        \n",
    "        # Calculate population timecourses for each correction method\n",
    "        population_timecourse = calculate_corrected_population_timecourse(full_results_corrected)\n",
    "        \n",
    "        print(f\"✓ Analysis complete with corrections!\")\n",
    "        print(f\"✓ {len(population_timecourse)} timepoints analyzed\")\n",
    "        print(f\"✓ Applied uncorrected, FDR, Bonferroni, and cluster-based corrections\")\n",
    "        \n",
    "        return population_timecourse, full_results_corrected\n",
    "    \n",
    "    else:\n",
    "        print(\"✗ No results generated\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9f8b54",
   "metadata": {},
   "source": [
    "## 7: MAIN ANALYSIS PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f99b785c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting time-resolved selectivity analysis with corrections...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing time windows:  15%|█▌        | 9/60 [04:06<23:45, 27.96s/it]"
     ]
    }
   ],
   "source": [
    "population_timecourse, full_results = analyze_time_resolved_selectivity_with_corrections(data_filtered)\n",
    "\n",
    "print(population_timecourse.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e488dd52",
   "metadata": {},
   "source": [
    "## 8: PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3a9e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# TIME-RESOLVED NEURAL SELECTIVITY VISUALIZATION FUNCTIONS\n",
    "# ===================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Set up plotting parameters\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.linewidth'] = 1.5\n",
    "plt.rcParams['xtick.major.size'] = 6\n",
    "plt.rcParams['ytick.major.size'] = 6\n",
    "\n",
    "def plot_main_selectivity_timecourse(population_timecourse, save_path=\"selectivity_timecourse.png\"):\n",
    "    \"\"\"\n",
    "    Main figure: % of neurons selective for different aspects over time\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Creating main selectivity timecourse plot...\")\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(16, 12), sharex=True)\n",
    "    \n",
    "    # Colors for different selectivity types\n",
    "    colors = {\n",
    "        'cat1': '#E74C3C',  # Red\n",
    "        'cat2': '#3498DB',  # Blue  \n",
    "        'num1': '#E67E22',  # Orange\n",
    "        'num2': '#9B59B6'   # Purple\n",
    "    }\n",
    "    \n",
    "    # Plot 1: Category Selectivity\n",
    "    ax1.plot(population_timecourse['time_ms'], population_timecourse['pct_cat1_selective'], \n",
    "             color=colors['cat1'], linewidth=3, label='First Category', alpha=0.9)\n",
    "    ax1.plot(population_timecourse['time_ms'], population_timecourse['pct_cat2_selective'], \n",
    "             color=colors['cat2'], linewidth=3, label='Second Category', alpha=0.9)\n",
    "    \n",
    "    ax1.fill_between(population_timecourse['time_ms'], population_timecourse['pct_cat1_selective'], \n",
    "                     alpha=0.3, color=colors['cat1'])\n",
    "    ax1.fill_between(population_timecourse['time_ms'], population_timecourse['pct_cat2_selective'], \n",
    "                     alpha=0.3, color=colors['cat2'])\n",
    "    \n",
    "    ax1.set_ylabel('% Neurons Selective\\nfor Category', fontsize=14, fontweight='bold')\n",
    "    ax1.set_title('Time Course of Neural Selectivity During Working Memory Task', \n",
    "                  fontsize=16, fontweight='bold', pad=20)\n",
    "    ax1.legend(loc='upper right', fontsize=12)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_ylim(0, max(population_timecourse[['pct_cat1_selective', 'pct_cat2_selective']].max()) * 1.1)\n",
    "    \n",
    "    # Plot 2: Numerosity Selectivity  \n",
    "    ax2.plot(population_timecourse['time_ms'], population_timecourse['pct_num1_selective'], \n",
    "             color=colors['num1'], linewidth=3, label='First Numerosity', alpha=0.9)\n",
    "    ax2.plot(population_timecourse['time_ms'], population_timecourse['pct_num2_selective'], \n",
    "             color=colors['num2'], linewidth=3, label='Second Numerosity', alpha=0.9)\n",
    "    \n",
    "    ax2.fill_between(population_timecourse['time_ms'], population_timecourse['pct_num1_selective'], \n",
    "                     alpha=0.3, color=colors['num1'])\n",
    "    ax2.fill_between(population_timecourse['time_ms'], population_timecourse['pct_num2_selective'], \n",
    "                     alpha=0.3, color=colors['num2'])\n",
    "    \n",
    "    ax2.set_ylabel('% Neurons Selective\\nfor Numerosity', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Time from First Stimulus Onset (ms)', fontsize=14, fontweight='bold')\n",
    "    ax2.legend(loc='upper right', fontsize=12)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_ylim(0, max(population_timecourse[['pct_num1_selective', 'pct_num2_selective']].max()) * 1.1)\n",
    "    \n",
    "    # Add task event markers\n",
    "    events = {\n",
    "        'First Stimulus': 0,\n",
    "        'First Delay': 1000, \n",
    "        'Second Stimulus': 2000,\n",
    "        'Second Delay': 3000,\n",
    "        'Probe': 5500\n",
    "    }\n",
    "    \n",
    "    for ax in [ax1, ax2]:\n",
    "        for event_name, event_time in events.items():\n",
    "            ax.axvline(event_time, color='black', linestyle='--', alpha=0.7, linewidth=2)\n",
    "            ax.text(event_time + 50, ax.get_ylim()[1] * 0.9, event_name, \n",
    "                   rotation=90, fontsize=10, alpha=0.8, fontweight='bold')\n",
    "    \n",
    "    # Add epoch shading\n",
    "    epoch_colors = {\n",
    "        'Encoding 1': (0, 1000, '#FFE5E5'),\n",
    "        'Delay 1': (1000, 2000, '#E5F2FF'), \n",
    "        'Encoding 2': (2000, 3000, '#FFE5E5'),\n",
    "        'Delay 2': (3000, 5500, '#E5F2FF'),\n",
    "        'Response': (5500, 6000, '#F0F0F0')\n",
    "    }\n",
    "    \n",
    "    for ax in [ax1, ax2]:\n",
    "        for epoch_name, (start, end, color) in epoch_colors.items():\n",
    "            ax.axvspan(start, end, alpha=0.2, color=color, zorder=0)\n",
    "    \n",
    "    # Add sample size info\n",
    "    avg_units = population_timecourse['n_units'].mean()\n",
    "    fig.text(0.02, 0.02, f'Average units per timepoint: {avg_units:.0f}', \n",
    "             fontsize=10, style='italic')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"✓ Saved main timecourse plot: {save_path}\")\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_selectivity_heatmap(population_timecourse, save_path=\"selectivity_heatmap.png\"):\n",
    "    \"\"\"\n",
    "    Heatmap showing all selectivity types over time\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Creating selectivity heatmap...\")\n",
    "    \n",
    "    # Prepare data for heatmap\n",
    "    heatmap_data = population_timecourse.set_index('time_ms')[\n",
    "        ['pct_cat1_selective', 'pct_cat2_selective', 'pct_num1_selective', 'pct_num2_selective']\n",
    "    ].T\n",
    "    \n",
    "    # Rename for better labels\n",
    "    heatmap_data.index = ['First Category', 'Second Category', 'First Numerosity', 'Second Numerosity']\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(16, 6))\n",
    "    \n",
    "    # Create heatmap\n",
    "    im = ax.imshow(heatmap_data.values, aspect='auto', cmap='viridis', interpolation='gaussian')\n",
    "    \n",
    "    # Customize axes\n",
    "    ax.set_xticks(np.arange(0, len(heatmap_data.columns), 5))\n",
    "    ax.set_xticklabels(heatmap_data.columns[::5])\n",
    "    ax.set_yticks(range(len(heatmap_data.index)))\n",
    "    ax.set_yticklabels(heatmap_data.index, fontsize=12)\n",
    "    \n",
    "    ax.set_xlabel('Time from First Stimulus Onset (ms)', fontsize=14, fontweight='bold')\n",
    "    ax.set_title('Neural Selectivity Heatmap: % of Neurons Over Time', \n",
    "                 fontsize=16, fontweight='bold', pad=20)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "    cbar.set_label('% Neurons Selective', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Add task event lines\n",
    "    events = [0, 1000, 2000, 3000, 5500]  # Convert to indices\n",
    "    time_points = heatmap_data.columns.values\n",
    "    \n",
    "    for event_time in events:\n",
    "        # Find closest time index\n",
    "        event_idx = np.argmin(np.abs(time_points - event_time))\n",
    "        ax.axvline(event_idx, color='white', linestyle='--', alpha=0.8, linewidth=2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"✓ Saved selectivity heatmap: {save_path}\")\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_brain_region_comparison(full_results, save_path=\"brain_region_comparison.png\"):\n",
    "    \"\"\"\n",
    "    Compare selectivity across brain regions\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Creating brain region comparison...\")\n",
    "    \n",
    "    # Calculate average selectivity by brain region across time\n",
    "    region_data = []\n",
    "    \n",
    "    for region in full_results['brainAreaOfCell'].unique():\n",
    "        region_subset = full_results[full_results['brainAreaOfCell'] == region]\n",
    "        \n",
    "        region_summary = region_subset.groupby('window_center').agg({\n",
    "            'cat1_selective': 'mean',\n",
    "            'cat2_selective': 'mean', \n",
    "            'num1_selective': 'mean',\n",
    "            'num2_selective': 'mean',\n",
    "            'unit_id': 'count'\n",
    "        }).reset_index()\n",
    "        \n",
    "        region_summary['region'] = region\n",
    "        region_data.append(region_summary)\n",
    "    \n",
    "    region_df = pd.concat(region_data, ignore_index=True)\n",
    "    \n",
    "    # Create subplot for each brain region\n",
    "    regions = sorted(full_results['brainAreaOfCell'].unique())\n",
    "    n_regions = len(regions)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12), sharex=True, sharey=True)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    colors = ['#E74C3C', '#3498DB', '#E67E22', '#9B59B6']\n",
    "    labels = ['First Category', 'Second Category', 'First Numerosity', 'Second Numerosity']\n",
    "    \n",
    "    for i, region in enumerate(regions):\n",
    "        if i >= len(axes):\n",
    "            break\n",
    "            \n",
    "        ax = axes[i]\n",
    "        region_data = region_df[region_df['region'] == region]\n",
    "        \n",
    "        if len(region_data) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Plot each selectivity type\n",
    "        for j, (col, color, label) in enumerate(zip(\n",
    "            ['cat1_selective', 'cat2_selective', 'num1_selective', 'num2_selective'],\n",
    "            colors, labels)):\n",
    "            \n",
    "            ax.plot(region_data['window_center'], region_data[col] * 100, \n",
    "                   color=color, linewidth=2.5, label=label, alpha=0.8)\n",
    "        \n",
    "        ax.set_title(f'{region} (n={region_data[\"unit_id\"].iloc[0]:.0f})', \n",
    "                    fontsize=14, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_ylim(0, 60)\n",
    "        \n",
    "        # Add task events\n",
    "        events = [0, 1000, 2000, 3000, 5500]\n",
    "        for event_time in events:\n",
    "            ax.axvline(event_time, color='gray', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        if i == 0:  # Add legend to first subplot\n",
    "            ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # Remove empty subplots\n",
    "    for i in range(n_regions, len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "    \n",
    "    fig.text(0.5, 0.02, 'Time from First Stimulus Onset (ms)', \n",
    "             ha='center', fontsize=14, fontweight='bold')\n",
    "    fig.text(0.02, 0.5, '% Neurons Selective', va='center', rotation=90, \n",
    "             fontsize=14, fontweight='bold')\n",
    "    fig.suptitle('Neural Selectivity by Brain Region', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"✓ Saved brain region comparison: {save_path}\")\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_peak_selectivity_analysis(population_timecourse, save_path=\"peak_selectivity_analysis.png\"):\n",
    "    \"\"\"\n",
    "    Analyze timing of peak selectivity for each condition\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Creating peak selectivity analysis...\")\n",
    "    \n",
    "    # Find peak times and values\n",
    "    conditions = ['pct_cat1_selective', 'pct_cat2_selective', 'pct_num1_selective', 'pct_num2_selective']\n",
    "    condition_names = ['First Category', 'Second Category', 'First Numerosity', 'Second Numerosity']\n",
    "    colors = ['#E74C3C', '#3498DB', '#E67E22', '#9B59B6']\n",
    "    \n",
    "    peak_data = []\n",
    "    for condition, name in zip(conditions, condition_names):\n",
    "        peak_idx = population_timecourse[condition].argmax()\n",
    "        peak_time = population_timecourse.iloc[peak_idx]['time_ms']\n",
    "        peak_value = population_timecourse.iloc[peak_idx][condition]\n",
    "        \n",
    "        peak_data.append({\n",
    "            'condition': name,\n",
    "            'peak_time': peak_time,\n",
    "            'peak_value': peak_value\n",
    "        })\n",
    "    \n",
    "    peak_df = pd.DataFrame(peak_data)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Plot 1: Peak times\n",
    "    bars1 = ax1.bar(peak_df['condition'], peak_df['peak_time'], color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax1.set_ylabel('Peak Time (ms)', fontsize=12, fontweight='bold')\n",
    "    ax1.set_title('Timing of Peak Selectivity', fontsize=14, fontweight='bold')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, time in zip(bars1, peak_df['peak_time']):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 50,\n",
    "                f'{time:.0f}ms', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Add task event reference lines\n",
    "    events = {'First Stim': 0, 'Second Stim': 2000, 'Probe': 5500}\n",
    "    for event, time in events.items():\n",
    "        ax1.axhline(time, color='gray', linestyle='--', alpha=0.5)\n",
    "        ax1.text(0.1, time + 100, event, fontsize=10, alpha=0.7)\n",
    "    \n",
    "    # Plot 2: Peak values\n",
    "    bars2 = ax2.bar(peak_df['condition'], peak_df['peak_value'], color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax2.set_ylabel('Peak Selectivity (%)', fontsize=12, fontweight='bold')\n",
    "    ax2.set_title('Maximum Selectivity Reached', fontsize=14, fontweight='bold')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars2, peak_df['peak_value']):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                f'{value:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"✓ Saved peak selectivity analysis: {save_path}\")\n",
    "    plt.show()\n",
    "    \n",
    "    return fig, peak_df\n",
    "\n",
    "\n",
    "def plot_selectivity_onset_analysis(population_timecourse, threshold=10, save_path=\"selectivity_onset_analysis.png\"):\n",
    "    \"\"\"\n",
    "    Analyze when selectivity first emerges (onset analysis)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Creating selectivity onset analysis...\")\n",
    "    \n",
    "    conditions = ['pct_cat1_selective', 'pct_cat2_selective', 'pct_num1_selective', 'pct_num2_selective']\n",
    "    condition_names = ['First Category', 'Second Category', 'First Numerosity', 'Second Numerosity']\n",
    "    colors = ['#E74C3C', '#3498DB', '#E67E22', '#9B59B6']\n",
    "    \n",
    "    onset_data = []\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    \n",
    "    for condition, name, color in zip(conditions, condition_names, colors):\n",
    "        # Find first time point where selectivity exceeds threshold\n",
    "        above_threshold = population_timecourse[population_timecourse[condition] > threshold]\n",
    "        \n",
    "        if len(above_threshold) > 0:\n",
    "            onset_time = above_threshold.iloc[0]['time_ms']\n",
    "            onset_data.append({'condition': name, 'onset_time': onset_time})\n",
    "            \n",
    "            # Plot full timecourse\n",
    "            ax.plot(population_timecourse['time_ms'], population_timecourse[condition], \n",
    "                   color=color, linewidth=3, label=name, alpha=0.8)\n",
    "            \n",
    "            # Mark onset\n",
    "            ax.scatter([onset_time], [threshold], color=color, s=100, zorder=5, \n",
    "                      marker='o', edgecolor='black', linewidth=2)\n",
    "            ax.text(onset_time + 100, threshold + 1, f'{onset_time:.0f}ms', \n",
    "                   color=color, fontweight='bold', fontsize=10)\n",
    "    \n",
    "    # Add threshold line\n",
    "    ax.axhline(threshold, color='black', linestyle=':', linewidth=2, alpha=0.7, \n",
    "              label=f'Threshold ({threshold}%)')\n",
    "    \n",
    "    # Add task events\n",
    "    events = [0, 1000, 2000, 3000, 5500]\n",
    "    for event_time in events:\n",
    "        ax.axvline(event_time, color='gray', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    ax.set_xlabel('Time from First Stimulus Onset (ms)', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('% Neurons Selective', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'Selectivity Onset Analysis (Threshold: {threshold}%)', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"✓ Saved selectivity onset analysis: {save_path}\")\n",
    "    plt.show()\n",
    "    \n",
    "    onset_df = pd.DataFrame(onset_data) if onset_data else pd.DataFrame()\n",
    "    return fig, onset_df\n",
    "\n",
    "\n",
    "def create_comprehensive_report(population_timecourse, full_results, save_prefix=\"selectivity_analysis\"):\n",
    "    \"\"\"\n",
    "    Create all visualizations and save them\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Creating comprehensive visualization report...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create main plots\n",
    "    fig1 = plot_main_selectivity_timecourse(population_timecourse, f\"{save_prefix}_main_timecourse.png\")\n",
    "    \n",
    "    fig2 = plot_selectivity_heatmap(population_timecourse, f\"{save_prefix}_heatmap.png\")\n",
    "    \n",
    "    # Skip brain region comparison if brainAreaOfCell column is missing\n",
    "    fig3 = None\n",
    "    if 'brainAreaOfCell' in full_results.columns:\n",
    "        fig3 = plot_brain_region_comparison(full_results, f\"{save_prefix}_brain_regions.png\")\n",
    "    else:\n",
    "        print(\"⚠️ Skipping brain region comparison - 'brainAreaOfCell' column not found\")\n",
    "    \n",
    "    fig4, peak_df = plot_peak_selectivity_analysis(population_timecourse, f\"{save_prefix}_peak_analysis.png\")\n",
    "    \n",
    "    fig5, onset_df = plot_selectivity_onset_analysis(population_timecourse, f\"{save_prefix}_onset_analysis.png\", threshold=10)\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SUMMARY STATISTICS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\nPeak Selectivity Times:\")\n",
    "    print(peak_df.to_string(index=False))\n",
    "    \n",
    "    if len(onset_df) > 0:\n",
    "        print(\"\\nSelectivity Onset Times (>10% threshold):\")\n",
    "        print(onset_df.to_string(index=False))\n",
    "    \n",
    "    print(f\"\\nAverage number of units analyzed: {population_timecourse['n_units'].mean():.1f}\")\n",
    "    print(f\"Time range analyzed: {population_timecourse['time_ms'].min():.0f} to {population_timecourse['time_ms'].max():.0f} ms\")\n",
    "    \n",
    "    # Only print brain regions if the column exists\n",
    "    if 'brainAreaOfCell' in full_results.columns:\n",
    "        print(f\"Number of brain regions: {full_results['brainAreaOfCell'].nunique()}\")\n",
    "    else:\n",
    "        print(\"Brain region information not available\")\n",
    "    \n",
    "    overall_max = population_timecourse[['pct_cat1_selective', 'pct_cat2_selective', \n",
    "                                       'pct_num1_selective', 'pct_num2_selective']].max().max()\n",
    "    print(f\"Maximum selectivity reached: {overall_max:.1f}%\")\n",
    "    \n",
    "    print(\"\\n✓ All visualizations complete!\")\n",
    "    \n",
    "    return {\n",
    "        'main_timecourse': fig1,\n",
    "        'heatmap': fig2, \n",
    "        'brain_regions': fig3,\n",
    "        'peak_analysis': fig4,\n",
    "        'onset_analysis': fig5,\n",
    "        'peak_data': peak_df,\n",
    "        'onset_data': onset_df\n",
    "    }\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# QUICK START: Run this to create all plots\n",
    "# ===================================================================\n",
    "\n",
    "# Uncomment to run all visualizations:\n",
    "# results = create_comprehensive_report(population_timecourse, full_results)\n",
    "\n",
    "print(\"✓ All visualization functions loaded!\")\n",
    "print(\"\\nTo create all plots, run:\")\n",
    "print(\"results = create_comprehensive_report(population_timecourse, full_results)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86a1ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add brain area back to full_results\n",
    "\n",
    "# Check what columns exist in your current data\n",
    "print(\"📊 CURRENT DATA STATUS:\")\n",
    "try:\n",
    "    print(f\"✅ data_filtered shape: {data_filtered.shape}\")\n",
    "    print(f\"✅ data_filtered columns: {list(data_filtered.columns)}\")\n",
    "    print(f\"❌ Contains brainAreaOfCell: {'brainAreaOfCell' in data_filtered.columns}\")\n",
    "except NameError:\n",
    "    print(\"❌ data_filtered not found\")\n",
    "\n",
    "# Check if original processed data still has brain area info\n",
    "print(\"\\n📊 CHECKING ORIGINAL DATA:\")\n",
    "try:\n",
    "    print(f\"✅ df_sample_new shape: {df_sample_new.shape}\")\n",
    "    print(f\"✅ Contains brainAreaOfCell: {'brainAreaOfCell' in df_sample_new.columns}\")\n",
    "    if 'brainAreaOfCell' in df_sample_new.columns:\n",
    "        print(f\"✅ Brain areas in df_sample_new: {df_sample_new['brainAreaOfCell'].unique()}\")\n",
    "        print(\"🎯 FOUND IT! Brain area info is in df_sample_new\")\n",
    "except NameError:\n",
    "    print(\"❌ df_sample_new not found\")\n",
    "\n",
    "# Check if we can trace back to the original df\n",
    "try:\n",
    "    print(f\"\\n✅ df shape: {df.shape}\")\n",
    "    print(f\"✅ Contains brainAreaOfCell: {'brainAreaOfCell' in df.columns}\")\n",
    "    if 'brainAreaOfCell' in df.columns:\n",
    "        print(f\"✅ Brain areas in df: {df['brainAreaOfCell'].unique()}\")\n",
    "except NameError:\n",
    "    print(\"❌ df not found\")\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# SOLUTION 1: RECREATE BRAIN AREA MAPPING FROM AVAILABLE DATA\n",
    "# ===================================================================\n",
    "\n",
    "def create_brain_area_mapping_from_source():\n",
    "    \"\"\"\n",
    "    Recreate the unit_id to brain area mapping from the available source data\n",
    "    \"\"\"\n",
    "    print(\"\\n🛠️  CREATING BRAIN AREA MAPPING...\")\n",
    "    \n",
    "    # Try to find brain area info in available DataFrames\n",
    "    brain_area_source = None\n",
    "    \n",
    "    # Option 1: Use df_sample_new if available\n",
    "    try:\n",
    "        if 'brainAreaOfCell' in df_sample_new.columns:\n",
    "            brain_area_source = df_sample_new[['unit_id', 'brainAreaOfCell']].drop_duplicates()\n",
    "            print(\"✅ Using brain area info from df_sample_new\")\n",
    "    except NameError:\n",
    "        pass\n",
    "    \n",
    "    # Option 2: Use df if df_sample_new not available\n",
    "    if brain_area_source is None:\n",
    "        try:\n",
    "            if 'brainAreaOfCell' in df.columns:\n",
    "                # Need to add unit_id to df first\n",
    "                df_with_unit_id = df.copy()\n",
    "                df_with_unit_id['unit_id'] = df_with_unit_id.index\n",
    "                brain_area_source = df_with_unit_id[['unit_id', 'brainAreaOfCell']].drop_duplicates()\n",
    "                print(\"✅ Using brain area info from df\")\n",
    "        except NameError:\n",
    "            pass\n",
    "    \n",
    "    # Option 3: Recreate from original MATLAB data\n",
    "    if brain_area_source is None:\n",
    "        print(\"🔧 Recreating from MATLAB data...\")\n",
    "        try:\n",
    "            # Use the collapsed area mapping we defined earlier\n",
    "            collapsed_area_map = {\n",
    "                1: 'H', 2: 'H', 3: 'A', 4: 'A', 5: 'AC', 6: 'AC',\n",
    "                7: 'SMA', 8: 'SMA', 9: 'PT', 10: 'PT', 11: 'OFC', 12: 'OFC',\n",
    "                50: 'FFA', 51: 'EC', 52: 'CM', 53: 'CM', 54: 'PUL', 55: 'PUL',\n",
    "                56: 'N/A', 57: 'PRV', 58: 'PRV'\n",
    "            }\n",
    "            \n",
    "            # If we have the original cell data, recreate the mapping\n",
    "            if 'df' in globals() and len(df) > 0:\n",
    "                # Create unit_id mapping\n",
    "                unit_brain_map = []\n",
    "                for idx, row in df.iterrows():\n",
    "                    brain_code = row['brainAreaOfCell']\n",
    "                    if isinstance(brain_code, np.ndarray):\n",
    "                        brain_code = int(brain_code[0, 0])\n",
    "                    brain_area = collapsed_area_map.get(brain_code, 'Unknown')\n",
    "                    unit_brain_map.append({'unit_id': idx, 'brainAreaOfCell': brain_area})\n",
    "                \n",
    "                brain_area_source = pd.DataFrame(unit_brain_map)\n",
    "                print(\"✅ Recreated brain area mapping from original data\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to recreate from MATLAB data: {e}\")\n",
    "    \n",
    "    return brain_area_source\n",
    "\n",
    "\n",
    "def fix_data_filtered_brain_area():\n",
    "    \"\"\"\n",
    "    Add brain area information back to data_filtered\n",
    "    \"\"\"\n",
    "    global data_filtered  # We'll modify the global variable\n",
    "    \n",
    "    print(\"\\n🔧 FIXING data_filtered...\")\n",
    "    \n",
    "    # Get brain area mapping\n",
    "    brain_mapping = create_brain_area_mapping_from_source()\n",
    "    \n",
    "    if brain_mapping is None:\n",
    "        print(\"❌ Could not find brain area information anywhere!\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"✅ Found brain area mapping for {len(brain_mapping)} units\")\n",
    "    print(f\"✅ Brain areas: {brain_mapping['brainAreaOfCell'].unique()}\")\n",
    "    \n",
    "    # Add brain area to data_filtered\n",
    "    data_filtered_fixed = data_filtered.merge(\n",
    "        brain_mapping, \n",
    "        on='unit_id', \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Check success\n",
    "    missing_areas = data_filtered_fixed['brainAreaOfCell'].isna().sum()\n",
    "    if missing_areas > 0:\n",
    "        print(f\"⚠️  Warning: {missing_areas} rows missing brain area info\")\n",
    "    else:\n",
    "        print(\"✅ Successfully added brain area info to all rows!\")\n",
    "    \n",
    "    print(f\"✅ Updated data_filtered shape: {data_filtered_fixed.shape}\")\n",
    "    print(f\"✅ Brain areas in fixed data: {data_filtered_fixed['brainAreaOfCell'].unique()}\")\n",
    "    \n",
    "    return data_filtered_fixed\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# SOLUTION 2: QUICK MANUAL RECONSTRUCTION\n",
    "# ===================================================================\n",
    "\n",
    "def manual_brain_area_reconstruction():\n",
    "    \"\"\"\n",
    "    Manually reconstruct brain area mapping if automated methods fail\n",
    "    \"\"\"\n",
    "    print(\"\\n🛠️  MANUAL RECONSTRUCTION:\")\n",
    "    print(\"If automated methods fail, we can manually recreate the mapping...\")\n",
    "    \n",
    "    collapsed_area_map = {\n",
    "        1: 'H', 2: 'H',           # Hippocampus\n",
    "        3: 'A', 4: 'A',           # Amygdala  \n",
    "        5: 'AC', 6: 'AC',         # Anterior Cingulate\n",
    "        7: 'SMA', 8: 'SMA',       # Supplementary Motor Area\n",
    "        9: 'PT', 10: 'PT',        # Parahippocampal cortex\n",
    "        11: 'OFC', 12: 'OFC',     # Orbitofrontal Cortex\n",
    "        50: 'FFA', 51: 'EC',      # Other areas\n",
    "        52: 'CM', 53: 'CM',\n",
    "        54: 'PUL', 55: 'PUL',\n",
    "        56: 'N/A', 57: 'PRV', 58: 'PRV'\n",
    "    }\n",
    "    \n",
    "    print(\"Brain area mapping codes:\")\n",
    "    for code, area in collapsed_area_map.items():\n",
    "        print(f\"  {code}: {area}\")\n",
    "    \n",
    "    return collapsed_area_map\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# RUN THE FIX\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🚀 RUNNING THE FIX...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Step 1: Diagnose what data we have\n",
    "brain_mapping = create_brain_area_mapping_from_source()\n",
    "\n",
    "if brain_mapping is not None:\n",
    "    # Step 2: Fix data_filtered\n",
    "    data_filtered_fixed = fix_data_filtered_brain_area()\n",
    "    \n",
    "    if data_filtered_fixed is not None:\n",
    "        # Update the global variable\n",
    "        data_filtered = data_filtered_fixed\n",
    "        print(\"\\n✅ SUCCESS! data_filtered now has brain area information\")\n",
    "        \n",
    "        # Step 3: Now fix the results\n",
    "        print(\"\\n🔧 Now fixing full_results...\")\n",
    "        try:\n",
    "            # Create the unit-brain mapping\n",
    "            unit_brain_mapping = data_filtered[['unit_id', 'brainAreaOfCell']].drop_duplicates()\n",
    "            \n",
    "            # Merge into full_results\n",
    "            full_results_fixed = full_results.merge(\n",
    "                unit_brain_mapping, \n",
    "                on='unit_id', \n",
    "                how='left'\n",
    "            )\n",
    "            \n",
    "            # Update global variable\n",
    "            full_results = full_results_fixed\n",
    "            \n",
    "            print(\"✅ SUCCESS! full_results now has brain area information\")\n",
    "            print(f\"✅ Brain areas in results: {full_results['brainAreaOfCell'].unique()}\")\n",
    "            \n",
    "            # Final verification\n",
    "            print(f\"\\n📊 FINAL STATUS:\")\n",
    "            print(f\"✅ data_filtered shape: {data_filtered.shape}\")\n",
    "            print(f\"✅ data_filtered has brainAreaOfCell: {'brainAreaOfCell' in data_filtered.columns}\")\n",
    "            print(f\"✅ full_results shape: {full_results.shape}\")\n",
    "            print(f\"✅ full_results has brainAreaOfCell: {'brainAreaOfCell' in full_results.columns}\")\n",
    "            \n",
    "            # Count units per area\n",
    "            if 'brainAreaOfCell' in full_results.columns:\n",
    "                area_counts = full_results.groupby(['unit_id', 'brainAreaOfCell']).size().reset_index()[['unit_id', 'brainAreaOfCell']].groupby('brainAreaOfCell').size()\n",
    "                print(f\"\\n📊 Units per brain area:\")\n",
    "                print(area_counts.to_string())\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error fixing full_results: {e}\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Could not create brain area mapping. Need to trace back further...\")\n",
    "    manual_brain_area_reconstruction()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎯 NEXT STEPS:\")\n",
    "print(\"1. Verify: print(data_filtered['brainAreaOfCell'].unique())\")\n",
    "print(\"2. Verify: print(full_results['brainAreaOfCell'].unique())\")  \n",
    "print(\"3. Run visualizations: create_comprehensive_report(population_timecourse, full_results)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed23d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all visualizations at once\n",
    "results = create_comprehensive_report(population_timecourse, full_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
