{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "311b5f76",
   "metadata": {},
   "source": [
    "# Neural Basic Selectivity Analysis\n",
    "\n",
    "Analysis of neuronal selectivity patterns during working memory binding tasks.\n",
    "\n",
    "## Analysis Overview\n",
    "\n",
    "1. **Data Loading & Initial Processing** - MATLAB data loading, brain area mapping, and unit quality control\n",
    "2. **Neural Activity & Trial Processing** - Trial extraction, firing rate computation across task epochs, and data integration\n",
    "3. **Feature Selectivity Analysis** - Statistical analysis identifying neurons selective for stimulus features and temporal position\n",
    "4. **Results & Visualization** - Selectivity summaries, population distributions, and single-unit response visualization\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Data Loading & Initial Processing\n",
    "\n",
    "### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12b221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af4a7f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install numpy pandas matplotlib seaborn scipy mat73\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from scipy.stats import ttest_1samp\n",
    "import glob\n",
    "import os\n",
    "# import mat73"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c48947",
   "metadata": {},
   "source": [
    "### Loading MATLAB Data Files\n",
    "\n",
    "Loading of MATLAB data files containing neural recordings and trial information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9c79ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all WMB_P*_v7.mat files in the current directory\n",
    "mat_files = glob.glob('./WMB_P*_v7.mat')\n",
    "print(f\"Found {len(mat_files)} .mat files: {[os.path.basename(f) for f in mat_files]}\")\n",
    "\n",
    "# Initialize lists to store data from each file\n",
    "cell_mats = []\n",
    "total_mats = []\n",
    "\n",
    "# Load each file and append its data\n",
    "for mat_file in mat_files:\n",
    "    print(f\"\\nLoading {mat_file}...\")\n",
    "    mat_data = loadmat(mat_file)\n",
    "    cell_mats.append(mat_data['cellStatsAll'])\n",
    "    total_mats.append(mat_data['totStats'])\n",
    "\n",
    "# Print shapes of loaded data for debugging\n",
    "print(\"\\nShapes of loaded data:\")\n",
    "for i, (cell, total) in enumerate(zip(cell_mats, total_mats)):\n",
    "    print(f\"File {i}: cell_mat shape: {cell.shape}, total_mat shape: {total.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6773f69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the data\n",
    "# For cell_mat, we need to handle the different dimensions\n",
    "# First, convert each cell_mat to a list of records\n",
    "all_cell_records = []\n",
    "for cell_mat in cell_mats:\n",
    "    # Convert to list of records\n",
    "    cell_list = cell_mat[0]  # now shape is (n,)\n",
    "    records = []\n",
    "    for cell in cell_list:\n",
    "        record = {key: cell[key] for key in cell.dtype.names}\n",
    "        records.append(record)\n",
    "    all_cell_records.extend(records)\n",
    "\n",
    "# Convert combined records to DataFrame\n",
    "df = pd.DataFrame(all_cell_records)\n",
    "\n",
    "# For total_mat, we can concatenate directly since they have the same structure\n",
    "total_mat = np.concatenate(total_mats, axis=0)\n",
    "\n",
    "print(f\"\\nCombined data shape - total_mat: {total_mat.shape}\")\n",
    "print(f\"\\nCombined data shape - df: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f401d2",
   "metadata": {},
   "source": [
    "### Translates numeric area codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d83a648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_area_codes(area_column):\n",
    "    \n",
    "    mapping = {\n",
    "        1: 'RH', 2: 'LH', 3: 'RA', 4: 'LA', 5: 'RAC', 6: 'LAC',\n",
    "        7: 'RSMA', 8: 'LSMA', 9: 'RPT', 10: 'LPT', 11: 'ROFC', 12: 'LOFC',\n",
    "        50: 'RFFA', 51: 'REC', 52: 'RCM', 53: 'LCM', 54: 'RPUL', 55: 'LPUL',\n",
    "        56: 'N/A', 57: 'RPRV', 58: 'LPRV'\n",
    "    }\n",
    "    \n",
    "    labels = []\n",
    "    for code in area_column:\n",
    "        label = mapping.get(code, 'Unknown')\n",
    "        labels.append(label)\n",
    "    \n",
    "    return dict(Counter(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6192ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neuron number in each area\n",
    "area_codes = total_mat[:, 3]\n",
    "\n",
    "counts = count_area_codes(area_codes)\n",
    "print(\"Area counts (no prefix):\")\n",
    "for area, count in counts.items():\n",
    "    print(f\"{area}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2029f482",
   "metadata": {},
   "source": [
    "### Data Preprocessing & Filtering\n",
    "\n",
    "First, we'll format the cell data and collapse brain areas into broader regions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d032add5",
   "metadata": {},
   "outputs": [],
   "source": [
    "collapsed_area_map = {\n",
    "    1: 'H', 2: 'H',\n",
    "    3: 'A', 4: 'A',\n",
    "    5: 'AC', 6: 'AC',\n",
    "    7: 'SMA', 8: 'SMA',\n",
    "    9: 'PT', 10: 'PT',\n",
    "    11: 'OFC', 12: 'OFC',\n",
    "    50: 'FFA', 51: 'EC',\n",
    "    52: 'CM', 53: 'CM',\n",
    "    54: 'PUL', 55: 'PUL',\n",
    "    56: 'N/A', 57: 'PRV', 58: 'PRV'\n",
    "}\n",
    "# Convert brain area codes in the DataFrame\n",
    "df['brainAreaOfCell'] = df['brainAreaOfCell'].apply(\n",
    "    lambda x: collapsed_area_map.get(int(x[0, 0]), 'Unknown') if isinstance(x, np.ndarray) else collapsed_area_map.get(x, 'Unknown')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3099b7eb",
   "metadata": {},
   "source": [
    "### Quality Control & Unit Selection\n",
    "\n",
    "Filtering criteria for reliable unit selection:\n",
    "- Minimum firing rate threshold\n",
    "- Signal quality metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "375ea159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out units with low firing rate\n",
    "fr = df['timestamps'].apply(lambda x: len(x) / (x[-1] - x[0]) * 1e6)\n",
    "df_sample_new = df[fr > 0.1].reset_index(drop=True)\n",
    "\n",
    "# unit id\n",
    "df_sample_new = df_sample_new.reset_index(drop=True)\n",
    "df_sample_new[\"unit_id\"] = df_sample_new.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c7fde2",
   "metadata": {},
   "source": [
    "## 2. Neural Activity & Trial Processing\n",
    "\n",
    "### Trial Data Extraction\n",
    "\n",
    "Extraction and alignment of trial-specific information with neural data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d90cfa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_trial_info(trials_struct, unit_id):\n",
    "    # Build a DataFrame from the trials structure.\n",
    "    df_trial = pd.DataFrame({field: trials_struct[field].squeeze() \n",
    "                             for field in trials_struct.dtype.names})\n",
    "    # Add the unit_id so that you can later separate trials by unit/session.\n",
    "    df_trial[\"unit_id\"] = unit_id\n",
    "    df_trial[\"trial_nr\"] = df_trial[\"trial\"].apply(lambda x: np.squeeze(x).item() if isinstance(x, (list, np.ndarray)) else x) - 1 # Adjust for 0-indexing\n",
    "    return df_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd9fe9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_info_list = []\n",
    "for idx, row in df_sample_new.iterrows():\n",
    "    # Use the unit identifier from this row\n",
    "    unit_id = row[\"unit_id\"]  \n",
    "    # Extract the trial DataFrame, including the unit identifier.\n",
    "    trial_info_list.append(extract_trial_info(row[\"Trials\"], unit_id, ))\n",
    "\n",
    "# Concatenate the list of trial info DataFrames into one.\n",
    "trial_info = pd.concat(trial_info_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae00e19",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Firing Rate Computation Functions\n",
    "\n",
    "The following functions extract event timestamps and compute firing rates for different task epochs. This is the core neural data processing step that converts spike timestamps into firing rates aligned to task events.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562f0c7a",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Event Timestamp Extraction & Firing Rate Computation\n",
    "\n",
    "Firing rate computation across task epochs:\n",
    "- Baseline period (pre-stimulus)\n",
    "- First stimulus encoding\n",
    "- First delay period\n",
    "- Second stimulus encoding\n",
    "- Second delay period\n",
    "- Response period\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d18301",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Data Integration & Column Selection\n",
    "\n",
    "Merge neural firing rate data with trial information and select relevant columns for analysis. This step combines the computed firing rates with behavioral trial data to enable selectivity analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fadef35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_event_timestamps(df_sample_new, start_idx_col='idxEnc1', end_idx_col='idxDel1', is_window=False, window_size=0.5):\n",
    "    \"\"\"\n",
    "    Extract event timestamps for computing firing rates across task epochs.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_sample_new : DataFrame\n",
    "        Neural data with event indices and timestamps\n",
    "    start_idx_col : str\n",
    "        Column name containing start event indices\n",
    "    end_idx_col : str  \n",
    "        Column name containing end event indices\n",
    "    is_window : bool\n",
    "        If True, extract fixed window around start event; if False, extract epoch between start and end\n",
    "    window_size : float\n",
    "        Window size in seconds (only used if is_window=True)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    epoch_ts : dict\n",
    "        Dictionary mapping unit_id to array of [start_time, end_time] pairs for each trial\n",
    "    \"\"\"\n",
    "    epoch_ts = {}\n",
    "    \n",
    "    for i, row in df_sample_new.iterrows():\n",
    "        unit_id = row['unit_id']\n",
    "        events = row['events'].squeeze()  # Event timestamps array [trial_idx, event_type, timestamp]\n",
    "        \n",
    "        if is_window:\n",
    "            # Extract fixed window around specific event (e.g., ±0.5s around response)\n",
    "            idxs = row[start_idx_col].squeeze() - 1  # Convert to 0-based indexing\n",
    "            extracted = events[idxs]\n",
    "            center_times = extracted[:, 0]  # Get timestamp column\n",
    "            window_start = center_times - window_size * 1e6  # Convert to microseconds\n",
    "            window_end = center_times + window_size * 1e6\n",
    "            combined = np.column_stack((window_start, window_end))\n",
    "        else:\n",
    "            # Extract epoch between two task events (e.g., encoding to delay)\n",
    "            idxs_start = row[start_idx_col].squeeze() - 1  # Start event indices (0-based)\n",
    "            idxs_end = row[end_idx_col].squeeze() - 1      # End event indices (0-based)\n",
    "            \n",
    "            # Handle mismatched trial counts between start and end events\n",
    "            min_length = min(len(idxs_start), len(idxs_end))\n",
    "            idxs_start = idxs_start[:min_length]\n",
    "            idxs_end = idxs_end[:min_length]\n",
    "            \n",
    "            # Extract timestamps for start and end events\n",
    "            extracted_start = events[idxs_start]  # [n_trials, 3] \n",
    "            extracted_end = events[idxs_end]      # [n_trials, 3]\n",
    "\n",
    "            # Combine start and end timestamps for each trial\n",
    "            combined = np.column_stack((extracted_start[:, 0], extracted_end[:, 0]))\n",
    "        \n",
    "        epoch_ts[unit_id] = combined\n",
    "        \n",
    "    return epoch_ts\n",
    "\n",
    "\n",
    "def compute_firing_rates(df_sample_new, start_idx_col='idxEnc1', end_idx_col='idxDel1', \n",
    "                         fr_prefix='fr', is_window=False, window_size=0.5):\n",
    "    \"\"\"\n",
    "    Compute firing rates for a specific task epoch.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_sample_new : DataFrame\n",
    "        Neural data containing spike timestamps and event indices\n",
    "    start_idx_col : str\n",
    "        Column with start event indices\n",
    "    end_idx_col : str\n",
    "        Column with end event indices  \n",
    "    fr_prefix : str\n",
    "        Prefix for the firing rate column name\n",
    "    is_window : bool\n",
    "        Whether to use fixed window (True) or epoch between events (False)\n",
    "    window_size : float\n",
    "        Window size in seconds (only for is_window=True)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    df_sample_new : DataFrame\n",
    "        Input dataframe with added firing rate columns\n",
    "    \"\"\"\n",
    "    # Extract epoch timestamps for all units\n",
    "    epoch_ts = extract_event_timestamps(df_sample_new, start_idx_col, end_idx_col, is_window, window_size)\n",
    "    \n",
    "    # Compute baseline firing rate (1 second before first stimulus) - only on first call\n",
    "    if 'fr_baseline' not in df_sample_new.columns:\n",
    "        print(\"Computing baseline firing rates...\")\n",
    "        \n",
    "        # Get first stimulus onset timestamps\n",
    "        enc_ts = extract_event_timestamps(df_sample_new, 'idxEnc1', 'idxEnc1')\n",
    "        \n",
    "        # Create baseline windows: 1 second before stimulus onset\n",
    "        baseline_ts = {}\n",
    "        for unit_id, timestamps in enc_ts.items():\n",
    "            baseline_start = timestamps[:, 0] - 1e6  # 1 second before (microseconds)\n",
    "            baseline_end = timestamps[:, 0]          # End at stimulus onset\n",
    "            baseline_ts[unit_id] = np.column_stack((baseline_start, baseline_end))\n",
    "            \n",
    "        # Compute baseline firing rates for each unit and trial\n",
    "        df_sample_new['fr_baseline'] = df_sample_new.apply(\n",
    "            lambda row: [\n",
    "                # Count spikes in baseline window and convert to Hz\n",
    "                np.sum((np.ravel(row[\"timestamps\"]) >= baseline_on) & \n",
    "                       (np.ravel(row[\"timestamps\"]) < baseline_off)) / ((baseline_off - baseline_on) / 1e6)\n",
    "                for baseline_on, baseline_off in baseline_ts[row[\"unit_id\"]]\n",
    "            ],\n",
    "            axis=1\n",
    "        )\n",
    "    \n",
    "    # Compute firing rates for the specified epoch\n",
    "    epoch_col = f\"{fr_prefix}_epoch\"\n",
    "    print(f\"Computing firing rates for {epoch_col}...\")\n",
    "    \n",
    "    df_sample_new[epoch_col] = df_sample_new.apply(\n",
    "        lambda row: [\n",
    "            # Count spikes in epoch window and convert to Hz\n",
    "            np.sum((np.ravel(row[\"timestamps\"]) >= epoch_on) & \n",
    "                   (np.ravel(row[\"timestamps\"]) < epoch_off)) / ((epoch_off - epoch_on) / 1e6)\n",
    "            for epoch_on, epoch_off in epoch_ts[row[\"unit_id\"]]\n",
    "        ],\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Add trial numbers if not already present\n",
    "    if \"trial_nr\" not in df_sample_new.columns:\n",
    "        df_sample_new[\"trial_nr\"] = df_sample_new[epoch_col].apply(lambda x: np.arange(len(x)))\n",
    "    \n",
    "    return df_sample_new\n",
    "\n",
    "\n",
    "print(\"Computing firing rates for all task epochs...\")\n",
    "\n",
    "# Compute firing rates for each task epoch\n",
    "# Each call adds a new firing rate column to the dataframe\n",
    "\n",
    "# 1. First stimulus encoding period (from stimulus onset to delay start)\n",
    "df_sample_new = compute_firing_rates(df_sample_new, 'idxEnc1', 'idxDel1', fr_prefix='fr')\n",
    "\n",
    "# 2. First delay period (between first stimulus and second stimulus)  \n",
    "df_sample_new = compute_firing_rates(df_sample_new, 'idxDel1', 'idxEnc2', fr_prefix='fr_del1')\n",
    "\n",
    "# 3. Second stimulus encoding period (from second stimulus onset to second delay)\n",
    "df_sample_new = compute_firing_rates(df_sample_new, 'idxEnc2', 'idxDel2', fr_prefix='fr_enc2')\n",
    "\n",
    "# 4. Second delay period (between second stimulus and probe)\n",
    "df_sample_new = compute_firing_rates(df_sample_new, 'idxDel2', 'idxProbeOn', fr_prefix='fr_del2')\n",
    "\n",
    "# 5. Response period (±0.5 second window around button press)\n",
    "df_sample_new = compute_firing_rates(df_sample_new, 'idxResp', None, fr_prefix='fr_resp', \n",
    "                                   is_window=True, window_size=0.5)\n",
    "\n",
    "print(\"Expanding dataframe from unit-based to trial-based format...\")\n",
    "\n",
    "# Transform from unit-level to trial-level format\n",
    "# Each unit currently has lists of firing rates (one per trial)\n",
    "# After exploding, each row will represent one unit-trial combination\n",
    "columns_to_explode = ['fr_baseline', 'fr_epoch', 'fr_del1_epoch', 'fr_enc2_epoch', \n",
    "                      'fr_del2_epoch', 'fr_resp_epoch', \"trial_nr\"]\n",
    "df_sample_new = df_sample_new.explode(columns_to_explode)\n",
    "\n",
    "print(f\"Final neural data shape: {df_sample_new.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cc4f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Merging neural data with trial information...\")\n",
    "\n",
    "# Reset indices to ensure proper alignment\n",
    "df_sample_new = df_sample_new.reset_index(drop=True)\n",
    "trial_info = trial_info.reset_index(drop=True)\n",
    "\n",
    "# Merge neural firing rates with behavioral trial data\n",
    "# This combines unit-trial firing rates with stimulus information for each trial\n",
    "data = pd.merge(\n",
    "    df_sample_new,\n",
    "    trial_info,\n",
    "    on=[\"unit_id\", \"trial_nr\"],  # Join on unit and trial identifiers\n",
    "    how=\"left\",                  # Keep all neural data, add matching trial info\n",
    ").infer_objects()\n",
    "\n",
    "print(f\"Combined data shape: {data.shape}\")\n",
    "\n",
    "# Select columns needed for selectivity analysis\n",
    "# Include: neural identifiers, firing rates, stimulus properties, task timing\n",
    "cols_to_keep = [\n",
    "    # Neural data identifiers and firing rates\n",
    "    \"unit_id\", \"timestamps\", \"brainAreaOfCell\", \n",
    "    \"fr_epoch\", \"fr_baseline\", \"fr_del1_epoch\", \"fr_enc2_epoch\", \"fr_del2_epoch\", \"fr_resp_epoch\", \n",
    "    \"trial_nr\",\n",
    "    \n",
    "    # Stimulus properties for selectivity analysis\n",
    "    \"first_cat\", \"second_cat\", \"first_num\", \"second_num\",\n",
    "    \"first_pic\", \"second_pic\", \"probe_cat\", \"probe_pic\",\n",
    "    \n",
    "    # Task variables\n",
    "    \"probe_validity\", \"probe_num\", \"correct_answer\",\n",
    "    \"rt\", \"acc\", \"key\", \"cat_comparison\", \n",
    "    \n",
    "    # Event timing and metadata\n",
    "    \"events\", \"nTrials\", \"Trials\", \n",
    "    \"idxEnc1\", \"idxEnc2\", \"idxDel1\", \"idxDel2\", \"idxProbeOn\", \"idxResp\", \n",
    "    \"nrProcessed\", \"periods_Enc1\", \"periods_Enc2\", \"periods_Del1\", \"periods_Del2\", \n",
    "    \"periods_Probe\", \"periods_Resp\", \"prestimEnc\", \"prestimMaint\", \"prestimProbe\",\n",
    "    \"prestimButtonPress\", \"poststimEnc\", \"poststimMaint\", \"poststimProbe\", \n",
    "    \"poststimButtonPress\", \"sessionIdx\", \"channel\", \"cellNr\", \"sessionID\", \"origClusterID\"\n",
    "]\n",
    "\n",
    "# Create filtered dataset with only necessary columns\n",
    "data_filtered = data[cols_to_keep].copy()\n",
    "\n",
    "print(\"Converting stimulus variables to simple string format for statistical analysis...\")\n",
    "\n",
    "# Convert stimulus variables to simple string format\n",
    "# MATLAB arrays need to be converted to hashable Python types for grouping operations\n",
    "data_filtered[\"first_cat_simple\"] = data_filtered[\"first_cat\"].apply(\n",
    "    lambda x: str(np.squeeze(x)) if isinstance(x, (list, np.ndarray)) else str(x)\n",
    ")\n",
    "data_filtered[\"second_cat_simple\"] = data_filtered[\"second_cat\"].apply(\n",
    "    lambda x: str(np.squeeze(x)) if isinstance(x, (list, np.ndarray)) else str(x)\n",
    ")\n",
    "data_filtered[\"first_num_simple\"] = data_filtered[\"first_num\"].apply(\n",
    "    lambda x: str(np.squeeze(x)) if isinstance(x, (list, np.ndarray)) else str(x)\n",
    ")\n",
    "data_filtered[\"second_num_simple\"] = data_filtered[\"second_num\"].apply(\n",
    "    lambda x: str(np.squeeze(x)) if isinstance(x, (list, np.ndarray)) else str(x)\n",
    ")\n",
    "\n",
    "print(f\"Final processed dataset shape: {data_filtered.shape}\")\n",
    "print(f\"Stimulus categories: {data_filtered['first_cat_simple'].unique()}\")\n",
    "print(f\"Stimulus numbers: {data_filtered['first_num_simple'].unique()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74db0e2",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Core Selectivity Analysis Functions\n",
    "\n",
    "The following functions implement the statistical analysis to identify selective neurons. The analysis uses ANOVA to test whether firing rates significantly differ across stimulus conditions (categories, numerosities, or temporal positions).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7432878",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. Feature Selectivity Analysis\n",
    "\n",
    "### Statistical Analysis of Neuronal Selectivity\n",
    "\n",
    "Identification of neurons selective for task-relevant features:\n",
    "1. **Stimulus Categories** - Visual category preferences during first and second stimulus presentation\n",
    "2. **Numerosities** - Number-selective responses during encoding periods  \n",
    "3. **Temporal Position** - Differential responses based on stimulus presentation order (first vs second)\n",
    "\n",
    "Using ANOVA to test for significant selectivity across stimulus conditions and brain regions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058d04a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def identify_selective_neurons(data_filtered):\n",
    "    selectivity_stats = []\n",
    "\n",
    "    for unit_id, unit_df in tqdm(data_filtered.groupby(\"unit_id\")):\n",
    "        unit_df = unit_df.copy()\n",
    "        \n",
    "        # Analyze first stimulus\n",
    "        unit_df[\"fr_first\"] = unit_df[\"fr_epoch\"]\n",
    "        unit_df[\"fr_second\"] = unit_df[\"fr_enc2_epoch\"]\n",
    "\n",
    "        # Skip if no variance\n",
    "        if unit_df[\"fr_first\"].std() == 0 or unit_df[\"fr_second\"].std() == 0:\n",
    "            continue\n",
    "\n",
    "        # First stimulus model\n",
    "        # Using ordinary least squares regression to model firing rate as a function of \n",
    "        # categorical variables for stimulus category and numerosity\n",
    "        model_first = smf.ols(\n",
    "            \"fr_first ~ C(first_cat_simple) + C(first_num_simple)\", \n",
    "            data=unit_df\n",
    "        ).fit()\n",
    "        anova_first = sm.stats.anova_lm(model_first, typ=2)\n",
    "\n",
    "        # Second stimulus model\n",
    "        model_second = smf.ols(\n",
    "            \"fr_second ~ C(second_cat_simple) + C(second_num_simple)\", \n",
    "            data=unit_df\n",
    "        ).fit()\n",
    "        anova_second = sm.stats.anova_lm(model_second, typ=2)\n",
    "\n",
    "        # Position analysis\n",
    "        position_df = pd.DataFrame({\n",
    "            'fr': np.concatenate([unit_df['fr_first'], unit_df['fr_second']]),\n",
    "            'category': np.concatenate([unit_df['first_cat_simple'], unit_df['second_cat_simple']]),\n",
    "            'number': np.concatenate([unit_df['first_num_simple'], unit_df['second_num_simple']]),\n",
    "            'position': ['first'] * len(unit_df) + ['second'] * len(unit_df)\n",
    "        })\n",
    "\n",
    "        model_position = smf.ols(\n",
    "            \"fr ~ C(category) + C(number) + C(position)\", \n",
    "            data=position_df\n",
    "        ).fit()\n",
    "        anova_position = sm.stats.anova_lm(model_position, typ=2)\n",
    "\n",
    "        stats = {\n",
    "            'unit_id': unit_id,\n",
    "            'area': unit_df['brainAreaOfCell'].iloc[0],\n",
    "            'first_cat_pvalue': anova_first.loc['C(first_cat_simple)', 'PR(>F)'],\n",
    "            'first_num_pvalue': anova_first.loc['C(first_num_simple)', 'PR(>F)'],\n",
    "            'second_cat_pvalue': anova_second.loc['C(second_cat_simple)', 'PR(>F)'],\n",
    "            'second_num_pvalue': anova_second.loc['C(second_num_simple)', 'PR(>F)'],\n",
    "            'position_pvalue': anova_position.loc['C(position)', 'PR(>F)'],\n",
    "            'is_first_cat_selective': anova_first.loc['C(first_cat_simple)', 'PR(>F)'] < 0.05,\n",
    "            'is_first_num_selective': anova_first.loc['C(first_num_simple)', 'PR(>F)'] < 0.05,\n",
    "            'is_second_cat_selective': anova_second.loc['C(second_cat_simple)', 'PR(>F)'] < 0.05,\n",
    "            'is_second_num_selective': anova_second.loc['C(second_num_simple)', 'PR(>F)'] < 0.05,\n",
    "            'is_position_selective': anova_position.loc['C(position)', 'PR(>F)'] < 0.05,\n",
    "            'r2_first': model_first.rsquared,\n",
    "            'r2_second': model_second.rsquared,\n",
    "            'r2_position': model_position.rsquared,\n",
    "            'is_any_selective': (\n",
    "                (anova_first.loc['C(first_cat_simple)', 'PR(>F)'] < 0.05) or\n",
    "                (anova_first.loc['C(first_num_simple)', 'PR(>F)'] < 0.05) or\n",
    "                (anova_second.loc['C(second_cat_simple)', 'PR(>F)'] < 0.05) or\n",
    "                (anova_second.loc['C(second_num_simple)', 'PR(>F)'] < 0.05) or\n",
    "                (anova_position.loc['C(position)', 'PR(>F)'] < 0.05)\n",
    "            )\n",
    "        }\n",
    "        selectivity_stats.append(stats)\n",
    "\n",
    "    results_df = pd.DataFrame(selectivity_stats)\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f290c1",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Running the Analysis\n",
    "\n",
    "Execute the selectivity analysis on all units and display summary statistics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dabb35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_selectivity(selectivity_results):\n",
    "    total_units = len(selectivity_results)\n",
    "\n",
    "    summary = {\n",
    "        \"total_units\": total_units,\n",
    "        \"first_category_selective\": {\n",
    "            \"count\": int(selectivity_results[\"is_first_cat_selective\"].sum()),\n",
    "            \"percentage\": round(100 * selectivity_results[\"is_first_cat_selective\"].mean(), 1)\n",
    "        },\n",
    "        \"first_number_selective\": {\n",
    "            \"count\": int(selectivity_results[\"is_first_num_selective\"].sum()),\n",
    "            \"percentage\": round(100 * selectivity_results[\"is_first_num_selective\"].mean(), 1)\n",
    "        },\n",
    "        \"second_category_selective\": {\n",
    "            \"count\": int(selectivity_results[\"is_second_cat_selective\"].sum()),\n",
    "            \"percentage\": round(100 * selectivity_results[\"is_second_cat_selective\"].mean(), 1)\n",
    "        },\n",
    "        \"second_number_selective\": {\n",
    "            \"count\": int(selectivity_results[\"is_second_num_selective\"].sum()),\n",
    "            \"percentage\": round(100 * selectivity_results[\"is_second_num_selective\"].mean(), 1)\n",
    "        },\n",
    "        \"position_selective\": {\n",
    "            \"count\": int(selectivity_results[\"is_position_selective\"].sum()),\n",
    "            \"percentage\": round(100 * selectivity_results[\"is_position_selective\"].mean(), 1)\n",
    "        },\n",
    "        \"any_selective\": {\n",
    "            \"count\": int(selectivity_results[\"is_any_selective\"].sum()),\n",
    "            \"percentage\": round(100 * selectivity_results[\"is_any_selective\"].mean(), 1)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    area_summary = selectivity_results.groupby(\"area\").agg({\n",
    "        \"is_first_cat_selective\": \"sum\",\n",
    "        \"is_first_num_selective\": \"sum\",\n",
    "        \"is_second_cat_selective\": \"sum\",\n",
    "        \"is_second_num_selective\": \"sum\",\n",
    "        \"is_position_selective\": \"sum\",\n",
    "        \"is_any_selective\": \"sum\",\n",
    "        \"unit_id\": \"count\"\n",
    "    }).rename(columns={\"unit_id\": \"total_units\"})\n",
    "\n",
    "    # Calculate percentage columns explicitly\n",
    "    for col in [\"is_first_cat_selective\", \"is_first_num_selective\",\n",
    "                \"is_second_cat_selective\", \"is_second_num_selective\",\n",
    "                \"is_position_selective\", \"is_any_selective\"]:\n",
    "        area_summary[f\"{col}_pct\"] = round(100 * area_summary[col] / area_summary[\"total_units\"], 1)\n",
    "\n",
    "    summary[\"by_area\"] = area_summary.reset_index().to_dict(orient='records')\n",
    "\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e311ac49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting selectivity analysis for all units...\")\n",
    "\n",
    "# Run the main selectivity analysis\n",
    "# This function analyzes each unit individually using ANOVA to test for selectivity\n",
    "selectivity_results = identify_selective_neurons(data_filtered)\n",
    "\n",
    "print(f\"Analysis complete! Analyzed {len(selectivity_results)} units.\")\n",
    "\n",
    "# Generate summary statistics across all units and brain regions\n",
    "selectivity_summary = summarize_selectivity(selectivity_results)\n",
    "\n",
    "# Display overall population results\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"POPULATION SELECTIVITY RESULTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"Total units analyzed: {selectivity_summary['total_units']}\")\n",
    "print(f\"First category selective: {selectivity_summary['first_category_selective']['count']} ({selectivity_summary['first_category_selective']['percentage']}%)\")\n",
    "print(f\"First number selective: {selectivity_summary['first_number_selective']['count']} ({selectivity_summary['first_number_selective']['percentage']}%)\")\n",
    "print(f\"Second category selective: {selectivity_summary['second_category_selective']['count']} ({selectivity_summary['second_category_selective']['percentage']}%)\")\n",
    "print(f\"Second number selective: {selectivity_summary['second_number_selective']['count']} ({selectivity_summary['second_number_selective']['percentage']}%)\")\n",
    "print(f\"Position selective: {selectivity_summary['position_selective']['count']} ({selectivity_summary['position_selective']['percentage']}%)\")\n",
    "print(f\"Any selective: {selectivity_summary['any_selective']['count']} ({selectivity_summary['any_selective']['percentage']}%)\")\n",
    "\n",
    "# Display regional breakdown\n",
    "print(f\"\\nRegional breakdown available for {len(selectivity_summary['by_area'])} brain areas.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2d6af8",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Population Distribution Plots\n",
    "\n",
    "Generate summary plots showing the distribution of different selectivity types across the population and brain regions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84f5bb5",
   "metadata": {},
   "source": [
    "## 4. Results & Visualization\n",
    "\n",
    "### Population Selectivity Summary\n",
    "\n",
    "Overview of selectivity patterns across the neural population and brain regions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e26bdd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating population selectivity distribution plots...\")\n",
    "\n",
    "# Plot 1: Overall distribution of selectivity types across the population\n",
    "selectivity_types = [\n",
    "    'First Category', \n",
    "    'First Number', \n",
    "    'Second Category', \n",
    "    'Second Number', \n",
    "    'Position', \n",
    "    'Any'\n",
    "]\n",
    "\n",
    "# Extract counts for each selectivity type\n",
    "selectivity_counts = [\n",
    "    selectivity_summary['first_category_selective']['count'],\n",
    "    selectivity_summary['first_number_selective']['count'],\n",
    "    selectivity_summary['second_category_selective']['count'],\n",
    "    selectivity_summary['second_number_selective']['count'],\n",
    "    selectivity_summary['position_selective']['count'],\n",
    "    selectivity_summary['any_selective']['count']\n",
    "]\n",
    "\n",
    "# Create bar plot showing absolute numbers of selective neurons\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(selectivity_types, selectivity_counts, color='skyblue')\n",
    "plt.title(\"Distribution of Feature Selectivity Across Population\")\n",
    "plt.ylabel(\"Number of Neurons\")\n",
    "plt.xlabel(\"Selectivity Type\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"feature_selectivity_overall.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"Creating regional selectivity comparison plot...\")\n",
    "\n",
    "# Plot 2: Regional distribution of selective neurons\n",
    "area_df = pd.DataFrame(selectivity_summary['by_area'])\n",
    "\n",
    "# Sort brain areas by percentage of selective neurons (descending)\n",
    "area_df_sorted = area_df.sort_values('is_any_selective_pct', ascending=False)\n",
    "\n",
    "# Create bar plot comparing selectivity percentages across brain regions\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(area_df_sorted['area'], area_df_sorted['is_any_selective_pct'], color='teal')\n",
    "plt.title(\"Percentage of Selective Neurons by Brain Region\")\n",
    "plt.ylabel(\"Percent of Selective Neurons (%)\")\n",
    "plt.xlabel(\"Brain Region\")\n",
    "\n",
    "# Add horizontal line showing overall population average\n",
    "plt.axhline(\n",
    "    y=selectivity_summary['any_selective']['percentage'], \n",
    "    color='red', \n",
    "    linestyle='--', \n",
    "    label=f\"Overall Average ({selectivity_summary['any_selective']['percentage']}%)\"\n",
    ")\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"selective_neurons_by_region.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"Population plots saved as:\")\n",
    "print(\"  - feature_selectivity_overall.png\")\n",
    "print(\"  - selective_neurons_by_region.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917e08db",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Regional Selectivity Analysis\n",
    "\n",
    "Detailed breakdown of selectivity patterns by brain region:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8582bbd1",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Single-Unit Response Visualization\n",
    "\n",
    "Spike raster plots and PSTHs for neurons exhibiting significant selectivity.\n",
    "\n",
    "**Task Event Markers:**\n",
    "- `pic1` (1): First picture presentation\n",
    "- `delay1` (2): First delay period  \n",
    "- `pic2` (3): Second picture presentation\n",
    "- `delay2` (4): Second delay period\n",
    "- `probeOnset` (5): Probe stimulus onset\n",
    "- `response` (6): Response period\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fae7361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the processed dataset\n",
    "print(f\"Dataset shape: {data_filtered.shape}\")\n",
    "print(f\"Available columns: {len(data_filtered.columns)}\")\n",
    "print(f\"Key variables for analysis:\")\n",
    "print(f\"  - Brain areas: {sorted(data_filtered['brainAreaOfCell'].unique())}\")\n",
    "print(f\"  - Stimulus categories: {sorted(data_filtered['first_cat_simple'].unique())}\")\n",
    "print(f\"  - Stimulus numbers: {sorted(data_filtered['first_num_simple'].unique())}\")\n",
    "data_filtered.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b194cd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def identify_selective_neurons(data_filtered):\n",
    "    selectivity_stats = []\n",
    "\n",
    "    for unit_id, unit_df in tqdm(data_filtered.groupby(\"unit_id\")):\n",
    "        unit_df = unit_df.copy()\n",
    "        \n",
    "        # Analyze first stimulus\n",
    "        unit_df[\"fr_first\"] = unit_df[\"fr_epoch\"]\n",
    "        unit_df[\"fr_second\"] = unit_df[\"fr_enc2_epoch\"]\n",
    "\n",
    "        # Skip if no variance\n",
    "        if unit_df[\"fr_first\"].std() == 0 or unit_df[\"fr_second\"].std() == 0:\n",
    "            continue\n",
    "\n",
    "        # First stimulus model\n",
    "        # Using ordinary least squares regression to model firing rate as a function of \n",
    "        # categorical variables for stimulus category and numerosity\n",
    "        model_first = smf.ols(\n",
    "            \"fr_first ~ C(first_cat_simple) + C(first_num_simple)\", \n",
    "            data=unit_df\n",
    "        ).fit()\n",
    "        anova_first = sm.stats.anova_lm(model_first, typ=2)\n",
    "\n",
    "        # Second stimulus model\n",
    "        model_second = smf.ols(\n",
    "            \"fr_second ~ C(second_cat_simple) + C(second_num_simple)\", \n",
    "            data=unit_df\n",
    "        ).fit()\n",
    "        anova_second = sm.stats.anova_lm(model_second, typ=2)\n",
    "\n",
    "        # Position analysis\n",
    "        position_df = pd.DataFrame({\n",
    "            'fr': np.concatenate([unit_df['fr_first'], unit_df['fr_second']]),\n",
    "            'category': np.concatenate([unit_df['first_cat_simple'], unit_df['second_cat_simple']]),\n",
    "            'number': np.concatenate([unit_df['first_num_simple'], unit_df['second_num_simple']]),\n",
    "            'position': ['first'] * len(unit_df) + ['second'] * len(unit_df)\n",
    "        })\n",
    "\n",
    "        model_position = smf.ols(\n",
    "            \"fr ~ C(category) + C(number) + C(position)\", \n",
    "            data=position_df\n",
    "        ).fit()\n",
    "        anova_position = sm.stats.anova_lm(model_position, typ=2)\n",
    "\n",
    "        stats = {\n",
    "            'unit_id': unit_id,\n",
    "            'area': unit_df['brainAreaOfCell'].iloc[0],\n",
    "            'first_cat_pvalue': anova_first.loc['C(first_cat_simple)', 'PR(>F)'],\n",
    "            'first_num_pvalue': anova_first.loc['C(first_num_simple)', 'PR(>F)'],\n",
    "            'second_cat_pvalue': anova_second.loc['C(second_cat_simple)', 'PR(>F)'],\n",
    "            'second_num_pvalue': anova_second.loc['C(second_num_simple)', 'PR(>F)'],\n",
    "            'position_pvalue': anova_position.loc['C(position)', 'PR(>F)'],\n",
    "            'is_first_cat_selective': anova_first.loc['C(first_cat_simple)', 'PR(>F)'] < 0.05,\n",
    "            'is_first_num_selective': anova_first.loc['C(first_num_simple)', 'PR(>F)'] < 0.05,\n",
    "            'is_second_cat_selective': anova_second.loc['C(second_cat_simple)', 'PR(>F)'] < 0.05,\n",
    "            'is_second_num_selective': anova_second.loc['C(second_num_simple)', 'PR(>F)'] < 0.05,\n",
    "            'is_position_selective': anova_position.loc['C(position)', 'PR(>F)'] < 0.05,\n",
    "            'r2_first': model_first.rsquared,\n",
    "            'r2_second': model_second.rsquared,\n",
    "            'r2_position': model_position.rsquared,\n",
    "            'is_any_selective': (\n",
    "                (anova_first.loc['C(first_cat_simple)', 'PR(>F)'] < 0.05) or\n",
    "                (anova_first.loc['C(first_num_simple)', 'PR(>F)'] < 0.05) or\n",
    "                (anova_second.loc['C(second_cat_simple)', 'PR(>F)'] < 0.05) or\n",
    "                (anova_second.loc['C(second_num_simple)', 'PR(>F)'] < 0.05) or\n",
    "                (anova_position.loc['C(position)', 'PR(>F)'] < 0.05)\n",
    "            )\n",
    "        }\n",
    "        selectivity_stats.append(stats)\n",
    "\n",
    "    results_df = pd.DataFrame(selectivity_stats)\n",
    "    return results_df\n",
    "\n",
    "\n",
    "def visualize_selective_responses(data_filtered, unit_id):\n",
    "    \"\"\"\n",
    "    Create visualizations showing the selectivity pattern of a neuron.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_filtered : pandas.DataFrame\n",
    "        DataFrame containing neural data\n",
    "    unit_id : int or str\n",
    "        ID of the unit to visualize\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    matplotlib.figure.Figure\n",
    "        Figure containing the visualization\n",
    "    \"\"\"\n",
    "    unit_data = data_filtered[data_filtered[\"unit_id\"] == unit_id].copy()\n",
    "    unit_data[\"fr_normalized\"] = unit_data[\"fr_epoch\"] - unit_data[\"fr_baseline\"]\n",
    "    \n",
    "    # Create figure with subplots for different selectivity types\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    \n",
    "    # First stimulus features\n",
    "    # 1. First Category selectivity\n",
    "    cat_responses = unit_data.groupby(\"first_cat_simple\")[\"fr_normalized\"].mean().reset_index()\n",
    "    cat_sem = unit_data.groupby(\"first_cat_simple\")[\"fr_normalized\"].sem().reset_index()\n",
    "    \n",
    "    sns.barplot(x=\"first_cat_simple\", y=\"fr_normalized\", data=cat_responses, ax=axes[0,0])\n",
    "    axes[0,0].errorbar(\n",
    "        x=range(len(cat_responses)), \n",
    "        y=cat_responses[\"fr_normalized\"],\n",
    "        yerr=cat_sem[\"fr_normalized\"],\n",
    "        fmt='none', color='black', capsize=5\n",
    "    )\n",
    "    axes[0,0].set_title(f\"First Stimulus Category Selectivity\")\n",
    "    axes[0,0].set_xlabel(\"Category\")\n",
    "    axes[0,0].set_ylabel(\"Normalized Firing Rate (Hz)\")\n",
    "    \n",
    "    # 2. First Number selectivity\n",
    "    num_responses = unit_data.groupby(\"first_num_simple\")[\"fr_normalized\"].mean().reset_index()\n",
    "    num_sem = unit_data.groupby(\"first_num_simple\")[\"fr_normalized\"].sem().reset_index()\n",
    "    \n",
    "    sns.barplot(x=\"first_num_simple\", y=\"fr_normalized\", data=num_responses, ax=axes[0,1])\n",
    "    axes[0,1].errorbar(\n",
    "        x=range(len(num_responses)), \n",
    "        y=num_responses[\"fr_normalized\"],\n",
    "        yerr=num_sem[\"fr_normalized\"],\n",
    "        fmt='none', color='black', capsize=5\n",
    "    )\n",
    "    axes[0,1].set_title(f\"First Stimulus Number Selectivity\")\n",
    "    axes[0,1].set_xlabel(\"Number\")\n",
    "    axes[0,1].set_ylabel(\"Normalized Firing Rate (Hz)\")\n",
    "    \n",
    "    # Second stimulus features\n",
    "    # 3. Second Category selectivity\n",
    "    cat_responses = unit_data.groupby(\"second_cat_simple\")[\"fr_normalized\"].mean().reset_index()\n",
    "    cat_sem = unit_data.groupby(\"second_cat_simple\")[\"fr_normalized\"].sem().reset_index()\n",
    "    \n",
    "    sns.barplot(x=\"second_cat_simple\", y=\"fr_normalized\", data=cat_responses, ax=axes[1,0])\n",
    "    axes[1,0].errorbar(\n",
    "        x=range(len(cat_responses)), \n",
    "        y=cat_responses[\"fr_normalized\"],\n",
    "        yerr=cat_sem[\"fr_normalized\"],\n",
    "        fmt='none', color='black', capsize=5\n",
    "    )\n",
    "    axes[1,0].set_title(f\"Second Stimulus Category Selectivity\")\n",
    "    axes[1,0].set_xlabel(\"Category\")\n",
    "    axes[1,0].set_ylabel(\"Normalized Firing Rate (Hz)\")\n",
    "    \n",
    "    # 4. Second Number selectivity\n",
    "    num_responses = unit_data.groupby(\"second_num_simple\")[\"fr_normalized\"].mean().reset_index()\n",
    "    num_sem = unit_data.groupby(\"second_num_simple\")[\"fr_normalized\"].sem().reset_index()\n",
    "    \n",
    "    sns.barplot(x=\"second_num_simple\", y=\"fr_normalized\", data=num_responses, ax=axes[1,1])\n",
    "    axes[1,1].errorbar(\n",
    "        x=range(len(num_responses)), \n",
    "        y=num_responses[\"fr_normalized\"],\n",
    "        yerr=num_sem[\"fr_normalized\"],\n",
    "        fmt='none', color='black', capsize=5\n",
    "    )\n",
    "    axes[1,1].set_title(f\"Second Stimulus Number Selectivity\")\n",
    "    axes[1,1].set_xlabel(\"Number\")\n",
    "    axes[1,1].set_ylabel(\"Normalized Firing Rate (Hz)\")\n",
    "    \n",
    "    # 5. Position selectivity (first vs second)\n",
    "    # Create a position comparison dataframe\n",
    "    position_df = pd.DataFrame({\n",
    "        'Position': ['First', 'Second'],\n",
    "        'FR': [unit_data['fr_normalized'].mean(), unit_data['fr_normalized'].mean()],\n",
    "        'SEM': [unit_data['fr_normalized'].sem(), unit_data['fr_normalized'].sem()]\n",
    "    })\n",
    "    \n",
    "    sns.barplot(x=\"Position\", y=\"FR\", data=position_df, ax=axes[0,2])\n",
    "    axes[0,2].errorbar(\n",
    "        x=range(len(position_df)), \n",
    "        y=position_df[\"FR\"],\n",
    "        yerr=position_df[\"SEM\"],\n",
    "        fmt='none', color='black', capsize=5\n",
    "    )\n",
    "    axes[0,2].set_title(f\"Position Selectivity (First vs Second)\")\n",
    "    axes[0,2].set_xlabel(\"Stimulus Position\")\n",
    "    axes[0,2].set_ylabel(\"Normalized Firing Rate (Hz)\")\n",
    "    \n",
    "    # Add unit information\n",
    "    axes[1,2].axis('off')\n",
    "    axes[1,2].text(0.5, 0.5, \n",
    "                  f\"Unit ID: {unit_id}\\nBrain Area: {unit_data['brainAreaOfCell'].iloc[0]}\", \n",
    "                  horizontalalignment='center',\n",
    "                  verticalalignment='center',\n",
    "                  fontsize=12)\n",
    "    \n",
    "    plt.suptitle(f\"Selectivity Profile for Unit {unit_id}\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def summarize_selectivity(selectivity_results):\n",
    "    total_units = len(selectivity_results)\n",
    "\n",
    "    summary = {\n",
    "        \"total_units\": total_units,\n",
    "        \"first_category_selective\": {\n",
    "            \"count\": int(selectivity_results[\"is_first_cat_selective\"].sum()),\n",
    "            \"percentage\": round(100 * selectivity_results[\"is_first_cat_selective\"].mean(), 1)\n",
    "        },\n",
    "        \"first_number_selective\": {\n",
    "            \"count\": int(selectivity_results[\"is_first_num_selective\"].sum()),\n",
    "            \"percentage\": round(100 * selectivity_results[\"is_first_num_selective\"].mean(), 1)\n",
    "        },\n",
    "        \"second_category_selective\": {\n",
    "            \"count\": int(selectivity_results[\"is_second_cat_selective\"].sum()),\n",
    "            \"percentage\": round(100 * selectivity_results[\"is_second_cat_selective\"].mean(), 1)\n",
    "        },\n",
    "        \"second_number_selective\": {\n",
    "            \"count\": int(selectivity_results[\"is_second_num_selective\"].sum()),\n",
    "            \"percentage\": round(100 * selectivity_results[\"is_second_num_selective\"].mean(), 1)\n",
    "        },\n",
    "        \"position_selective\": {\n",
    "            \"count\": int(selectivity_results[\"is_position_selective\"].sum()),\n",
    "            \"percentage\": round(100 * selectivity_results[\"is_position_selective\"].mean(), 1)\n",
    "        },\n",
    "        \"any_selective\": {\n",
    "            \"count\": int(selectivity_results[\"is_any_selective\"].sum()),\n",
    "            \"percentage\": round(100 * selectivity_results[\"is_any_selective\"].mean(), 1)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    area_summary = selectivity_results.groupby(\"area\").agg({\n",
    "        \"is_first_cat_selective\": \"sum\",\n",
    "        \"is_first_num_selective\": \"sum\",\n",
    "        \"is_second_cat_selective\": \"sum\",\n",
    "        \"is_second_num_selective\": \"sum\",\n",
    "        \"is_position_selective\": \"sum\",\n",
    "        \"is_any_selective\": \"sum\",\n",
    "        \"unit_id\": \"count\"\n",
    "    }).rename(columns={\"unit_id\": \"total_units\"})\n",
    "\n",
    "    # Calculate percentage columns explicitly\n",
    "    for col in [\"is_first_cat_selective\", \"is_first_num_selective\",\n",
    "                \"is_second_cat_selective\", \"is_second_num_selective\",\n",
    "                \"is_position_selective\", \"is_any_selective\"]:\n",
    "        area_summary[f\"{col}_pct\"] = round(100 * area_summary[col] / area_summary[\"total_units\"], 1)\n",
    "\n",
    "    summary[\"by_area\"] = area_summary.reset_index().to_dict(orient='records')\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a78af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the selectivity analysis\n",
    "selectivity_results = identify_selective_neurons(data_filtered)\n",
    "\n",
    "# Summarize results\n",
    "selectivity_summary = summarize_selectivity(selectivity_results)\n",
    "\n",
    "# Print overall results\n",
    "print(\"\\nOVERALL RESULTS:\")\n",
    "print(f\"Total units analyzed: {selectivity_summary['total_units']}\")\n",
    "print(f\"First category selective: {selectivity_summary['first_category_selective']['count']} ({selectivity_summary['first_category_selective']['percentage']}%)\")\n",
    "print(f\"First number selective: {selectivity_summary['first_number_selective']['count']} ({selectivity_summary['first_number_selective']['percentage']}%)\")\n",
    "print(f\"Second category selective: {selectivity_summary['second_category_selective']['count']} ({selectivity_summary['second_category_selective']['percentage']}%)\")\n",
    "print(f\"Second number selective: {selectivity_summary['second_number_selective']['count']} ({selectivity_summary['second_number_selective']['percentage']}%)\")\n",
    "print(f\"Position selective: {selectivity_summary['position_selective']['count']} ({selectivity_summary['position_selective']['percentage']}%)\")\n",
    "print(f\"Any selective: {selectivity_summary['any_selective']['count']} ({selectivity_summary['any_selective']['percentage']}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8f5ddb",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Regional Analysis Function\n",
    "\n",
    "Area-by-area breakdown of selectivity patterns. This function analyzes each brain region separately to identify regional differences in selectivity patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858834b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Overall distribution of selectivity types\n",
    "selectivity_types = [\n",
    "    'First Category', \n",
    "    'First Number', \n",
    "    'Second Category', \n",
    "    'Second Number', \n",
    "    'Position', \n",
    "    'Any'\n",
    "]\n",
    "\n",
    "selectivity_counts = [\n",
    "    selectivity_summary['first_category_selective']['count'],\n",
    "    selectivity_summary['first_number_selective']['count'],\n",
    "    selectivity_summary['second_category_selective']['count'],\n",
    "    selectivity_summary['second_number_selective']['count'],\n",
    "    selectivity_summary['position_selective']['count'],\n",
    "    selectivity_summary['any_selective']['count']\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(selectivity_types, selectivity_counts, color='skyblue')\n",
    "plt.title(\"Distribution of Feature Selectivity\")\n",
    "plt.ylabel(\"Number of Neurons\")\n",
    "plt.xlabel(\"Selectivity Type\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"feature_selectivity_overall.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Regional distribution of selective neurons\n",
    "area_df = pd.DataFrame(selectivity_summary['by_area'])\n",
    "\n",
    "# Sort by percent selective\n",
    "area_df_sorted = area_df.sort_values('is_any_selective_pct', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(area_df_sorted['area'], area_df_sorted['is_any_selective_pct'], color='teal')\n",
    "plt.title(\"Percentage of Selective Neurons by Brain Region\")\n",
    "plt.ylabel(\"Percent of Selective Neurons (%)\")\n",
    "plt.xlabel(\"Brain Region\")\n",
    "plt.axhline(\n",
    "    y=selectivity_summary['any_selective']['percentage'], \n",
    "    color='red', \n",
    "    linestyle='--', \n",
    "    label='Overall Average'\n",
    ")\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"selective_neurons_by_region.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7881aaf8",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Execute Regional Analysis\n",
    "\n",
    "Run the area-by-area analysis and generate summary plots for each brain region.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1583ccdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_selective_neurons_by_area(data_filtered):\n",
    "    area_results = {}\n",
    "\n",
    "    for area, area_df in data_filtered.groupby(\"brainAreaOfCell\"):\n",
    "        selectivity_stats = []\n",
    "\n",
    "        for unit_id, unit_df in tqdm(area_df.groupby(\"unit_id\"), desc=f\"Analyzing {area}\"):\n",
    "            unit_df = unit_df.copy()\n",
    "\n",
    "            # Analyze first and second stimulus separately\n",
    "            unit_df[\"fr_first\"] = unit_df[\"fr_epoch\"]\n",
    "            unit_df[\"fr_second\"] = unit_df[\"fr_enc2_epoch\"]\n",
    "\n",
    "            # Skip if no variance\n",
    "            if unit_df[\"fr_first\"].std() == 0 or unit_df[\"fr_second\"].std() == 0:\n",
    "                continue\n",
    "\n",
    "            # First stimulus model\n",
    "            model_first = smf.ols(\n",
    "                \"fr_first ~ C(first_cat_simple) + C(first_num_simple)\", \n",
    "                data=unit_df\n",
    "            ).fit()\n",
    "            anova_first = sm.stats.anova_lm(model_first, typ=2)\n",
    "\n",
    "            # Second stimulus model\n",
    "            model_second = smf.ols(\n",
    "                \"fr_second ~ C(second_cat_simple) + C(second_num_simple)\", \n",
    "                data=unit_df\n",
    "            ).fit()\n",
    "            anova_second = sm.stats.anova_lm(model_second, typ=2)\n",
    "\n",
    "            # Position analysis\n",
    "            position_df = pd.DataFrame({\n",
    "                'fr': np.concatenate([unit_df['fr_first'], unit_df['fr_second']]),\n",
    "                'category': np.concatenate([unit_df['first_cat_simple'], unit_df['second_cat_simple']]),\n",
    "                'number': np.concatenate([unit_df['first_num_simple'], unit_df['second_num_simple']]),\n",
    "                'position': ['first'] * len(unit_df) + ['second'] * len(unit_df)\n",
    "            })\n",
    "\n",
    "            model_position = smf.ols(\n",
    "                \"fr ~ C(category) + C(number) + C(position)\", \n",
    "                data=position_df\n",
    "            ).fit()\n",
    "            anova_position = sm.stats.anova_lm(model_position, typ=2)\n",
    "\n",
    "            stats = {\n",
    "                'unit_id': unit_id,\n",
    "                'area': area,\n",
    "                'first_cat_pvalue': anova_first.loc['C(first_cat_simple)', 'PR(>F)'],\n",
    "                'first_num_pvalue': anova_first.loc['C(first_num_simple)', 'PR(>F)'],\n",
    "                'second_cat_pvalue': anova_second.loc['C(second_cat_simple)', 'PR(>F)'],\n",
    "                'second_num_pvalue': anova_second.loc['C(second_num_simple)', 'PR(>F)'],\n",
    "                'position_pvalue': anova_position.loc['C(position)', 'PR(>F)'],\n",
    "                'is_first_cat_selective': anova_first.loc['C(first_cat_simple)', 'PR(>F)'] < 0.05,\n",
    "                'is_first_num_selective': anova_first.loc['C(first_num_simple)', 'PR(>F)'] < 0.05,\n",
    "                'is_second_cat_selective': anova_second.loc['C(second_cat_simple)', 'PR(>F)'] < 0.05,\n",
    "                'is_second_num_selective': anova_second.loc['C(second_num_simple)', 'PR(>F)'] < 0.05,\n",
    "                'is_position_selective': anova_position.loc['C(position)', 'PR(>F)'] < 0.05,\n",
    "                'r2_first': model_first.rsquared,\n",
    "                'r2_second': model_second.rsquared,\n",
    "                'r2_position': model_position.rsquared,\n",
    "                'is_any_selective': (\n",
    "                    (anova_first.loc['C(first_cat_simple)', 'PR(>F)'] < 0.05) or\n",
    "                    (anova_first.loc['C(first_num_simple)', 'PR(>F)'] < 0.05) or\n",
    "                    (anova_second.loc['C(second_cat_simple)', 'PR(>F)'] < 0.05) or\n",
    "                    (anova_second.loc['C(second_num_simple)', 'PR(>F)'] < 0.05) or\n",
    "                    (anova_position.loc['C(position)', 'PR(>F)'] < 0.05)\n",
    "                )\n",
    "            }\n",
    "            selectivity_stats.append(stats)\n",
    "\n",
    "        area_results[area] = pd.DataFrame(selectivity_stats)\n",
    "\n",
    "    areas = list(area_results.keys())\n",
    "    return area_results, areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b295b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Analyzing selectivity patterns by brain region...\")\n",
    "\n",
    "# Run the regional analysis\n",
    "area_selectivity_results, brain_areas = identify_selective_neurons_by_area(data_filtered)\n",
    "\n",
    "print(f\"Analysis complete for {len(brain_areas)} brain regions: {brain_areas}\")\n",
    "\n",
    "# Generate detailed reports and plots for each brain area\n",
    "for area, df in area_selectivity_results.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"BRAIN AREA: {area}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Total units: {len(df)}\")\n",
    "    \n",
    "    # Calculate selectivity counts for this area\n",
    "    summary = {\n",
    "        \"first_cat\": df['is_first_cat_selective'].sum(),\n",
    "        \"first_num\": df['is_first_num_selective'].sum(), \n",
    "        \"second_cat\": df['is_second_cat_selective'].sum(),\n",
    "        \"second_num\": df['is_second_num_selective'].sum(),\n",
    "        \"position\": df['is_position_selective'].sum(),\n",
    "        \"any_selective\": df['is_any_selective'].sum()\n",
    "    }\n",
    "    \n",
    "    # Display selectivity statistics for this area\n",
    "    print(\"Selectivity counts and percentages:\")\n",
    "    for k, v in summary.items():\n",
    "        pct = 100 * v / len(df) if len(df) > 0 else 0\n",
    "        print(f\"  {k}: {v} ({pct:.1f}%)\")\n",
    "\n",
    "    # Create area-specific selectivity plot\n",
    "    n_units = len(df)\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    counts = {\n",
    "        \"First Cat\": df[\"is_first_cat_selective\"].sum(),\n",
    "        \"First Num\": df[\"is_first_num_selective\"].sum(),\n",
    "        \"Second Cat\": df[\"is_second_cat_selective\"].sum(),\n",
    "        \"Second Num\": df[\"is_second_num_selective\"].sum(),\n",
    "        \"Position\": df[\"is_position_selective\"].sum(),\n",
    "        \"Any\": df[\"is_any_selective\"].sum()\n",
    "    }\n",
    "    \n",
    "    # Generate bar plot for this brain area\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    bars = plt.bar(counts.keys(), counts.values(), color='skyblue')\n",
    "    plt.title(f\"Selectivity Distribution in {area} (n={n_units})\")\n",
    "    plt.ylabel(\"Number of Selective Neurons\")\n",
    "    plt.xlabel(\"Selectivity Type\")\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, count in zip(bars, counts.values()):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "                str(count), ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nRegional analysis complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95ed0df",
   "metadata": {},
   "source": [
    "### Plot Responsive Neurons - All Trials\n",
    "\n",
    "Visualize spike rasters and PSTHs for neurons showing significant selectivity:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c3894b",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Required Libraries for Spike Visualization\n",
    "\n",
    "Import specialized neural analysis libraries for generating spike raster plots and PSTHs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd8d423",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Generate Individual Unit Raster Plots\n",
    "\n",
    "Create detailed spike raster plots and PSTHs for all selective neurons. Each plot shows trial-by-trial spiking activity aligned to task events, grouped by stimulus conditions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03e13926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install neural analysis package if needed:\n",
    "# pip install git+https://github.com/ioqfwfq/rlab_neural_analysis.git@jz\n",
    "\n",
    "# Import functions for spike visualization and analysis\n",
    "from neural_analysis.visualize import plot_spikes_with_PSTH\n",
    "from neural_analysis.spikes import get_spikes\n",
    "\n",
    "print(\"Neural analysis libraries loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa6668a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Preparing spike alignment data for visualization...\")\n",
    "\n",
    "# Extract stimulus onset timestamps for aligning spike plots\n",
    "epoch_ts = extract_event_timestamps(\n",
    "    df_sample_new=data_filtered,\n",
    "    start_idx_col='idxEnc1',     # Align to first stimulus onset\n",
    "    is_window=True,\n",
    "    window_size=0                # Use exact timestamps, not windows\n",
    ")\n",
    "\n",
    "print(\"Setting up output directory for plots...\")\n",
    "\n",
    "# Create output directory for individual unit plots\n",
    "os.makedirs(\"plot_out\", exist_ok=True)\n",
    "\n",
    "print(\"Generating raster plots for selective neurons...\")\n",
    "\n",
    "# Define the types of selectivity to visualize\n",
    "selectivity_types = [\n",
    "    (\"first_cat\", \"first_cat_simple\"),      # First stimulus category selectivity\n",
    "    (\"first_num\", \"first_num_simple\"),      # First stimulus number selectivity  \n",
    "    (\"second_cat\", \"second_cat_simple\"),    # Second stimulus category selectivity\n",
    "    (\"second_num\", \"second_num_simple\"),    # Second stimulus number selectivity\n",
    "    (\"position\", \"position\")                # Temporal position selectivity\n",
    "]\n",
    "\n",
    "plot_count = 0\n",
    "total_selective = sum(selectivity_results[f\"is_{st[0]}_selective\"].sum() for st in selectivity_types)\n",
    "print(f\"Will generate plots for {total_selective} selective units across {len(selectivity_types)} selectivity types...\")\n",
    "\n",
    "# Generate plots for each selectivity type\n",
    "for select_type, cond in selectivity_types:\n",
    "    pval_col = f\"{select_type}_pvalue\"\n",
    "    flag_col = f\"is_{select_type}_selective\"\n",
    "\n",
    "    # Get units that are selective for this particular feature\n",
    "    selective_units = selectivity_results[selectivity_results[flag_col]]\n",
    "    print(f\"\\nProcessing {len(selective_units)} units selective for {select_type}...\")\n",
    "    \n",
    "    for _, row in selective_units.iterrows():\n",
    "        unit_id = row[\"unit_id\"]\n",
    "        pval = row[pval_col]\n",
    "        area = row[\"area\"]\n",
    "\n",
    "        # Extract trial data for this unit\n",
    "        df_unit = data_filtered[data_filtered[\"unit_id\"] == unit_id].reset_index(drop=True)\n",
    "        if df_unit.empty or unit_id not in epoch_ts:\n",
    "            print(f\"  Skipping unit {unit_id} (no data available)\")\n",
    "            continue\n",
    "\n",
    "        # Set up trial grouping based on selectivity type\n",
    "        if cond == \"position\":\n",
    "            # For position selectivity: compare first vs second stimulus presentation\n",
    "            group_labels = [\"first\"] * len(df_unit) + [\"second\"] * len(df_unit)\n",
    "            df_unit_extended = pd.concat([df_unit.copy(), df_unit.copy()], ignore_index=True)\n",
    "            df_unit_extended[\"fr_combined\"] = pd.concat([\n",
    "                df_unit[\"fr_epoch\"], df_unit[\"fr_enc2_epoch\"]\n",
    "            ], ignore_index=True)\n",
    "            alignments = np.tile(np.asarray(epoch_ts[unit_id][:, 0], dtype=np.float64) / 1e6, 2)\n",
    "            df_for_stats = df_unit_extended\n",
    "        else:\n",
    "            # For stimulus feature selectivity: group by stimulus condition\n",
    "            group_labels = df_unit[cond].apply(\n",
    "                lambda x: np.squeeze(x).item() if isinstance(x, (list, np.ndarray)) else x\n",
    "            )\n",
    "            alignments = np.asarray(epoch_ts[unit_id][:, 0], dtype=np.float64) / 1e6\n",
    "            df_unit[\"fr_combined\"] = df_unit[\"fr_epoch\"]\n",
    "            df_for_stats = df_unit\n",
    "\n",
    "        # Extract spike timestamps (convert from microseconds to seconds)\n",
    "        spikes = np.asarray(df_unit[\"timestamps\"].iloc[0]).flatten().astype(np.float64) / 1e6\n",
    "        spikes = np.sort(spikes)\n",
    "        \n",
    "        # Select appropriate variable for statistical annotation\n",
    "        if select_type in [\"first_cat\", \"second_cat\"]:\n",
    "            # For category selectivity, show number as secondary grouping\n",
    "            stats_var = \"first_num_simple\" if select_type == \"first_cat\" else \"second_num_simple\"\n",
    "        elif select_type in [\"first_num\", \"second_num\"]:\n",
    "            # For number selectivity, show category as secondary grouping\n",
    "            stats_var = \"first_cat_simple\" if select_type == \"first_num\" else \"second_cat_simple\"\n",
    "        elif select_type == \"position\":\n",
    "            # For position selectivity, show probe picture as secondary grouping\n",
    "            stats_var = \"probe_pic\"\n",
    "        else:\n",
    "            # Default fallback\n",
    "            stats_var = \"first_cat_simple\"\n",
    "        \n",
    "        stats = df_for_stats[stats_var]\n",
    "\n",
    "        try:    \n",
    "            # Generate raster plot and PSTH\n",
    "            axes = plot_spikes_with_PSTH(\n",
    "                spikes,\n",
    "                alignments,\n",
    "                window=(-1, 8),          # 1s before to 8s after stimulus\n",
    "                group_labels=group_labels,\n",
    "                stats=stats,\n",
    "                plot_stats=False,        # Don't show statistical annotations\n",
    "                sig_test=True,           # Perform significance testing\n",
    "                cmap=\"Set1\",            # Color scheme\n",
    "            )\n",
    "\n",
    "            # Add baseline firing rate reference line\n",
    "            unit_baseline = np.mean(df_unit[\"fr_baseline\"])\n",
    "            xmin, xmax = axes[1].get_xlim()\n",
    "            axes[1].hlines(\n",
    "                y=unit_baseline,\n",
    "                xmin=xmin,\n",
    "                xmax=xmax,\n",
    "                colors=\"gray\",\n",
    "                linestyles=\"--\",\n",
    "                label=f\"baseline = {unit_baseline:.1f} Hz\"\n",
    "            )\n",
    "\n",
    "            # Add vertical lines marking task events\n",
    "            # Task timeline: Enc1(1s), Del1(2s), Enc2(3s), Del2, Probe(5.5s)\n",
    "            event_times = [1, 2, 3, 5.5]  # Times relative to first stimulus onset\n",
    "            for event_time in event_times:\n",
    "                for ax in axes:\n",
    "                    ax.axvline(x=event_time, color=\"black\", linestyle=\"--\", \n",
    "                              linewidth=0.5, alpha=0.7)\n",
    "\n",
    "            # Add informative title and labels\n",
    "            axes[0].set_title(\n",
    "                f\"{area} Unit {unit_id} — {select_type.replace('_', ' ').title()} Selective\\n\"\n",
    "                f\"p = {pval:.3g}\"\n",
    "            )\n",
    "            axes[1].set_xlabel(\"Time from First Stimulus Onset [s]\")\n",
    "            axes[1].legend()\n",
    "\n",
    "            # Save plot with descriptive filename\n",
    "            fname = f\"plot_out/{area}_{unit_id}_{select_type}_p{pval:.3f}.png\"\n",
    "            plt.savefig(fname, dpi=300, bbox_inches=\"tight\")\n",
    "            plt.close()\n",
    "            \n",
    "            plot_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error plotting unit {unit_id} ({select_type}): {e}\")\n",
    "\n",
    "print(f\"\\nRaster plot generation complete!\")\n",
    "print(f\"Generated {plot_count} plots saved in 'plot_out/' directory.\")\n",
    "print(f\"Plot naming convention: [BrainArea]_[UnitID]_[SelectivityType]_p[pvalue].png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
