{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WM binding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install numpy pandas matplotlib seaborn scipy mat73\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "# import mat73"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "## Loading MATLAB objects\n",
    "\n",
    "Reading .mat files can be achieved either through scipy.io.loadmat (well maintained, but only loads simple structs) or pymatreader.read_mat (supports all types of objects). What you will typically end up with is a dictionary of values or NumPy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the mat file\n",
    "cell_mat = loadmat('./WMB_P106_1_20250511.mat')['cellStatsAll']\n",
    "total_mat = loadmat('./WMB_P106_1_20250511.mat')['totStats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OR Load the CSV file\n",
    "# file_path = \"D:/neuro1/code/psychophysics/WM_binding_pilot/data/P106_session_1_WM_binding_202527_9381_behav_cleaned.csv\"\n",
    "# df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basic database stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_area_codes(area_column):\n",
    "    \"\"\"\n",
    "    Translates numeric area codes to text labels and counts their occurrences.\n",
    "\n",
    "    Args:\n",
    "        area_column (array-like): A list or array of area code numbers.\n",
    "        addMicroPrefix (bool): If True, prepend 'micro-' to each area label.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping area label to its count.\n",
    "    \"\"\"\n",
    "    mapping = {\n",
    "        1: 'RH', 2: 'LH', 3: 'RA', 4: 'LA', 5: 'RAC', 6: 'LAC',\n",
    "        7: 'RSMA', 8: 'LSMA', 9: 'RPT', 10: 'LPT', 11: 'ROFC', 12: 'LOFC',\n",
    "        50: 'RFFA', 51: 'REC', 52: 'RCM', 53: 'LCM', 54: 'RPUL', 55: 'LPUL',\n",
    "        56: 'N/A', 57: 'RPRV', 58: 'LPRV'\n",
    "    }\n",
    "    \n",
    "    labels = []\n",
    "    for code in area_column:\n",
    "        label = mapping.get(code, 'Unknown')\n",
    "        labels.append(label)\n",
    "    \n",
    "    return dict(Counter(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area counts (no prefix):\n",
      "LAC: 17\n",
      "LSMA: 10\n",
      "LA: 9\n",
      "LH: 13\n",
      "RAC: 4\n",
      "RSMA: 16\n",
      "RA: 16\n",
      "RH: 7\n",
      "LOFC: 17\n",
      "LPT: 5\n",
      "ROFC: 11\n"
     ]
    }
   ],
   "source": [
    "# neuron number in each area\n",
    "area_codes = total_mat[:, 3]\n",
    "\n",
    "counts = count_area_codes(area_codes)\n",
    "print(\"Area counts (no prefix):\")\n",
    "for area, count in counts.items():\n",
    "    print(f\"{area}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format cell data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brain area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "collapsed_area_map = {\n",
    "    1: 'H', 2: 'H',\n",
    "    3: 'A', 4: 'A',\n",
    "    5: 'AC', 6: 'AC',\n",
    "    7: 'SMA', 8: 'SMA',\n",
    "    9: 'PT', 10: 'PT',\n",
    "    11: 'OFC', 12: 'OFC',\n",
    "    50: 'FFA', 51: 'EC',\n",
    "    52: 'CM', 53: 'CM',\n",
    "    54: 'PUL', 55: 'PUL',\n",
    "    56: 'N/A', 57: 'PRV', 58: 'PRV'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the (1, 305) array to (305,)\n",
    "flat_array = cell_mat['brainAreaOfCell'].flatten()\n",
    "\n",
    "# Extract the scalar code from each array([[x]])\n",
    "flattened_codes = [int(item[0, 0]) for item in flat_array]\n",
    "\n",
    "# Apply the collapsed area map\n",
    "converted_labels = [collapsed_area_map.get(code, 'Unknown') for code in flattened_codes]\n",
    "\n",
    "# Overwrite the original field with the new labels\n",
    "cell_mat['brainAreaOfCell'][0] = converted_labels  # because it's 1 row\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell_mat['brainAreaOfCell']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten to a 1D array\n",
    "cell_list = cell_mat[0]  # now shape is (n,)\n",
    "\n",
    "# Extract each field into a dictionary\n",
    "records = []\n",
    "for cell in cell_list:\n",
    "    record = {key: cell[key] for key in cell.dtype.names}\n",
    "    records.append(record)\n",
    "\n",
    "# Convert list of dicts to DataFrame\n",
    "df = pd.DataFrame(records)\n",
    "# df.convert_dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sample = df.sample(10) # OPTIONAL: make sample for testing\n",
    "df_sample = df\n",
    "# df_sample.convert_dtypes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out units with low firing rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out units with low firing rate\n",
    "fr = df_sample['timestamps'].apply(lambda x: len(x) / (x[-1] - x[0]) * 1e6)\n",
    "df_sample_new = df_sample[fr > 0.1].reset_index(drop=True)\n",
    "\n",
    "# unit id\n",
    "df_sample_new = df_sample_new.reset_index(drop=True)\n",
    "df_sample_new[\"unit_id\"] = df_sample_new.index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract trial info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(299, 34)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "('trial', 'rt', 'acc', 'key', 't_pre_stim', 't_delay1', 't_delay2', 'first_cat', 'second_cat', 'first_num', 'second_num', 'first_pic', 'second_pic', 'probe_cat', 'probe_pic', 'probe_validity', 'probe_num', 'correct_answer', 'rt_sliding_mean', 'cat_comparison', 'error_type')\n"
     ]
    }
   ],
   "source": [
    "trial = df_sample_new[\"Trials\"].iloc[0]\n",
    "print(type(trial))\n",
    "print(trial.dtype.names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_trial_info(trials_struct, unit_id):\n",
    "    # Build a DataFrame from the trials structure.\n",
    "    # We use .squeeze() for each field – adjust if necessary.\n",
    "    df_trial = pd.DataFrame({field: trials_struct[field].squeeze() \n",
    "                             for field in trials_struct.dtype.names})\n",
    "    # Add the unit_id so that you can later separate trials by unit/session.\n",
    "    df_trial[\"unit_id\"] = unit_id\n",
    "    df_trial[\"trial_nr\"] = df_trial[\"trial\"].apply(lambda x: np.squeeze(x).item() if isinstance(x, (list, np.ndarray)) else x) - 1 # Adjust for 0-indexing\n",
    "    return df_trial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trial_info_list = []\n",
    "for idx, row in df_sample_new.iterrows():\n",
    "    # Use the unit identifier from this row\n",
    "    unit_id = row[\"unit_id\"]  \n",
    "    # Extract the trial DataFrame, including the unit identifier.\n",
    "    trial_info_list.append(extract_trial_info(row[\"Trials\"], unit_id, ))\n",
    "\n",
    "# Concatenate the list of trial info DataFrames into one.\n",
    "trial_info = pd.concat(trial_info_list, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_info.iloc[0:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single unit analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % ttl values\n",
    "# c.marker.expstart        = 89;\n",
    "# c.marker.expend          = 91;\n",
    "# c.marker.fixOnset        = 10;\n",
    "# c.marker.pic1            = 1;\n",
    "# c.marker.delay1          = 2;\n",
    "# c.marker.pic2            = 3;\n",
    "# c.marker.delay2          = 4;\n",
    "# c.marker.probeOnset      = 5;\n",
    "# c.marker.response        = 6;\n",
    "# c.marker.break           = 90;\n",
    "\n",
    "# what names are in the df\n",
    "df_sample_new.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event ts extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Event ts extraction\n",
    "result_temp = []\n",
    "for i, row in df_sample_new.iterrows():\n",
    "    events = row['events'].squeeze()       # Ensure it's 1D array\n",
    "    idxs1 = row['idxEnc1'].squeeze() - 1   # Ensure indices are 1D array; start with 0!!!\n",
    "    idxs2 = row['idxEnc1'].squeeze() - 1\n",
    "    # Index into events using the adjusted indices:\n",
    "    extracted1 = events[idxs1]   # shape (n_trials, 3)\n",
    "    extracted2 = events[idxs2]   # shape (n_trials, 3)\n",
    "\n",
    "    combined = np.column_stack((extracted1[:, 0], extracted2[:, 0]))\n",
    "    result_temp.append(combined)\n",
    "\n",
    "# Save as numpy array of arrays (object dtype)\n",
    "result_array_temp = np.array(result_temp, dtype=object)\n",
    "\n",
    "# Extract the first element from each 3-element array\n",
    "epoch_ts = result_array_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute baseline FR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = df_sample_new.iloc[0]\n",
    "# print(row.name)\n",
    "# print(np.array(row[\"periods_Enc1\"]).shape)     # Should be (n, 3)\n",
    "# print(np.array(row[\"periods_Enc1\"])[:, 1:3])   # Should be the start and end timestamps\n",
    "\n",
    "# # Count spikes during epochs defined in periods_Enc1\n",
    "# df_sample_new[\"fr_enc1\"] = df_sample_new.apply(\n",
    "#     lambda row: [\n",
    "#         np.searchsorted(np.ravel(row[\"timestamps\"]), end, side=\"right\") \n",
    "#         - np.searchsorted(np.ravel(row[\"timestamps\"]), start, side=\"left\")\n",
    "#         for start, end in row[\"periods_Enc1\"][:, 1:3]\n",
    "#     ],\n",
    "#     axis=1\n",
    "# )\n",
    "\n",
    "# Count spikes during epochs defined in using epoch_ts\n",
    "df_sample_new[\"fr_baseline\"] = df_sample_new.apply(\n",
    "    lambda row: [\n",
    "        np.searchsorted(np.ravel(row[\"timestamps\"]), epoch_off + 0 * 1e6, side=\"right\") \n",
    "        - np.searchsorted(np.ravel(row[\"timestamps\"]), epoch_on - 1 * 1e6, side=\"left\")\n",
    "        for epoch_on, epoch_off in np.array(epoch_ts[row.name])\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "# Generate trial numbers for each row\n",
    "df_sample_new[\"trial_nr\"] = df_sample_new[\"fr_baseline\"].apply(lambda x: np.arange(len(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epoch of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event ts extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Event ts extraction\n",
    "result_temp = []\n",
    "for i, row in df_sample_new.iterrows():\n",
    "    events = row['events'].squeeze()       # Ensure it's 1D array\n",
    "    idxs1 = row['idxEnc1'].squeeze() - 1   # Ensure indices are 1D array; start with 0!!!\n",
    "    idxs2 = row['idxEnc1'].squeeze() - 1\n",
    "    # Index into events using the adjusted indices:\n",
    "    extracted1 = events[idxs1]   # shape (n_trials, 3)\n",
    "    extracted2 = events[idxs2]   # shape (n_trials, 3)\n",
    "\n",
    "    combined = np.column_stack((extracted1[:, 0], extracted2[:, 0]))\n",
    "    result_temp.append(combined)\n",
    "\n",
    "# Save as numpy array of arrays (object dtype)\n",
    "result_array_temp = np.array(result_temp, dtype=object)\n",
    "\n",
    "# Extract the first element from each 3-element array\n",
    "epoch_ts = result_array_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute epoch FR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = df_sample_new.iloc[0]\n",
    "# print(row.name)\n",
    "# print(np.array(row[\"periods_Enc1\"]).shape)     # Should be (n, 3)\n",
    "# print(np.array(row[\"periods_Enc1\"])[:, 1:3])   # Should be the start and end timestamps\n",
    "\n",
    "# # Count spikes during epochs defined in periods_Enc1\n",
    "# df_sample_new[\"fr_enc1\"] = df_sample_new.apply(\n",
    "#     lambda row: [\n",
    "#         np.searchsorted(np.ravel(row[\"timestamps\"]), end, side=\"right\") \n",
    "#         - np.searchsorted(np.ravel(row[\"timestamps\"]), start, side=\"left\")\n",
    "#         for start, end in row[\"periods_Enc1\"][:, 1:3]\n",
    "#     ],\n",
    "#     axis=1\n",
    "# )\n",
    "\n",
    "# Count spikes during epochs defined in using epoch_ts\n",
    "df_sample_new[\"fr_epoch\"] = df_sample_new.apply(\n",
    "    lambda row: [\n",
    "        np.searchsorted(np.ravel(row[\"timestamps\"]), epoch_off + 1 * 1e6, side=\"right\") \n",
    "        - np.searchsorted(np.ravel(row[\"timestamps\"]), epoch_on - 0 * 1e6, side=\"left\")\n",
    "        for epoch_on, epoch_off in np.array(epoch_ts[row.name])\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "# # Generate trial numbers for each row\n",
    "# df_sample_new[\"trial_nr\"] = df_sample_new[\"fr_epoch\"].apply(lambda x: np.arange(len(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to rows\n",
    "df_sample_new = df_sample_new.explode([\"fr_epoch\", \"trial_nr\", \"fr_baseline\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete df_sample_new[\"trials\"]\n",
    "df_sample_new = df_sample_new.drop(columns=[\"Trials\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trial_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_new = df_sample_new.reset_index(drop=True)\n",
    "trial_info = trial_info.reset_index(drop=True)\n",
    "\n",
    "data = pd.merge(\n",
    "    df_sample_new,\n",
    "    trial_info,\n",
    "    on=[\"unit_id\", \"trial_nr\"],\n",
    "    how=\"left\",\n",
    ").infer_objects()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = [\n",
    "    \"unit_id\", \"timestamps\", \"brainAreaOfCell\", \"fr_epoch\",\"fr_baseline\", \"trial_nr\",\n",
    "    \"first_cat\", \"second_cat\", \"first_num\", \"second_num\",\n",
    "    \"first_pic\", \"second_pic\", \"probe_cat\", \"probe_pic\",\n",
    "    \"probe_validity\", \"probe_num\", \"correct_answer\",\n",
    "    \"rt\", \"acc\", \"key\"\n",
    "]\n",
    "\n",
    "data_filtered = data[cols_to_keep]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_filtered.timestamps[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to simpler, hashable values.\n",
    "data_filtered[\"first_cat_simple\"] = data_filtered[\"first_cat\"].apply(\n",
    "    lambda x: str(np.squeeze(x)) if isinstance(x, (list, np.ndarray)) else str(x)\n",
    ")\n",
    "data_filtered[\"second_cat_simple\"] = data_filtered[\"second_cat\"].apply(\n",
    "    lambda x: str(np.squeeze(x)) if isinstance(x, (list, np.ndarray)) else str(x)\n",
    ")\n",
    "data_filtered[\"first_num_simple\"] = data_filtered[\"first_num\"].apply(\n",
    "    lambda x: str(np.squeeze(x)) if isinstance(x, (list, np.ndarray)) else str(x)\n",
    ")\n",
    "data_filtered[\"second_num_simple\"] = data_filtered[\"second_num\"].apply(\n",
    "    lambda x: str(np.squeeze(x)) if isinstance(x, (list, np.ndarray)) else str(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose to use Poisson GLM or OLS/ANOVA (set use_poisson = True or False)\n",
    "use_poisson = False\n",
    "records = []\n",
    "\n",
    "# Group the data by unit (neuron)\n",
    "for unit_id, unit_df in tqdm(data_filtered.groupby(\"unit_id\"), desc=\"Tuning analysis per unit\"):\n",
    "    unit_df[\"fr_epoch\"] =unit_df[\"fr_epoch\"] - unit_df[\"fr_baseline\"]  # Subtract baseline firing rate\n",
    "\n",
    "    # Skip if the firing rate doesn't vary (avoid singular fit issues)\n",
    "    if unit_df[\"fr_epoch\"].std() == 0:\n",
    "        continue\n",
    "\n",
    "    # Use a GLM with Poisson family + Wald Test if use_poisson is True\n",
    "    if use_poisson:\n",
    "        # Formula with categorical predictors for first_cat and first_num\n",
    "        model = smf.glm(\n",
    "            formula=\"fr_epoch ~ C(first_cat_simple) * C(first_num_simple)\",\n",
    "            # formula=\"fr_enc1 ~ C(first_cat) + C(first_num)\",\n",
    "            data=unit_df,\n",
    "            family=sm.families.Poisson(),\n",
    "        )\n",
    "        # Fit the model and perform Wald tests for each term\n",
    "        results = model.fit().wald_test_terms(scalar=True).table\n",
    "    else:\n",
    "        # Otherwise, use OLS and compute Type II ANOVA\n",
    "        model = smf.ols(\n",
    "            # formula=\"fr_epoch ~ C(first_cat_simple) + C(first_num_simple)\",\n",
    "            formula=\"fr_epoch ~ C(first_cat_simple) + C(first_num_simple)\",\n",
    "            data=unit_df,\n",
    "        )\n",
    "        results = sm.stats.anova_lm(model.fit(), typ=2)[:-1]\n",
    "        results = results.rename(columns={\"PR(>F)\": \"pvalue\"})\n",
    "\n",
    "    # Add the neuron identifier and additional info, if available\n",
    "    results[\"unit_id\"] = unit_id\n",
    "    if \"brainAreaOfCell\" in unit_df.columns:\n",
    "        results[\"brainAreaOfCell\"] = [unit_df[\"brainAreaOfCell\"].iloc[0]] * len(results)\n",
    "    \n",
    "    records.append(results)\n",
    "\n",
    "# Combine records from all units into a single DataFrame.\n",
    "records = pd.concat(records).reset_index(names=\"predictor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from wide to long format for plotting\n",
    "df_stats = records.melt(id_vars=[\"unit_id\", \"brainAreaOfCell\", \"predictor\"], value_vars=[\"pvalue\"])\n",
    "df_stats[\"is_significant\"] = df_stats[\"value\"] < 0.05\n",
    "\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot significant counts\n",
    "fg = sns.catplot(\n",
    "    data=df_stats[df_stats[\"predictor\"] != \"Intercept\"],\n",
    "    x=\"brainAreaOfCell\",\n",
    "    # order=[\"AMY\", \"HPC\", \"dACC\", \"preSMA\", \"vmPFC\", \"VTC\"],\n",
    "    hue=\"is_significant\",\n",
    "    col=\"predictor\",\n",
    "    kind=\"count\",\n",
    "    palette=[\"tab:red\", \"tab:green\"],\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg = sns.catplot(\n",
    "    data=df_stats.query(\"predictor != 'Intercept'\"),\n",
    "    x=\"brainAreaOfCell\",\n",
    "    hue=\"is_significant\",\n",
    "    col=\"predictor\",\n",
    "    kind=\"count\",\n",
    "    stat=\"percent\",        # <— tell seaborn to show % instead of raw counts\n",
    "    palette=[\"tab:red\", \"tab:green\"],\n",
    ")\n",
    "\n",
    "# optionally format the y-axis ticks as percents\n",
    "for ax in fg.axes.flat:\n",
    "    ax.yaxis.set_major_formatter(mpl.ticker.PercentFormatter())\n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return sig neuron idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sig_units = df_stats.loc[\n",
    "#     (df_stats[\"predictor\"] == \"C(first_cat_simple):C(first_num_simple)\") & (df_stats[\"is_significant\"]),\n",
    "#     \"unit_id\",\n",
    "# ].unique().tolist()\n",
    "\n",
    "sig_units = df_stats.loc[\n",
    "    (df_stats[\"predictor\"] == \"C(first_cat_simple)\") & (df_stats[\"is_significant\"]),\n",
    "    \"unit_id\"\n",
    "].unique().tolist()\n",
    "\n",
    "# sig_interaction_units = df_stats.loc[\n",
    "#     (df_stats[\"predictor\"] == \"C(first_cat_simple):C(first_num_simple)\") & (df_stats[\"is_significant\"]),\n",
    "#     \"unit_id\"\n",
    "# ].unique().tolist()\n",
    "\n",
    "# sig_cat_units = df_stats.loc[\n",
    "#     (df_stats[\"predictor\"] == \"C(first_cat_simple)\") & (df_stats[\"is_significant\"]),\n",
    "#     \"unit_id\"\n",
    "# ].unique().tolist()\n",
    "\n",
    "# sig_units = [u for u in sig_num_units if u in sig_cat_units if u not in sig_interaction_units]\n",
    "\n",
    "print(\"Significant units:\", sig_units)\n",
    "# print(\"Significant units for first_num:\", sig_num_units)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install git+https://github.com/ioqfwfq/rlab_neural_analysis.git@jz\n",
    "from neural_analysis.visualize import plot_spikes_with_PSTH\n",
    "from neural_analysis.spikes import get_spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the unit IDs of interest\n",
    "unit_ids = sig_units # or any other list of unit IDs you want to plot\n",
    "\n",
    "for unit_id in unit_ids:\n",
    "  # Select the data for this specific unit.\n",
    "  df_unit = data_filtered[data_filtered[\"unit_id\"] == unit_id].reset_index(drop=True)\n",
    "  \n",
    "  # Get the real brain area from the unit's data.\n",
    "  area = df_unit[\"brainAreaOfCell\"].iloc[0]\n",
    "  \n",
    "  cond = \"second_cat_simple\"    # or whichever condition you plan to use in your plotting\n",
    "  cmap = \"Set1\"\n",
    "  \n",
    "  # Filter data_filtered rows for the given unit\n",
    "  df_unit = data_filtered[data_filtered[\"unit_id\"] == unit_id].reset_index(drop=True)\n",
    "  # Now extract the corresponding labels and stats from this subset\n",
    "  group_labels = df_unit[cond].apply(lambda x: np.squeeze(x).item() if isinstance(x, (list, np.ndarray)) else x)\n",
    "  # print(group_labels)\n",
    "  stats = df_unit[\"first_cat_simple\"]\n",
    "\n",
    "  # Also get the alignments for this unit\n",
    "  alignments = np.asarray(epoch_ts[unit_id][:, 0], dtype=np.float64) / 1e6  # if using nanosecond times\n",
    "\n",
    "  # Get full spike train for the unit\n",
    "  spikes = np.asarray(df_unit[\"timestamps\"].iloc[0]).flatten().astype(np.float64) / 1e6\n",
    "  spikes = np.sort(spikes)\n",
    "\n",
    "  # Plot\n",
    "  axes = plot_spikes_with_PSTH(\n",
    "      spikes,\n",
    "      alignments,\n",
    "      window = (-1, 8),\n",
    "      group_labels=group_labels,\n",
    "      stats=stats,\n",
    "      plot_stats=False,\n",
    "      sig_test=True,\n",
    "      cmap=cmap,\n",
    "  )\n",
    "\n",
    "  # adjust plot visuals\n",
    "  # [ax.axvline(np.mean(trial_info[\"rt\"]), color=\"red\", ls=\"--\") for ax in axes]\n",
    "  axes[1].set_xlabel(\"Time from stim onset [s]\")\n",
    "  axes[0].set_title(f\"{area} {unit_id} [{cond} tuned]\")\n",
    "\n",
    "  plt.show()\n",
    "  # plt.savefig(f\"{area}_{unit_id}_2.png\", dpi=300, bbox_inches=\"tight\")\n",
    "  # plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pupulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import MDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select data\n",
    "# sub_df = data_filtered.query(\"unit_id in @sig_units\").reset_index(drop=True)\n",
    "sub_df = data_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly select 20 trials of each condition from each unit\n",
    "# note the the resultant df is sorted\n",
    "sub_df = sub_df.groupby([\"unit_id\", \"first_num_simple\", \"first_cat_simple\"]).sample(10)\n",
    "\n",
    "# collect into design matrix + labels\n",
    "X = np.column_stack(sub_df.groupby(\"unit_id\")[\"fr_epoch\"].agg(list))\n",
    "y = sub_df[\"first_num_simple\"].iloc[:len(X)].to_numpy(str)\n",
    "\n",
    "# fit SVM with cross-validation\n",
    "# Don't expect good performance since this is single-session\n",
    "pipe = Pipeline([(\"scaler\", StandardScaler()), (\"clf\", LinearSVC())])\n",
    "scores = cross_val_score(pipe, X, y, cv=5)\n",
    "print(f\"CV Accuracy: {np.mean(scores):.2f} ± {np.std(scores):.2f}\")\n",
    "\n",
    "# # collect into design matrix + labels\n",
    "# X = np.column_stack(sub_df.groupby(\"unit_id\")[\"fr_epoch\"].agg(list))\n",
    "# y = sub_df[\"first_num_simple\"].iloc[:len(X)].to_numpy(str)\n",
    "\n",
    "# # fit SVM with cross-validation\n",
    "# # Don't expect good performance since this is single-session\n",
    "# pipe = Pipeline([(\"scaler\", StandardScaler()), (\"clf\", LinearSVC())])\n",
    "# scores = cross_val_score(pipe, X, y, cv=5)\n",
    "# print(f\"CV Accuracy: {np.mean(scores):.2f} ± {np.std(scores):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode string labels into numeric\n",
    "y_encoded = LabelEncoder().fit_transform(y)\n",
    "\n",
    "\n",
    "# Dimensionality reduction\n",
    "pca = Pipeline([(\"scaler\", StandardScaler()), (\"pca\", PCA(n_components=3))])\n",
    "mds = Pipeline([(\"scaler\", StandardScaler()), (\"mds\", MDS(n_components=3))])\n",
    "X_pca = pca.fit_transform(X)\n",
    "X_mds = mds.fit_transform(X)\n",
    "\n",
    "# Visualize low-D representation\n",
    "# Note: matplotlib's 3D plotting is basic and \"fake.\"\n",
    "#       Use other packages recommended above as needed.\n",
    "plt.figure(figsize=(16, 8))\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "ax2 = plt.subplot(1, 2, 2, projection=\"3d\")\n",
    "ax1.scatter(X_pca[:, 1], X_pca[:, 2], c=y_encoded)\n",
    "scatter = ax2.scatter(X_mds[:, 0], X_mds[:, 1], X_mds[:, 2], c=y_encoded)\n",
    "legend1 = ax2.legend(*scatter.legend_elements()) # infer legend from scatter\n",
    "ax2.add_artist(legend1)\n",
    "ax1.set_title(\"PCA\")\n",
    "ax2.set_title(\"MDS\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
