{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "311b5f76",
   "metadata": {},
   "source": [
    "# WM conjunction coding\n",
    "conjunction coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12b221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af4a7f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install numpy pandas matplotlib seaborn scipy mat73\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from scipy.stats import ttest_1samp\n",
    "import glob\n",
    "import os\n",
    "# import mat73"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c48947",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "### Loading MATLAB objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9c79ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all WMB_P*_v7.mat files in the current directory\n",
    "mat_files = glob.glob('./WMB_P*_v7.mat')\n",
    "print(f\"Found {len(mat_files)} .mat files: {[os.path.basename(f) for f in mat_files]}\")\n",
    "\n",
    "# Initialize lists to store data from each file\n",
    "cell_mats = []\n",
    "total_mats = []\n",
    "\n",
    "# Load each file and append its data\n",
    "for mat_file in mat_files:\n",
    "    print(f\"\\nLoading {mat_file}...\")\n",
    "    mat_data = loadmat(mat_file)\n",
    "    cell_mats.append(mat_data['cellStatsAll'])\n",
    "    total_mats.append(mat_data['totStats'])\n",
    "\n",
    "# Print shapes of loaded data for debugging\n",
    "print(\"\\nShapes of loaded data:\")\n",
    "for i, (cell, total) in enumerate(zip(cell_mats, total_mats)):\n",
    "    print(f\"File {i}: cell_mat shape: {cell.shape}, total_mat shape: {total.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6f1d04",
   "metadata": {},
   "source": [
    "### Combine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6773f69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First, convert each cell_mat to a list of records\n",
    "all_cell_records = []\n",
    "for cell_mat in cell_mats:\n",
    "    # Convert to list of records\n",
    "    cell_list = cell_mat[0]  # now shape is (n,)\n",
    "    records = []\n",
    "    for cell in cell_list:\n",
    "        record = {key: cell[key] for key in cell.dtype.names}\n",
    "        records.append(record)\n",
    "    all_cell_records.extend(records)\n",
    "\n",
    "# Convert combined records to DataFrame\n",
    "df = pd.DataFrame(all_cell_records)\n",
    "\n",
    "# For total_mat, we can concatenate directly since they have the same structure\n",
    "total_mat = np.concatenate(total_mats, axis=0)\n",
    "\n",
    "print(f\"\\nCombined data shape - total_mat: {total_mat.shape}\")\n",
    "print(f\"\\nCombined data shape - df: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f401d2",
   "metadata": {},
   "source": [
    "### Translates area codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d83a648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_area_codes(area_column):\n",
    "    \n",
    "    mapping = {\n",
    "        1: 'RH', 2: 'LH', 3: 'RA', 4: 'LA', 5: 'RAC', 6: 'LAC',\n",
    "        7: 'RSMA', 8: 'LSMA', 9: 'RPT', 10: 'LPT', 11: 'ROFC', 12: 'LOFC',\n",
    "        50: 'RFFA', 51: 'REC', 52: 'RCM', 53: 'LCM', 54: 'RPUL', 55: 'LPUL',\n",
    "        56: 'N/A', 57: 'RPRV', 58: 'LPRV'\n",
    "    }\n",
    "    \n",
    "    labels = []\n",
    "    for code in area_column:\n",
    "        label = mapping.get(code, 'Unknown')\n",
    "        labels.append(label)\n",
    "    \n",
    "    return dict(Counter(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6192ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neuron number in each area\n",
    "area_codes = total_mat[:, 3]\n",
    "\n",
    "counts = count_area_codes(area_codes)\n",
    "print(\"Area counts (no prefix):\")\n",
    "for area, count in counts.items():\n",
    "    print(f\"{area}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2029f482",
   "metadata": {},
   "source": [
    "### Format cell data\n",
    "\n",
    "All brain area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d032add5",
   "metadata": {},
   "outputs": [],
   "source": [
    "collapsed_area_map = {\n",
    "    1: 'H', 2: 'H',\n",
    "    3: 'A', 4: 'A',\n",
    "    5: 'AC', 6: 'AC',\n",
    "    7: 'SMA', 8: 'SMA',\n",
    "    9: 'PT', 10: 'PT',\n",
    "    11: 'OFC', 12: 'OFC',\n",
    "    50: 'FFA', 51: 'EC',\n",
    "    52: 'CM', 53: 'CM',\n",
    "    54: 'PUL', 55: 'PUL',\n",
    "    56: 'N/A', 57: 'PRV', 58: 'PRV'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffeae2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert brain area codes in the DataFrame\n",
    "df['brainAreaOfCell'] = df['brainAreaOfCell'].apply(\n",
    "    lambda x: collapsed_area_map.get(int(x[0, 0]), 'Unknown') if isinstance(x, np.ndarray) else collapsed_area_map.get(x, 'Unknown')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3099b7eb",
   "metadata": {},
   "source": [
    "### Filter out units with low firing rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "375ea159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out units with low firing rate\n",
    "fr = df['timestamps'].apply(lambda x: len(x) / (x[-1] - x[0]) * 1e6)\n",
    "df_sample_new = df[fr > 0.1].reset_index(drop=True)\n",
    "\n",
    "# unit id\n",
    "df_sample_new = df_sample_new.reset_index(drop=True)\n",
    "df_sample_new[\"unit_id\"] = df_sample_new.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c7fde2",
   "metadata": {},
   "source": [
    "## Extract trial info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d90cfa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_trial_info(trials_struct, unit_id):\n",
    "    # Build a DataFrame from the trials structure.\n",
    "    # We use .squeeze() for each field – adjust if necessary.\n",
    "    df_trial = pd.DataFrame({field: trials_struct[field].squeeze() \n",
    "                             for field in trials_struct.dtype.names})\n",
    "    # Add the unit_id so that you can later separate trials by unit/session.\n",
    "    df_trial[\"unit_id\"] = unit_id\n",
    "    df_trial[\"trial_nr\"] = df_trial[\"trial\"].apply(lambda x: np.squeeze(x).item() if isinstance(x, (list, np.ndarray)) else x) - 1 # Adjust for 0-indexing\n",
    "    return df_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd9fe9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_info_list = []\n",
    "for idx, row in df_sample_new.iterrows():\n",
    "    # Use the unit identifier from this row\n",
    "    unit_id = row[\"unit_id\"]  \n",
    "    # Extract the trial DataFrame, including the unit identifier.\n",
    "    trial_info_list.append(extract_trial_info(row[\"Trials\"], unit_id, ))\n",
    "\n",
    "# Concatenate the list of trial info DataFrames into one.\n",
    "trial_info = pd.concat(trial_info_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3dcb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# % ttl values\n",
    "# c.marker.expstart        = 89;\n",
    "# c.marker.expend          = 90;\n",
    "# c.marker.fixOnset        = 10;\n",
    "# c.marker.pic1            = 1;\n",
    "# c.marker.delay1          = 2;\n",
    "# c.marker.pic2            = 3;\n",
    "# c.marker.delay2          = 4;\n",
    "# c.marker.probeOnset      = 5;\n",
    "# c.marker.response        = 6;\n",
    "# c.marker.break           = 91;\n",
    "\n",
    "# what names are in the df\n",
    "df_sample_new.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b4b604",
   "metadata": {},
   "source": [
    "## Event ts extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "747a58fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Event ts extraction\n",
    "def extract_event_timestamps(df_sample_new):\n",
    "    # Extract timestamp arrays for each epoch\n",
    "    epoch_ts = {}\n",
    "    \n",
    "    for i, row in df_sample_new.iterrows():\n",
    "        unit_id = row['unit_id']\n",
    "        events = row['events'].squeeze()       # Ensure it's 1D array\n",
    "        idxs1 = row['idxEnc1'].squeeze() - 1   # Ensure indices are 1D array; start with 0\n",
    "        idxs2 = row['idxDel1'].squeeze() - 1   # Use delay onset for epoch end\n",
    "        \n",
    "        # Index into events using the adjusted indices\n",
    "        extracted1 = events[idxs1]   # shape (n_trials, 3)\n",
    "        extracted2 = events[idxs2]   # shape (n_trials, 3)\n",
    "\n",
    "        # Store as event start/end times\n",
    "        combined = np.column_stack((extracted1[:, 0], extracted2[:, 0]))\n",
    "        epoch_ts[unit_id] = combined\n",
    "        \n",
    "    return epoch_ts\n",
    "\n",
    "# Get event timestamps\n",
    "epoch_ts = extract_event_timestamps(df_sample_new)\n",
    "\n",
    "# Now compute firing rates for baseline and stimulus epochs\n",
    "def compute_firing_rates(df_sample_new, epoch_ts):\n",
    "    # Baseline period: 1 second before stimulus\n",
    "    df_sample_new[\"fr_baseline\"] = df_sample_new.apply(\n",
    "        lambda row: [\n",
    "            np.sum((np.ravel(row[\"timestamps\"]) >= epoch_on - 1 * 1e6) & \n",
    "                   (np.ravel(row[\"timestamps\"]) < epoch_on)) / 1.0  # 1 second window\n",
    "            for epoch_on, _ in epoch_ts[row[\"unit_id\"]]\n",
    "        ],\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Stimulus period: from stimulus onset to end of encoding (defined by epoch_ts)\n",
    "    df_sample_new[\"fr_epoch\"] = df_sample_new.apply(\n",
    "        lambda row: [\n",
    "            np.sum((np.ravel(row[\"timestamps\"]) >= epoch_on) & \n",
    "                   (np.ravel(row[\"timestamps\"]) < epoch_off)) / ((epoch_off - epoch_on) / 1e6)\n",
    "            for epoch_on, epoch_off in epoch_ts[row[\"unit_id\"]]\n",
    "        ],\n",
    "        axis=1\n",
    "    )\n",
    "    # Add trial_nr column to track trial numbers\n",
    "    df_sample_new[\"trial_nr\"] = df_sample_new[\"fr_epoch\"].apply(lambda x: np.arange(len(x)))\n",
    "    \n",
    "    # Explode the dataframe so each trial is a row\n",
    "    df_exploded = df_sample_new.explode([\"fr_baseline\", \"fr_epoch\", \"trial_nr\"])\n",
    "    \n",
    "    return df_exploded\n",
    "\n",
    "# Compute firing rates\n",
    "df_sample_new = compute_firing_rates(df_sample_new, epoch_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "204c75c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_new = df_sample_new.reset_index(drop=True)\n",
    "trial_info = trial_info.reset_index(drop=True)\n",
    "\n",
    "data = pd.merge(\n",
    "    df_sample_new,\n",
    "    trial_info,\n",
    "    on=[\"unit_id\", \"trial_nr\"],\n",
    "    how=\"left\",\n",
    ").infer_objects()\n",
    "\n",
    "cols_to_keep = [\n",
    "    \"unit_id\", \"timestamps\", \"brainAreaOfCell\", \"fr_epoch\", \"fr_baseline\", \"trial_nr\",\n",
    "    \"first_cat\", \"second_cat\", \"first_num\", \"second_num\",\n",
    "    \"first_pic\", \"second_pic\", \"probe_cat\", \"probe_pic\",\n",
    "    \"probe_validity\", \"probe_num\", \"correct_answer\",\n",
    "    \"rt\", \"acc\", \"key\", \"cat_comparison\", \"events\", \"nTrials\", \"Trials\", \"idxEnc1\", \"idxEnc2\", \"idxDel1\",\n",
    "    \"idxDel2\", \"idxProbeOn\", \"idxResp\", \"nrProcessed\", \"periods_Enc1\", \"periods_Enc2\", \"periods_Del1\",\n",
    "    \"periods_Del2\", \"periods_Probe\", \"periods_Resp\", \"prestimEnc\", \"prestimMaint\", \"prestimProbe\",\n",
    "    \"prestimButtonPress\", \"poststimEnc\", \"poststimMaint\", \"poststimProbe\", \"poststimButtonPress\", \"sessionIdx\", \"channel\", \"cellNr\", \"sessionID\", \"origClusterID\"\n",
    "]\n",
    "\n",
    "\n",
    "data_filtered = data[cols_to_keep].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ef077e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to simpler, hashable values for categories and numbers\n",
    "data_filtered[\"first_cat_simple\"] = data_filtered[\"first_cat\"].apply(\n",
    "    lambda x: str(np.squeeze(x)) if isinstance(x, (list, np.ndarray)) else str(x)\n",
    ")\n",
    "data_filtered[\"second_cat_simple\"] = data_filtered[\"second_cat\"].apply(\n",
    "    lambda x: str(np.squeeze(x)) if isinstance(x, (list, np.ndarray)) else str(x)\n",
    ")\n",
    "data_filtered[\"first_num_simple\"] = data_filtered[\"first_num\"].apply(\n",
    "    lambda x: str(np.squeeze(x)) if isinstance(x, (list, np.ndarray)) else str(x)\n",
    ")\n",
    "data_filtered[\"second_num_simple\"] = data_filtered[\"second_num\"].apply(\n",
    "    lambda x: str(np.squeeze(x)) if isinstance(x, (list, np.ndarray)) else str(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf1a684",
   "metadata": {},
   "source": [
    "## Conjunction coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e33a911f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from tqdm import tqdm\n",
    "\n",
    "def identify_conjunctive_neurons(data_filtered):\n",
    "    \"\"\"\n",
    "    Identifies neurons that show conjunctive coding of stimulus features.\n",
    "    \n",
    "    This function analyzes neural responses to identify units that encode:\n",
    "    1. Conjunctive representations (interaction between features)\n",
    "    2. Feature-selective responses (main effects only)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_filtered : pandas.DataFrame\n",
    "        DataFrame containing neural data with columns:\n",
    "        - unit_id: unique identifier for each neuron\n",
    "        - fr_epoch: firing rate during the epoch\n",
    "        - fr_baseline: baseline firing rate\n",
    "        - first_cat_simple: stimulus category\n",
    "        - first_num_simple: stimulus number\n",
    "        - brainAreaOfCell: brain region of recorded neuron\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    conjunctive_units : list\n",
    "        List of unit IDs showing significant conjunctive coding\n",
    "    feature_selective_units : list\n",
    "        List of unit IDs showing significant selectivity for individual features\n",
    "    mixed_selectivity_stats : list\n",
    "        Detailed statistics for each unit's selectivity patterns\n",
    "    \"\"\"\n",
    "    conjunctive_units = []\n",
    "    feature_selective_units = []\n",
    "    mixed_selectivity_stats = []\n",
    "    \n",
    "    for unit_id, unit_df in data_filtered.groupby(\"unit_id\"):\n",
    "        # Center firing rates around baseline\n",
    "        unit_df = unit_df.copy()\n",
    "        unit_df[\"fr_normalized\"] = unit_df[\"fr_epoch\"] - unit_df[\"fr_baseline\"]\n",
    "        \n",
    "        # Skip units with no variance\n",
    "        if unit_df[\"fr_normalized\"].std() == 0:\n",
    "            continue\n",
    "            \n",
    "        # Model 1: Main effects only (category and number independently)\n",
    "        model_independent = smf.ols(\n",
    "            \"fr_normalized ~ C(first_cat_simple) + C(first_num_simple)\", \n",
    "            data=unit_df\n",
    "        )\n",
    "        \n",
    "        # Model 2: Main effects + Interaction (conjunctive coding)\n",
    "        model_conjunctive = smf.ols(\n",
    "            \"fr_normalized ~ C(first_cat_simple) * C(first_num_simple)\", \n",
    "            data=unit_df\n",
    "        )\n",
    "        \n",
    "        # Fit both models\n",
    "        results_independent = model_independent.fit()\n",
    "        results_conjunctive = model_conjunctive.fit()\n",
    "        \n",
    "        # Formal F-test comparing the models\n",
    "        from statsmodels.stats.anova import anova_lm\n",
    "        comparison = anova_lm(results_independent, results_conjunctive)\n",
    "        \n",
    "        # Extract interaction significance\n",
    "        interaction_pvalue = comparison[\"Pr(>F)\"].iloc[1]\n",
    "        \n",
    "        # Calculate effect sizes for main effects and interaction\n",
    "        anova_table = sm.stats.anova_lm(results_conjunctive, typ=2)\n",
    "        \n",
    "        # Store detailed statistics\n",
    "        stats = {\n",
    "            'unit_id': unit_id,\n",
    "            'area': unit_df['brainAreaOfCell'].iloc[0],\n",
    "            'interaction_pvalue': interaction_pvalue,\n",
    "            'cat_pvalue': anova_table.loc['C(first_cat_simple)', 'PR(>F)'],\n",
    "            'num_pvalue': anova_table.loc['C(first_num_simple)', 'PR(>F)'],\n",
    "            'r2_independent': results_independent.rsquared,\n",
    "            'r2_conjunctive': results_conjunctive.rsquared,\n",
    "            'r2_increase': results_conjunctive.rsquared - results_independent.rsquared,\n",
    "            'is_conjunctive': interaction_pvalue < 0.05,\n",
    "            'is_cat_selective': anova_table.loc['C(first_cat_simple)', 'PR(>F)'] < 0.05,\n",
    "            'is_num_selective': anova_table.loc['C(first_num_simple)', 'PR(>F)'] < 0.05\n",
    "        }\n",
    "        \n",
    "        mixed_selectivity_stats.append(stats)\n",
    "        \n",
    "        # Classify the neuron\n",
    "        if interaction_pvalue < 0.05:\n",
    "            conjunctive_units.append(unit_id)\n",
    "        elif anova_table.loc['C(first_cat_simple)', 'PR(>F)'] < 0.05 or \\\n",
    "             anova_table.loc['C(first_num_simple)', 'PR(>F)'] < 0.05:\n",
    "            feature_selective_units.append(unit_id)\n",
    "    \n",
    "    return pd.DataFrame(mixed_selectivity_stats), conjunctive_units, feature_selective_units\n",
    "def visualize_conjunctive_responses(data_filtered, unit_id):\n",
    "    \"\"\"Create a heatmap showing firing rates for each category-number combination\"\"\"\n",
    "    unit_data = data_filtered[data_filtered[\"unit_id\"] == unit_id].copy()\n",
    "    \n",
    "    # Pivot data to create category-number response matrix\n",
    "    response_matrix = unit_data.pivot_table(\n",
    "        index=\"first_cat_simple\", \n",
    "        columns=\"first_num_simple\",\n",
    "        values=\"fr_epoch\",\n",
    "        aggfunc=\"mean\"\n",
    "    )\n",
    "    \n",
    "    # Normalize by subtracting mean baseline\n",
    "    baseline = unit_data[\"fr_baseline\"].mean()\n",
    "    response_matrix_norm = response_matrix - baseline\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Raw firing rates\n",
    "    sns.heatmap(response_matrix, annot=True, fmt=\".1f\", cmap=\"viridis\", ax=ax1)\n",
    "    ax1.set_title(f\"Unit {unit_id}: Raw firing rates (Hz)\")\n",
    "    ax1.set_xlabel(\"Number\")\n",
    "    ax1.set_ylabel(\"Category\")\n",
    "    \n",
    "    # Normalized firing rates (change from baseline)\n",
    "    sns.heatmap(response_matrix_norm, annot=True, fmt=\".1f\", cmap=\"coolwarm\", \n",
    "                center=0, ax=ax2)\n",
    "    ax2.set_title(f\"Unit {unit_id}: Normalized firing rates (change from baseline)\")\n",
    "    ax2.set_xlabel(\"Number\")\n",
    "    ax2.set_ylabel(\"Category\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def characterize_conjunction_pattern(data_filtered, unit_id):\n",
    "    \"\"\"Analyze the pattern of conjunctive coding for a specific neuron\"\"\"\n",
    "    unit_data = data_filtered[data_filtered[\"unit_id\"] == unit_id].copy()\n",
    "    \n",
    "    # Fit the full model with interaction\n",
    "    model = smf.ols(\n",
    "        \"fr_epoch ~ C(first_cat_simple) * C(first_num_simple)\", \n",
    "        data=unit_data\n",
    "    ).fit()\n",
    "    \n",
    "    # Get the coefficients table\n",
    "    coef_table = model.summary2().tables[1]\n",
    "    \n",
    "    # Extract interaction coefficients\n",
    "    interaction_coefs = coef_table[coef_table.index.str.contains(\":\")].copy()\n",
    "    \n",
    "    # Parse the interaction terms to get category-number pairs\n",
    "    interaction_coefs[\"Category\"] = interaction_coefs.index.str.extract(r'C\\(first_cat_simple\\)\\[T.(.*?)\\]')[0]\n",
    "    interaction_coefs[\"Number\"] = interaction_coefs.index.str.extract(r'C\\(first_num_simple\\)\\[T.(.*?)\\]')[0]\n",
    "    \n",
    "    # Sort by coefficient magnitude to find strongest conjunctions\n",
    "    interaction_coefs[\"Abs_Coef\"] = abs(interaction_coefs[\"Coef.\"])\n",
    "    interaction_coefs = interaction_coefs.sort_values(\"Abs_Coef\", ascending=False)\n",
    "    \n",
    "    # Calculate selectivity index: ratio of strongest conjunction to mean of others\n",
    "    top_conj = interaction_coefs[\"Abs_Coef\"].max() if len(interaction_coefs) > 0 else 0\n",
    "    others_mean = interaction_coefs[\"Abs_Coef\"].iloc[1:].mean() if len(interaction_coefs) > 1 else 0\n",
    "    selectivity_index = top_conj / others_mean if others_mean > 0 else float('inf')\n",
    "    \n",
    "    return {\n",
    "        \"top_conjunctions\": interaction_coefs[[\"Category\", \"Number\", \"Coef.\", \"P>|t|\"]].head(3) if len(interaction_coefs) > 0 else pd.DataFrame(),\n",
    "        \"selectivity_index\": selectivity_index,\n",
    "        \"num_significant_conjunctions\": sum(interaction_coefs[\"P>|t|\"] < 0.05) if len(interaction_coefs) > 0 else 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba2dd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the main analysis to identify conjunctive neurons across all data\n",
    "print(\"Running conjunction analysis across all neurons...\")\n",
    "mixed_selectivity_stats, conjunctive_units, feature_selective_units = identify_conjunctive_neurons(data_filtered)\n",
    "\n",
    "# Summarize findings\n",
    "n_total = len(data_filtered['unit_id'].unique())\n",
    "n_conj = len(conjunctive_units)\n",
    "n_feat = len(feature_selective_units)\n",
    "\n",
    "print(f\"\\nOVERALL RESULTS:\")\n",
    "print(f\"Total units analyzed: {n_total}\")\n",
    "print(f\"Conjunctive-coding units: {n_conj} ({n_conj/n_total:.1%})\")\n",
    "print(f\"Feature-selective units: {n_feat} ({n_feat/n_total:.1%})\")\n",
    "print(f\"Non-responsive units: {n_total - n_conj - n_feat} ({(n_total - n_conj - n_feat)/n_total:.1%})\")\n",
    "\n",
    "# Visualize overall distribution of neuron types\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(['Conjunctive', 'Feature-selective', 'Non-responsive'], \n",
    "       [n_conj, n_feat, n_total - n_conj - n_feat])\n",
    "plt.title(\"Distribution of Neuron Types\")\n",
    "plt.ylabel(\"Number of Neurons\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"neuron_types_overall.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Analyze distribution by brain region\n",
    "region_stats = mixed_selectivity_stats.groupby('area').agg(\n",
    "    total_units=('unit_id', 'count'),\n",
    "    conjunctive=('is_conjunctive', 'sum'),\n",
    "    category_selective=('is_cat_selective', 'sum'),\n",
    "    number_selective=('is_num_selective', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "region_stats['conj_percent'] = region_stats['conjunctive'] / region_stats['total_units'] * 100\n",
    "region_stats['cat_percent'] = region_stats['category_selective'] / region_stats['total_units'] * 100\n",
    "region_stats['num_percent'] = region_stats['number_selective'] / region_stats['total_units'] * 100\n",
    "\n",
    "# Sort by percentage of conjunctive neurons\n",
    "region_stats_sorted = region_stats.sort_values('conj_percent', ascending=False)\n",
    "\n",
    "# Visualize regional distribution of conjunctive neurons\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(region_stats_sorted['area'], region_stats_sorted['conj_percent'])\n",
    "plt.title(\"Percentage of Conjunctive Neurons by Brain Region\")\n",
    "plt.ylabel(\"Percent of Units\")\n",
    "plt.xlabel(\"Brain Region\")\n",
    "plt.axhline(y=n_conj/n_total*100, color='r', linestyle='--', label='Overall Average')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"conjunctive_neurons_by_region.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Analyze the top 5 conjunctive neurons in detail\n",
    "top_conj_units = mixed_selectivity_stats.sort_values('r2_increase', ascending=False)['unit_id'].iloc[:5]\n",
    "print(f\"\\nAnalyzing top 5 conjunctive neurons in detail:\")\n",
    "\n",
    "for unit_id in top_conj_units:\n",
    "    unit_info = mixed_selectivity_stats[mixed_selectivity_stats['unit_id'] == unit_id].iloc[0]\n",
    "    print(f\"\\nUnit {unit_id} (Area: {unit_info['area']}):\")\n",
    "    print(f\"  R² increase from conjunction: {unit_info['r2_increase']:.3f}\")\n",
    "    print(f\"  Category p-value: {unit_info['cat_pvalue']:.4f}\")\n",
    "    print(f\"  Number p-value: {unit_info['num_pvalue']:.4f}\")\n",
    "    print(f\"  Interaction p-value: {unit_info['interaction_pvalue']:.4f}\")\n",
    "    \n",
    "    # Create response heatmap\n",
    "    fig = visualize_conjunctive_responses(data_filtered, unit_id)\n",
    "    plt.savefig(f\"unit_{unit_id}_response_matrix.png\", dpi=300)\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # Analyze conjunction pattern\n",
    "    pattern = characterize_conjunction_pattern(data_filtered, unit_id)\n",
    "    print(f\"  Selectivity index: {pattern['selectivity_index']:.2f}\")\n",
    "    print(f\"  Significant conjunctions: {pattern['num_significant_conjunctions']}\")\n",
    "    if len(pattern['top_conjunctions']) > 0:\n",
    "        print(\"  Top conjunctions:\")\n",
    "        print(pattern['top_conjunctions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7482ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate binding index to quantify the degree of conjunction coding\n",
    "def calculate_binding_index(data_filtered, unit_id):\n",
    "    \"\"\"\n",
    "    Calculate binding index for a given unit.\n",
    "    \n",
    "    Binding Index = Conjunction Selectivity / (Category Selectivity + Numerosity Selectivity)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_filtered : DataFrame\n",
    "        DataFrame containing neural data\n",
    "    unit_id : int\n",
    "        Unit identifier\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary with selectivity indices and binding index\n",
    "    \"\"\"\n",
    "    unit_data = data_filtered[data_filtered['unit_id'] == unit_id].copy()\n",
    "    \n",
    "    # Extract trial conditions and firing rates\n",
    "    categories = unit_data['first_cat_simple'].values\n",
    "    numerosities = unit_data['first_num_simple'].values\n",
    "    firing_rates = unit_data['fr_epoch'].values\n",
    "    \n",
    "    # Calculate mean firing rates for each condition\n",
    "    unique_categories = np.unique(categories)\n",
    "    unique_numerosities = np.unique(numerosities)\n",
    "    \n",
    "    # Category selectivity\n",
    "    fr_category = [np.mean(firing_rates[categories == cat]) for cat in unique_categories]\n",
    "    category_selectivity = np.max(fr_category) - np.min(fr_category)\n",
    "    \n",
    "    # Numerosity selectivity\n",
    "    fr_numerosity = [np.mean(firing_rates[numerosities == num]) for num in unique_numerosities]\n",
    "    numerosity_selectivity = np.max(fr_numerosity) - np.min(fr_numerosity)\n",
    "    \n",
    "    # Conjunction selectivity\n",
    "    fr_conjunction = []\n",
    "    for cat in unique_categories:\n",
    "        for num in unique_numerosities:\n",
    "            mask = (categories == cat) & (numerosities == num)\n",
    "            if np.any(mask):\n",
    "                fr_conjunction.append(np.mean(firing_rates[mask]))\n",
    "    \n",
    "    conjunction_selectivity = np.max(fr_conjunction) - np.min(fr_conjunction)\n",
    "    \n",
    "    # Calculate binding index\n",
    "    # Avoid division by zero\n",
    "    denominator = category_selectivity + numerosity_selectivity\n",
    "    binding_index = conjunction_selectivity / denominator if denominator > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'category_selectivity': category_selectivity,\n",
    "        'numerosity_selectivity': numerosity_selectivity,\n",
    "        'conjunction_selectivity': conjunction_selectivity,\n",
    "        'binding_index': binding_index\n",
    "    }\n",
    "\n",
    "# Calculate binding indices for all units\n",
    "print(\"Calculating binding indices for all units...\")\n",
    "binding_indices = []\n",
    "\n",
    "for unit_id in data_filtered['unit_id'].unique():\n",
    "    indices = calculate_binding_index(data_filtered, unit_id)\n",
    "    indices['unit_id'] = unit_id\n",
    "    indices['area'] = mixed_selectivity_stats[mixed_selectivity_stats['unit_id'] == unit_id]['area'].iloc[0]\n",
    "    binding_indices.append(indices)\n",
    "\n",
    "binding_df = pd.DataFrame(binding_indices)\n",
    "\n",
    "# Get conjunctive neurons\n",
    "conjunctive_units = mixed_selectivity_stats[mixed_selectivity_stats['is_conjunctive'] == 1]['unit_id'].values\n",
    "binding_df['is_conjunctive'] = binding_df['unit_id'].isin(conjunctive_units)\n",
    "\n",
    "# Summary statistics for all neurons\n",
    "print(f\"\\nBinding Index Summary (All Neurons):\")\n",
    "print(f\"Mean: {binding_df['binding_index'].mean():.3f}\")\n",
    "print(f\"Median: {binding_df['binding_index'].median():.3f}\")\n",
    "print(f\"Range: {binding_df['binding_index'].min():.3f} - {binding_df['binding_index'].max():.3f}\")\n",
    "print(f\"n: {len(binding_df)}\")\n",
    "\n",
    "# Summary statistics for conjunctive neurons\n",
    "conj_df = binding_df[binding_df['is_conjunctive']]\n",
    "print(f\"\\nBinding Index Summary (Conjunctive Neurons):\")\n",
    "print(f\"Mean: {conj_df['binding_index'].mean():.3f}\")\n",
    "print(f\"Median: {conj_df['binding_index'].median():.3f}\")\n",
    "print(f\"Range: {conj_df['binding_index'].min():.3f} - {conj_df['binding_index'].max():.3f}\")\n",
    "print(f\"n: {len(conj_df)}\")\n",
    "\n",
    "# Visualize distribution of binding indices\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(binding_df[~binding_df['is_conjunctive']]['binding_index'], bins=20, \n",
    "         alpha=0.7, label='Non-conjunctive Neurons')\n",
    "plt.hist(conj_df['binding_index'], bins=20, \n",
    "         alpha=0.7, label='Conjunctive Neurons')\n",
    "plt.title(\"Distribution of Binding Indices\")\n",
    "plt.xlabel(\"Binding Index\")\n",
    "plt.ylabel(\"Number of Units\")\n",
    "plt.axvline(binding_df['binding_index'].mean(), color='r', linestyle='--', \n",
    "            label=f'All Mean: {binding_df[\"binding_index\"].mean():.3f}')\n",
    "plt.axvline(conj_df['binding_index'].mean(), color='g', linestyle='--', \n",
    "            label=f'Conjunctive Mean: {conj_df[\"binding_index\"].mean():.3f}')\n",
    "plt.legend()\n",
    "plt.savefig(\"binding_index_distribution.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Create 3D visualization of neurons in selectivity space\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Color points by brain area\n",
    "areas = binding_df['area'].unique()\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(areas)))\n",
    "area_color_map = dict(zip(areas, colors))\n",
    "\n",
    "for area in areas:\n",
    "    area_data = binding_df[binding_df['area'] == area]\n",
    "    ax.scatter(\n",
    "        area_data['category_selectivity'],\n",
    "        area_data['numerosity_selectivity'],\n",
    "        area_data['binding_index'],\n",
    "        label=area,\n",
    "        alpha=0.7,\n",
    "        s=50,\n",
    "        color=area_color_map[area]\n",
    "    )\n",
    "\n",
    "# Highlight conjunctive neurons\n",
    "conjunctive_neurons = binding_df[binding_df['unit_id'].isin(mixed_selectivity_stats[mixed_selectivity_stats['is_conjunctive'] == 1]['unit_id'])]\n",
    "ax.scatter(\n",
    "    conjunctive_neurons['category_selectivity'],\n",
    "    conjunctive_neurons['numerosity_selectivity'],\n",
    "    conjunctive_neurons['binding_index'],\n",
    "    color='red',\n",
    "    s=100,\n",
    "    marker='*',\n",
    "    label='Conjunctive Neurons'\n",
    ")\n",
    "\n",
    "ax.set_xlabel('Category Selectivity')\n",
    "ax.set_ylabel('Numerosity Selectivity')\n",
    "ax.set_zlabel('Binding Index')\n",
    "ax.set_title('3D Distribution of Neuronal Selectivity')\n",
    "\n",
    "# Add legend with smaller font size\n",
    "ax.legend(fontsize='small', loc='upper right')\n",
    "\n",
    "# Add grid for better visualization\n",
    "ax.grid(True)\n",
    "\n",
    "# Adjust view angle for better visualization\n",
    "ax.view_init(elev=30, azim=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"selectivity_3d_space.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Analyze relationship between binding index and brain regions\n",
    "region_binding = binding_df.groupby('area').agg(\n",
    "    mean_binding=('binding_index', 'mean'),\n",
    "    median_binding=('binding_index', 'median'),\n",
    "    max_binding=('binding_index', 'max'),\n",
    "    unit_count=('unit_id', 'count')\n",
    ").reset_index()\n",
    "\n",
    "# Sort by mean binding index\n",
    "region_binding_sorted = region_binding.sort_values('mean_binding', ascending=False)\n",
    "\n",
    "# Visualize binding index by brain region\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(region_binding_sorted['area'], region_binding_sorted['mean_binding'])\n",
    "plt.title(\"Mean Binding Index by Brain Region\")\n",
    "plt.ylabel(\"Mean Binding Index\")\n",
    "plt.xlabel(\"Brain Region\")\n",
    "plt.axhline(y=binding_df['binding_index'].mean(), color='r', linestyle='--', label='Overall Average')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"binding_index_by_region.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4743a78f",
   "metadata": {},
   "source": [
    "### Temporal Dynamics of Binding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06f2a918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter1d\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def extract_trial_aligned_spikes(data_filtered, unit_id, event_marker='idxEnc1', window=(-1.0, 7.0)):\n",
    "    \"\"\"\n",
    "    Extract spike times aligned to a specific event for a given unit.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_filtered : DataFrame\n",
    "        DataFrame containing neural data\n",
    "    unit_id : int\n",
    "        Unit identifier\n",
    "    event_marker : str\n",
    "        Event marker to align spikes to ('idxEnc1', 'idxDel1', 'idxProbeOn', etc.)\n",
    "    window : tuple\n",
    "        Time window around event in seconds (pre, post)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary with aligned spike times for each trial\n",
    "    \"\"\"\n",
    "    unit_data = data_filtered[data_filtered['unit_id'] == unit_id].copy()\n",
    "    \n",
    "    # Get full spike train for this unit\n",
    "    spikes = np.asarray(unit_data[\"timestamps\"].iloc[0]).flatten().astype(np.float64) / 1e6\n",
    "    spikes = np.sort(spikes)\n",
    "    \n",
    "    # Get events for alignment\n",
    "    events = unit_data['events'].iloc[0].squeeze()\n",
    "    event_indices = unit_data[event_marker].iloc[0].squeeze() - 1  # Convert to 0-indexed\n",
    "    \n",
    "    trial_aligned_spikes = {}\n",
    "    \n",
    "    # For each trial, get spike times relative to the alignment event\n",
    "    for trial_nr, event_idx in enumerate(event_indices):\n",
    "        if isinstance(event_idx, np.ndarray):\n",
    "            event_idx = event_idx[0]\n",
    "            \n",
    "        # Get timestamp for this event\n",
    "        event_time = events[event_idx, 0] / 1e6  # Convert to seconds\n",
    "        \n",
    "        # Get spikes around this event\n",
    "        window_start = event_time + window[0]  # pre-event time\n",
    "        window_end = event_time + window[1]    # post-event time\n",
    "        \n",
    "        # Find spikes in this window\n",
    "        trial_spikes = spikes[(spikes >= window_start) & (spikes <= window_end)]\n",
    "        \n",
    "        # Convert to seconds relative to event\n",
    "        trial_spikes_aligned = trial_spikes - event_time\n",
    "        \n",
    "        # Store with trial info\n",
    "        try:\n",
    "            first_cat = unit_data.iloc[trial_nr]['first_cat_simple']\n",
    "            first_num = unit_data.iloc[trial_nr]['first_num_simple']\n",
    "            \n",
    "            trial_aligned_spikes[trial_nr] = {\n",
    "                'spikes': trial_spikes_aligned,\n",
    "                'first_cat': first_cat,\n",
    "                'first_num': first_num\n",
    "            }\n",
    "        except:\n",
    "            # Skip trials with missing info\n",
    "            continue\n",
    "    \n",
    "    return trial_aligned_spikes\n",
    "\n",
    "def analyze_binding_dynamics(data_filtered, unit_id):\n",
    "    \"\"\"\n",
    "    Analyze how conjunction coding emerges over time for a specific neuron.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_filtered : DataFrame\n",
    "        DataFrame containing neural data\n",
    "    unit_id : int\n",
    "        Unit identifier\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        DataFrame with p-values over time\n",
    "    matplotlib.figure.Figure\n",
    "        Figure showing temporal dynamics\n",
    "    \"\"\"\n",
    "    unit_data = data_filtered[data_filtered[\"unit_id\"] == unit_id].copy()\n",
    "    \n",
    "    # Get trial-aligned spikes\n",
    "    aligned_spikes = extract_trial_aligned_spikes(data_filtered, unit_id)\n",
    "    \n",
    "    # Define time windows with overlap\n",
    "    window_width = 50  # milliseconds - smaller window for more time bins\n",
    "    step_size = 25  # milliseconds - smaller step size for more time bins\n",
    "    t_start = -500  # start before stimulus (ms)\n",
    "    t_end = 2000    # end after stimulus (ms)\n",
    "    \n",
    "    windows = [(t, t + window_width) for t in range(t_start, t_end - window_width, step_size)]\n",
    "    window_centers = [t + window_width/2 for t in range(t_start, t_end - window_width, step_size)]\n",
    "    \n",
    "    # Initialize results containers\n",
    "    cat_pvals = []\n",
    "    num_pvals = []\n",
    "    interaction_pvals = []\n",
    "    r2_increases = []\n",
    "    \n",
    "    # Import required modules\n",
    "    import statsmodels.formula.api as smf\n",
    "    import statsmodels.api as sm\n",
    "    \n",
    "    # Analyze each time window\n",
    "    for window in windows:\n",
    "        # Count spikes in window for each trial\n",
    "        spike_counts = []\n",
    "        categories = []\n",
    "        numbers = []\n",
    "        \n",
    "        for trial_nr, trial_data in aligned_spikes.items():\n",
    "            # Extract spikes in this window (convert ms to s)\n",
    "            window_start_s = window[0] / 1000\n",
    "            window_end_s = window[1] / 1000\n",
    "            trial_spikes = trial_data['spikes']\n",
    "            \n",
    "            # Count spikes in window\n",
    "            count = np.sum((trial_spikes >= window_start_s) & (trial_spikes < window_end_s))\n",
    "            \n",
    "            spike_counts.append(count)\n",
    "            categories.append(trial_data['first_cat'])\n",
    "            numbers.append(trial_data['first_num'])\n",
    "        \n",
    "        # Create temporary dataframe for this window\n",
    "        temp_df = pd.DataFrame({\n",
    "            'window_fr': spike_counts,\n",
    "            'first_cat_simple': categories,\n",
    "            'first_num_simple': numbers\n",
    "        })\n",
    "        \n",
    "        # Skip windows with no spikes\n",
    "        if sum(spike_counts) == 0:\n",
    "            cat_pvals.append(1.0)\n",
    "            num_pvals.append(1.0)\n",
    "            interaction_pvals.append(1.0)\n",
    "            r2_increases.append(0.0)\n",
    "            continue\n",
    "        \n",
    "        # Fit models\n",
    "        try:\n",
    "            model_main = smf.ols(\"window_fr ~ C(first_cat_simple) + C(first_num_simple)\", data=temp_df).fit()\n",
    "            model_inter = smf.ols(\"window_fr ~ C(first_cat_simple) * C(first_num_simple)\", data=temp_df).fit()\n",
    "            \n",
    "            # Extract statistics\n",
    "            anova = sm.stats.anova_lm(model_inter, typ=2)\n",
    "            \n",
    "            # Safely extract p-values\n",
    "            cat_pval = anova.loc['C(first_cat_simple)', 'PR(>F)'] if 'C(first_cat_simple)' in anova.index else 1.0\n",
    "            num_pval = anova.loc['C(first_num_simple)', 'PR(>F)'] if 'C(first_num_simple)' in anova.index else 1.0\n",
    "            \n",
    "            cat_pvals.append(cat_pval)\n",
    "            num_pvals.append(num_pval)\n",
    "            \n",
    "            if 'C(first_cat_simple):C(first_num_simple)' in anova.index:\n",
    "                interaction_pvals.append(anova.loc['C(first_cat_simple):C(first_num_simple)', 'PR(>F)'])\n",
    "            else:\n",
    "                interaction_pvals.append(1.0)\n",
    "                \n",
    "            r2_increases.append(model_inter.rsquared - model_main.rsquared)\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Handle errors (e.g., perfect separation)\n",
    "            cat_pvals.append(1.0)\n",
    "            num_pvals.append(1.0)\n",
    "            interaction_pvals.append(1.0)\n",
    "            r2_increases.append(0.0)\n",
    "    \n",
    "    # Create result dataframe\n",
    "    result = pd.DataFrame({\n",
    "        'time': window_centers,\n",
    "        'category_p': cat_pvals,\n",
    "        'number_p': num_pvals,\n",
    "        'interaction_p': interaction_pvals,\n",
    "        'r2_increase': r2_increases,\n",
    "        'category_sig': np.array(cat_pvals) < 0.05,\n",
    "        'number_sig': np.array(num_pvals) < 0.05,\n",
    "        'conjunction_sig': np.array(interaction_pvals) < 0.05\n",
    "    })\n",
    "    \n",
    "    # Apply smoothing for visualization\n",
    "    smoothed_cat = gaussian_filter1d(-np.log10(np.array(cat_pvals)), sigma=2)\n",
    "    smoothed_num = gaussian_filter1d(-np.log10(np.array(num_pvals)), sigma=2)\n",
    "    smoothed_int = gaussian_filter1d(-np.log10(np.array(interaction_pvals)), sigma=2)\n",
    "    smoothed_r2 = gaussian_filter1d(np.array(r2_increases), sigma=2)\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(12, 10), sharex=True)\n",
    "    \n",
    "    # Plot significance\n",
    "    axes[0].plot(window_centers, smoothed_cat, 'b-', label='Category', linewidth=2)\n",
    "    axes[0].plot(window_centers, smoothed_num, 'g-', label='Number', linewidth=2)\n",
    "    axes[0].plot(window_centers, smoothed_int, 'r-', label='Conjunction', linewidth=2)\n",
    "    axes[0].axhline(-np.log10(0.05), color='k', linestyle='--', label='p=0.05')\n",
    "    axes[0].set_ylabel('-log10(p-value)')\n",
    "    axes[0].set_title(f'Unit {unit_id}: Temporal Evolution of Feature Encoding')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Plot R² increase\n",
    "    axes[1].plot(window_centers, smoothed_r2, 'k-', linewidth=2)\n",
    "    axes[1].set_ylabel('R² increase from interaction')\n",
    "    axes[1].set_xlabel('Time from stimulus onset (ms)')\n",
    "    \n",
    "    # Mark key epochs\n",
    "    for ax in axes:\n",
    "        ax.axvline(0, color='k', linestyle='-', alpha=0.3, label='Stimulus onset')\n",
    "        ax.axvline(1000, color='r', linestyle='-', alpha=0.3, label='Delay 1')\n",
    "        \n",
    "        # Only add legend to the first plot to avoid duplicates\n",
    "        if ax == axes[0]:\n",
    "            handles, labels = ax.get_legend_handles_labels()\n",
    "            unique = [(h, l) for i, (h, l) in enumerate(zip(handles, labels)) if l not in labels[:i]]\n",
    "            ax.legend(*zip(*unique), loc='upper right')\n",
    "    \n",
    "    # Set reasonable y-limits\n",
    "    axes[0].set_ylim(bottom=0)\n",
    "    if np.max(smoothed_r2) > 0:\n",
    "        axes[1].set_ylim(bottom=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return result, fig\n",
    "\n",
    "def cross_temporal_binding_analysis(data_filtered, responsive_units):\n",
    "    \"\"\"\n",
    "    Perform cross-temporal decoding to track binding information across the neural population.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_filtered : DataFrame\n",
    "        DataFrame containing neural data\n",
    "    responsive_units : list\n",
    "        List of responsive unit IDs\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary with decoding accuracies\n",
    "    matplotlib.figure.Figure\n",
    "        Figure showing cross-temporal decoding results\n",
    "    \"\"\"\n",
    "    # Define time windows covering the entire trial\n",
    "    window_width = 100  # milliseconds - smaller window for more time bins\n",
    "    step_size = 50     # milliseconds - smaller step size for more time bins\n",
    "    t_start = -500     # start 500ms before stimulus\n",
    "    t_end = 2000        # end 2 seconds after stimulus\n",
    "    \n",
    "    windows = [(t, t + window_width) for t in range(t_start, t_end - window_width, step_size)]\n",
    "    window_centers = [t + window_width/2 for t in range(t_start, t_end - window_width, step_size)]\n",
    "    window_centers = np.array(window_centers)\n",
    "    n_windows = len(windows)\n",
    "    \n",
    "    # Initialize matrices to store population response\n",
    "    n_units = len(responsive_units)\n",
    "    n_trials = len(data_filtered['trial_nr'].unique())\n",
    "    \n",
    "    # Dictionary to store neural data for each time window\n",
    "    X_time_windows = {}\n",
    "    \n",
    "    # We also need to track categories and numbers for each trial\n",
    "    categories = []\n",
    "    numbers = []\n",
    "    conjunctions = []\n",
    "    \n",
    "    # Process each unit\n",
    "    all_trial_data = []\n",
    "    \n",
    "    print(\"Extracting spike data for each unit and time window...\")\n",
    "    for unit_idx, unit_id in enumerate(tqdm(responsive_units)):\n",
    "        # Get aligned spikes for this unit\n",
    "        aligned_spikes = extract_trial_aligned_spikes(data_filtered, unit_id, window=(-0.5, 2.0))\n",
    "        \n",
    "        # Process each time window\n",
    "        for window_idx, (window_start, window_end) in enumerate(windows):\n",
    "            window_key = f\"window_{window_idx}\"\n",
    "            \n",
    "            if window_key not in X_time_windows:\n",
    "                X_time_windows[window_key] = np.zeros((n_trials, n_units))\n",
    "            \n",
    "            # Process each trial\n",
    "            for trial_idx, (trial_nr, trial_data) in enumerate(aligned_spikes.items()):\n",
    "                # Convert window boundaries to seconds\n",
    "                window_start_s = window_start / 1000\n",
    "                window_end_s = window_end / 1000\n",
    "                \n",
    "                # Count spikes in this window\n",
    "                spike_count = np.sum((trial_data['spikes'] >= window_start_s) & \n",
    "                                     (trial_data['spikes'] < window_end_s))\n",
    "                \n",
    "                # Store spike count\n",
    "                X_time_windows[window_key][trial_idx, unit_idx] = spike_count\n",
    "                \n",
    "                # Store labels (only once per trial)\n",
    "                if unit_idx == 0 and window_idx == 0:\n",
    "                    categories.append(trial_data['first_cat'])\n",
    "                    numbers.append(trial_data['first_num'])\n",
    "                    \n",
    "                    # Create a unique conjunction ID\n",
    "                    conj_id = f\"{trial_data['first_cat']}_{trial_data['first_num']}\"\n",
    "                    conjunctions.append(conj_id)\n",
    "                    \n",
    "                    # Store trial data for reference\n",
    "                    all_trial_data.append({\n",
    "                        'trial_nr': trial_nr,\n",
    "                        'first_cat': trial_data['first_cat'],\n",
    "                        'first_num': trial_data['first_num'],\n",
    "                        'conj_id': conj_id\n",
    "                    })\n",
    "    \n",
    "    # Convert labels to arrays\n",
    "    categories = np.array(categories)\n",
    "    numbers = np.array(numbers)\n",
    "    \n",
    "    # Create numerical labels for conjunctions\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    conjunction_labels = le.fit_transform(conjunctions)\n",
    "    \n",
    "    # Initialize decoding accuracy matrices\n",
    "    cat_accuracy = np.zeros((n_windows, n_windows))\n",
    "    num_accuracy = np.zeros((n_windows, n_windows))\n",
    "    conj_accuracy = np.zeros((n_windows, n_windows))\n",
    "    \n",
    "    print(\"Performing cross-temporal decoding...\")\n",
    "    # Perform cross-temporal decoding\n",
    "    for train_idx in tqdm(range(n_windows)):\n",
    "        train_key = f\"window_{train_idx}\"\n",
    "        X_train = X_time_windows[train_key]\n",
    "        \n",
    "        # Standardize\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        \n",
    "        # Train models\n",
    "        cat_model = LogisticRegression(max_iter=1000, C=1.0, solver='liblinear')\n",
    "        num_model = LogisticRegression(max_iter=1000, C=1.0, solver='liblinear')\n",
    "        conj_model = LogisticRegression(max_iter=1000, C=1.0, solver='liblinear')\n",
    "        \n",
    "        # Fit models\n",
    "        cat_model.fit(X_train_scaled, categories)\n",
    "        num_model.fit(X_train_scaled, numbers)\n",
    "        conj_model.fit(X_train_scaled, conjunction_labels)\n",
    "        \n",
    "        # Test on all time windows\n",
    "        for test_idx in range(n_windows):\n",
    "            test_key = f\"window_{test_idx}\"\n",
    "            X_test = X_time_windows[test_key]\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "            # Predict and calculate accuracy\n",
    "            cat_pred = cat_model.predict(X_test_scaled)\n",
    "            num_pred = num_model.predict(X_test_scaled)\n",
    "            conj_pred = conj_model.predict(X_test_scaled)\n",
    "            \n",
    "            cat_accuracy[train_idx, test_idx] = balanced_accuracy_score(categories, cat_pred)\n",
    "            num_accuracy[train_idx, test_idx] = balanced_accuracy_score(numbers, num_pred)\n",
    "            conj_accuracy[train_idx, test_idx] = balanced_accuracy_score(conjunction_labels, conj_pred)\n",
    "    \n",
    "    # Apply smoothing for better visualization\n",
    "    from scipy.ndimage import gaussian_filter\n",
    "    cat_accuracy_smooth = gaussian_filter(cat_accuracy, sigma=1.0)\n",
    "    num_accuracy_smooth = gaussian_filter(num_accuracy, sigma=1.0)\n",
    "    conj_accuracy_smooth = gaussian_filter(conj_accuracy, sigma=1.0)\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "    \n",
    "    # Calculate chance levels\n",
    "    cat_chance = 1.0 / len(np.unique(categories))\n",
    "    num_chance = 1.0 / len(np.unique(numbers))\n",
    "    conj_chance = 1.0 / len(np.unique(conjunction_labels))\n",
    "    \n",
    "    # Set vmin and vmax for consistent color scaling\n",
    "    vmin = min(cat_chance, num_chance, conj_chance)\n",
    "    vmax = max(np.max(cat_accuracy_smooth), np.max(num_accuracy_smooth), np.max(conj_accuracy_smooth))\n",
    "    \n",
    "    # Category decoding\n",
    "    im0 = axes[0].imshow(cat_accuracy_smooth, cmap='viridis', vmin=vmin, vmax=vmax,\n",
    "                     extent=[window_centers[0], window_centers[-1], window_centers[-1], window_centers[0]])\n",
    "    axes[0].set_title(f\"Category Decoding (chance={cat_chance:.2f})\")\n",
    "    axes[0].set_xlabel(\"Testing time (ms)\")\n",
    "    axes[0].set_ylabel(\"Training time (ms)\")\n",
    "    plt.colorbar(im0, ax=axes[0])\n",
    "    \n",
    "    # Number decoding\n",
    "    im1 = axes[1].imshow(num_accuracy_smooth, cmap='viridis', vmin=vmin, vmax=vmax,\n",
    "                     extent=[window_centers[0], window_centers[-1], window_centers[-1], window_centers[0]])\n",
    "    axes[1].set_title(f\"Number Decoding (chance={num_chance:.2f})\")\n",
    "    axes[1].set_xlabel(\"Testing time (ms)\")\n",
    "    plt.colorbar(im1, ax=axes[1])\n",
    "    \n",
    "    # Conjunction decoding\n",
    "    im2 = axes[2].imshow(conj_accuracy_smooth, cmap='viridis', vmin=vmin, vmax=vmax,\n",
    "                     extent=[window_centers[0], window_centers[-1], window_centers[-1], window_centers[0]])\n",
    "    axes[2].set_title(f\"Conjunction Decoding (chance={conj_chance:.2f})\")\n",
    "    axes[2].set_xlabel(\"Testing time (ms)\")\n",
    "    plt.colorbar(im2, ax=axes[2])\n",
    "    \n",
    "    # Add reference lines for key task events\n",
    "    for ax in axes:\n",
    "        # Diagonal line (training=testing)\n",
    "        ax.plot([window_centers[0], window_centers[-1]], [window_centers[0], window_centers[-1]], \n",
    "                'k--', alpha=0.5)\n",
    "        \n",
    "        # Stimulus onsets and task events\n",
    "        ax.axvline(0, color='r', linestyle='-', alpha=0.5, label='Stim 1')\n",
    "        ax.axhline(0, color='r', linestyle='-', alpha=0.5)\n",
    "        \n",
    "        # Delay 1 onset\n",
    "        ax.axvline(1000, color='b', linestyle='--', alpha=0.5, label='Delay 1')\n",
    "        ax.axhline(1000, color='b', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        # Add a legend to the first plot only\n",
    "        if ax == axes[0]:\n",
    "            handles, labels = ax.get_legend_handles_labels()\n",
    "            by_label = dict(zip(labels, handles))\n",
    "            ax.legend(by_label.values(), by_label.keys(), loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Extract diagonal accuracies (same-time decoding)\n",
    "    cat_diag = np.diag(cat_accuracy_smooth)\n",
    "    num_diag = np.diag(num_accuracy_smooth)\n",
    "    conj_diag = np.diag(conj_accuracy_smooth)\n",
    "    \n",
    "    # Create a second figure showing time course of decoding accuracy\n",
    "    fig2, ax2 = plt.subplots(figsize=(14, 7))\n",
    "    ax2.plot(window_centers, cat_diag, 'b-', label='Category', linewidth=2)\n",
    "    ax2.plot(window_centers, num_diag, 'g-', label='Number', linewidth=2)\n",
    "    ax2.plot(window_centers, conj_diag, 'r-', label='Conjunction', linewidth=2)\n",
    "    \n",
    "    # Add chance levels\n",
    "    ax2.axhline(y=cat_chance, color='b', linestyle='--', alpha=0.5, label='Cat. chance')\n",
    "    ax2.axhline(y=num_chance, color='g', linestyle='--', alpha=0.5, label='Num. chance')\n",
    "    ax2.axhline(y=conj_chance, color='r', linestyle='--', alpha=0.5, label='Conj. chance')\n",
    "    \n",
    "    # Add event markers with shaded regions for key trial phases\n",
    "    # Stimulus 1 presentation (0-1000ms)\n",
    "    ax2.axvspan(0, 1000, color='lightgray', alpha=0.3, label='Stim 1')\n",
    "    \n",
    "    # Delay 1 period (1000-2000ms)\n",
    "    ax2.axvspan(1000, 2000, color='lightblue', alpha=0.2, label='Delay 1')\n",
    "    \n",
    "    # Add vertical lines for key events\n",
    "    ax2.axvline(x=0, color='k', linestyle='-', alpha=0.5, label='Stim 1 onset')\n",
    "    ax2.axvline(x=1000, color='k', linestyle='--', alpha=0.5, label='Delay 1 onset')\n",
    "    \n",
    "    ax2.set_xlabel('Time (ms)')\n",
    "    ax2.set_ylabel('Decoding Accuracy')\n",
    "    ax2.set_title('Time Course of Information Representation Throughout Trial')\n",
    "    \n",
    "    # Only show some handles in the legend to avoid overcrowding\n",
    "    handles, labels = ax2.get_legend_handles_labels()\n",
    "    selected_handles = [handles[0], handles[1], handles[2], handles[3], handles[4], handles[5]]\n",
    "    selected_labels = [labels[0], labels[1], labels[2], labels[3], labels[4], labels[5]]\n",
    "    ax2.legend(selected_handles, selected_labels, loc='upper right')\n",
    "    \n",
    "    ax2.set_ylim(vmin-0.05, vmax+0.05)\n",
    "    ax2.set_xlim(t_start, t_end)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Calculate when each type of information reaches peak\n",
    "    cat_peak_idx = np.argmax(cat_diag)\n",
    "    num_peak_idx = np.argmax(num_diag)\n",
    "    conj_peak_idx = np.argmax(conj_diag)\n",
    "    \n",
    "    cat_peak_time = window_centers[cat_peak_idx]\n",
    "    num_peak_time = window_centers[num_peak_idx]\n",
    "    conj_peak_time = window_centers[conj_peak_idx]\n",
    "    \n",
    "    # Calculate temporal offsets\n",
    "    if conj_peak_time > cat_peak_time and conj_peak_time > num_peak_time:\n",
    "        binding_delay = min(conj_peak_time - cat_peak_time, conj_peak_time - num_peak_time)\n",
    "    else:\n",
    "        binding_delay = 0\n",
    "    \n",
    "    # Calculate persistence of information\n",
    "    # Find all timepoints with above-chance decoding\n",
    "    cat_above_chance = np.where(cat_diag > (cat_chance + 0.05))[0]\n",
    "    num_above_chance = np.where(num_diag > (num_chance + 0.05))[0]\n",
    "    conj_above_chance = np.where(conj_diag > (conj_chance + 0.05))[0]\n",
    "    \n",
    "    # Calculate persistence (duration of significant decoding)\n",
    "    if len(cat_above_chance) > 0:\n",
    "        cat_persistence = window_centers[cat_above_chance[-1]] - window_centers[cat_above_chance[0]]\n",
    "    else:\n",
    "        cat_persistence = 0\n",
    "        \n",
    "    if len(num_above_chance) > 0:\n",
    "        num_persistence = window_centers[num_above_chance[-1]] - window_centers[num_above_chance[0]]\n",
    "    else:\n",
    "        num_persistence = 0\n",
    "        \n",
    "    if len(conj_above_chance) > 0:\n",
    "        conj_persistence = window_centers[conj_above_chance[-1]] - window_centers[conj_above_chance[0]]\n",
    "    else:\n",
    "        conj_persistence = 0\n",
    "    \n",
    "    results = {\n",
    "        'cat_accuracy': cat_accuracy_smooth,\n",
    "        'num_accuracy': num_accuracy_smooth,\n",
    "        'conj_accuracy': conj_accuracy_smooth,\n",
    "        'cat_peak_time': cat_peak_time,\n",
    "        'num_peak_time': num_peak_time,\n",
    "        'conj_peak_time': conj_peak_time,\n",
    "        'binding_delay': binding_delay,\n",
    "        'cat_persistence': cat_persistence,\n",
    "        'num_persistence': num_persistence,\n",
    "        'conj_persistence': conj_persistence,\n",
    "        'window_centers': window_centers,\n",
    "        'time_course_fig': fig2,\n",
    "        'cross_temporal_fig': fig\n",
    "    }\n",
    "\n",
    "    return results, fig, fig2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3221f021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze temporal dynamics for each brain area separately\n",
    "print(\"\\nAnalyzing temporal dynamics of binding by brain area...\")\n",
    "\n",
    "# Get unique brain regions\n",
    "brain_regions = data_filtered['brainAreaOfCell'].unique()\n",
    "print(f\"Found {len(brain_regions)} brain regions: {brain_regions.tolist()}\")\n",
    "\n",
    "# Dictionary to store binding delays for each region\n",
    "region_binding_delays = {}\n",
    "\n",
    "# For each brain region, analyze all units\n",
    "for region in brain_regions:\n",
    "    print(f\"\\nAnalyzing temporal dynamics for region: {region}\")\n",
    "    \n",
    "    # Select only data from this region\n",
    "    region_data = data_filtered[data_filtered['brainAreaOfCell'] == region].copy()\n",
    "    \n",
    "    # Get all units from this region\n",
    "    region_units = region_data['unit_id'].unique().tolist()\n",
    "    \n",
    "    # Skip regions with too few units\n",
    "    if len(region_units) < 5:\n",
    "        print(f\"  Skipping {region} - only {len(region_units)} units (minimum 5 required)\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"  Analyzing {len(region_units)} units from {region}\")\n",
    "    \n",
    "    # Store binding delays for this region\n",
    "    binding_delays = []\n",
    "    \n",
    "    # Analyze each unit in this region\n",
    "    for unit_id in tqdm(region_units):\n",
    "        # Run the temporal dynamics analysis\n",
    "        dynamics, _ = analyze_binding_dynamics(region_data, unit_id)\n",
    "        \n",
    "        # Find when different signals become significant\n",
    "        cat_onset = None\n",
    "        if any(dynamics['category_sig']):\n",
    "            cat_onset = dynamics.loc[dynamics['category_sig'].idxmax(), 'time']\n",
    "            \n",
    "        num_onset = None\n",
    "        if any(dynamics['number_sig']):\n",
    "            num_onset = dynamics.loc[dynamics['number_sig'].idxmax(), 'time']\n",
    "            \n",
    "        conj_onset = None\n",
    "        if any(dynamics['conjunction_sig']):\n",
    "            conj_onset = dynamics.loc[dynamics['conjunction_sig'].idxmax(), 'time']\n",
    "            \n",
    "        # Calculate binding delay if all onsets are present\n",
    "        if cat_onset is not None and num_onset is not None and conj_onset is not None:\n",
    "            feature_onset = min(cat_onset, num_onset)\n",
    "            binding_delay = conj_onset - feature_onset\n",
    "            binding_delays.append(binding_delay)\n",
    "    \n",
    "    # Store results for this region\n",
    "    if binding_delays:\n",
    "        region_binding_delays[region] = binding_delays\n",
    "        print(f\"  {region}: {len(binding_delays)} units with valid binding delays\")\n",
    "        print(f\"  Average binding delay: {np.mean(binding_delays):.0f} ms\")\n",
    "        print(f\"  Range: {np.min(binding_delays):.0f} - {np.max(binding_delays):.0f} ms\")\n",
    "    else:\n",
    "        print(f\"  {region}: No units with valid binding delays\")\n",
    "\n",
    "# Create bar plot with error bars for binding delays by region\n",
    "if region_binding_delays:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    regions = []\n",
    "    means = []\n",
    "    errors = []\n",
    "    \n",
    "    for region, delays in region_binding_delays.items():\n",
    "        if len(delays) > 0:\n",
    "            regions.append(region)\n",
    "            means.append(np.mean(delays))\n",
    "            errors.append(np.std(delays) / np.sqrt(len(delays)))  # Standard error\n",
    "    \n",
    "    # Sort by mean binding delay\n",
    "    sorted_indices = np.argsort(means)\n",
    "    sorted_regions = [regions[i] for i in sorted_indices]\n",
    "    sorted_means = [means[i] for i in sorted_indices]\n",
    "    sorted_errors = [errors[i] for i in sorted_indices]\n",
    "    \n",
    "    plt.bar(sorted_regions, sorted_means, yerr=sorted_errors, capsize=10)\n",
    "    plt.ylabel('Binding Delay (ms)')\n",
    "    plt.title('Average Binding Delay by Brain Region')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"binding_delay_by_region.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No regions with valid binding delays to plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7060c958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform population-level temporal dynamics analysis for each brain area separately\n",
    "print(\"\\nPerforming population-level temporal dynamics analysis by brain area...\")\n",
    "\n",
    "# Make sure we have imported the required modules\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Get unique brain regions\n",
    "brain_regions = data_filtered['brainAreaOfCell'].unique()\n",
    "print(f\"Found {len(brain_regions)} brain regions: {brain_regions.tolist()}\")\n",
    "\n",
    "# For each brain region, perform independent analysis\n",
    "for region in brain_regions:\n",
    "    print(f\"\\nAnalyzing temporal dynamics for region: {region}\")\n",
    "    \n",
    "    # Select only data from this region\n",
    "    region_data = data_filtered[data_filtered['brainAreaOfCell'] == region].copy()\n",
    "    \n",
    "    # Get all units from this region\n",
    "    region_units = region_data['unit_id'].unique().tolist()\n",
    "    \n",
    "    # Skip regions with too few units\n",
    "    if len(region_units) < 5:\n",
    "        print(f\"  Skipping {region} - only {len(region_units)} units (minimum 5 required)\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"  Using all {len(region_units)} units from this region\")\n",
    "    \n",
    "    # Run the cross-temporal analysis for this region\n",
    "    try:\n",
    "        ct_results, ct_fig, time_course_fig = cross_temporal_binding_analysis(region_data, region_units)\n",
    "        plt.figure(ct_fig.number)\n",
    "        plt.suptitle(f\"Cross-Temporal Decoding - {region}\", fontsize=16)\n",
    "        plt.savefig(f\"{region}_cross_temporal_decoding.png\", dpi=300, bbox_inches='tight')\n",
    "              \n",
    "        # Save the time course figure\n",
    "        plt.figure(ct_results['time_course_fig'].number)\n",
    "        plt.suptitle(f\"Decoding Time Course - {region}\", fontsize=16)\n",
    "        plt.savefig(f\"{region}_decoding_time_course.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.close(ct_results['time_course_fig'])\n",
    "        \n",
    "     \n",
    "        # Report timing results for this region\n",
    "        print(f\"\\nTemporal dynamics in {region}:\")\n",
    "        print(f\"Category information peaks at {ct_results['cat_peak_time']:.0f} ms\")\n",
    "        print(f\"Number information peaks at {ct_results['num_peak_time']:.0f} ms\")\n",
    "        print(f\"Conjunction information peaks at {ct_results['conj_peak_time']:.0f} ms\")\n",
    "        \n",
    "        if ct_results['binding_delay'] > 0:\n",
    "            print(f\"Binding delay: {ct_results['binding_delay']:.0f} ms\")\n",
    "        else:\n",
    "            print(\"No binding delay observed\")\n",
    "        \n",
    "        print(f\"\\nPersistence of information in {region}:\")\n",
    "        print(f\"Category information persists for {ct_results['cat_persistence']:.0f} ms\")\n",
    "        print(f\"Number information persists for {ct_results['num_persistence']:.0f} ms\")\n",
    "        print(f\"Conjunction information persists for {ct_results['conj_persistence']:.0f} ms\")\n",
    "        \n",
    "        # Add analysis of information persistence during delay periods\n",
    "        delay1_indices = np.where((ct_results['window_centers'] >= 1000) & \n",
    "                                  (ct_results['window_centers'] < 2000))[0]\n",
    "        delay2_indices = np.where((ct_results['window_centers'] >= 3000) & \n",
    "                                  (ct_results['window_centers'] < 5500))[0]\n",
    "        \n",
    "        if len(delay1_indices) > 0 and len(delay2_indices) > 0:\n",
    "            # Get decoding accuracies during delay periods\n",
    "            cat_diag = np.diag(ct_results['cat_accuracy'])\n",
    "            num_diag = np.diag(ct_results['num_accuracy'])\n",
    "            conj_diag = np.diag(ct_results['conj_accuracy'])\n",
    "            \n",
    "            cat_delay1 = np.mean(cat_diag[delay1_indices])\n",
    "            num_delay1 = np.mean(num_diag[delay1_indices])\n",
    "            conj_delay1 = np.mean(conj_diag[delay1_indices])\n",
    "            \n",
    "            cat_delay2 = np.mean(cat_diag[delay2_indices])\n",
    "            num_delay2 = np.mean(num_diag[delay2_indices])\n",
    "            conj_delay2 = np.mean(conj_diag[delay2_indices])\n",
    "            \n",
    "            print(f\"\\nDelay period information maintenance in {region}:\")\n",
    "            print(f\"Delay 1 - Category: {cat_delay1:.3f}, Number: {num_delay1:.3f}, Conjunction: {conj_delay1:.3f}\")\n",
    "            print(f\"Delay 2 - Category: {cat_delay2:.3f}, Number: {num_delay2:.3f}, Conjunction: {conj_delay2:.3f}\")\n",
    "            \n",
    "            # Calculate relative change in decoding accuracy\n",
    "            if cat_delay1 > 0:\n",
    "                cat_change = (cat_delay2 - cat_delay1) / cat_delay1 * 100\n",
    "                print(f\"Category change: {cat_change:.1f}%\")\n",
    "            if num_delay1 > 0:\n",
    "                num_change = (num_delay2 - num_delay1) / num_delay1 * 100\n",
    "                print(f\"Number change: {num_change:.1f}%\")\n",
    "            if conj_delay1 > 0:\n",
    "                conj_change = (conj_delay2 - conj_delay1) / conj_delay1 * 100\n",
    "                print(f\"Conjunction change: {conj_change:.1f}%\")\n",
    "        \n",
    "        # Update summary report with region-specific information\n",
    "        with open(\"conjunction_coding_summary.txt\", \"a\") as f:\n",
    "            f.write(f\"\\nTEMPORAL DYNAMICS IN {region.upper()}\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            f.write(f\"Number of units: {len(region_units)}\\n\")\n",
    "            f.write(f\"Category information peaks at {ct_results['cat_peak_time']:.0f} ms\\n\")\n",
    "            f.write(f\"Number information peaks at {ct_results['num_peak_time']:.0f} ms\\n\")\n",
    "            f.write(f\"Conjunction information peaks at {ct_results['conj_peak_time']:.0f} ms\\n\\n\")\n",
    "            \n",
    "            if ct_results['binding_delay'] > 0:\n",
    "                f.write(f\"Binding delay: {ct_results['binding_delay']:.0f} ms\\n\\n\")\n",
    "            else:\n",
    "                f.write(\"No binding delay observed\\n\\n\")\n",
    "            \n",
    "            f.write(f\"Persistence of information:\\n\")\n",
    "            f.write(f\"Category: {ct_results['cat_persistence']:.0f} ms, \")\n",
    "            f.write(f\"Number: {ct_results['num_persistence']:.0f} ms, \")\n",
    "            f.write(f\"Conjunction: {ct_results['conj_persistence']:.0f} ms\\n\\n\")\n",
    "            \n",
    "            if len(delay1_indices) > 0 and len(delay2_indices) > 0:\n",
    "                f.write(\"Delay period information maintenance:\\n\")\n",
    "                f.write(f\"Delay 1 - Category: {cat_delay1:.3f}, Number: {num_delay1:.3f}, Conjunction: {conj_delay1:.3f}\\n\")\n",
    "                f.write(f\"Delay 2 - Category: {cat_delay2:.3f}, Number: {num_delay2:.3f}, Conjunction: {conj_delay2:.3f}\\n\\n\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing {region}: {str(e)}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06eb20d1",
   "metadata": {},
   "source": [
    "### Compare conjunction coding between valid and invalid probe conditions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
